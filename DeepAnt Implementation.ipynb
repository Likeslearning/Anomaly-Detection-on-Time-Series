{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project-no2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmGiG1nUTy3W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPvJ3LsUVhS4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r394d4ovVJ-e"
      },
      "source": [
        "\r\n",
        "import numpy as np\r\n",
        "from numpy import array\r\n",
        "import random\r\n",
        "from random import randint\r\n",
        "import os\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Conv1D, Flatten, Activation, MaxPooling1D, Dropout\r\n",
        "from keras.optimizers import SGD\r\n",
        "import io                      \r\n",
        "import torch\r\n",
        "import torchvision.models as models\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9rC0tz0Ty8j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "9ae9fece-5f57-42f7-f997-4845f1a3b784"
      },
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/dev-aadarsh/DeepAnT/master/ydata-labeled-time-series-anomalies-v1_0/A1Benchmark/real_60.csv',index_col=0)\r\n",
        "data.head()\r\n",
        "\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>is_anomaly</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.265278</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.100833</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.147778</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.053889</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.051944</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              value  is_anomaly\n",
              "timestamp                      \n",
              "1          1.265278           0\n",
              "2          1.100833           0\n",
              "3          1.147778           0\n",
              "4          1.053889           0\n",
              "5          1.051944           0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEV1Gwbx6oTg"
      },
      "source": [
        "\r\n",
        "from pandas import read_csv\r\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ5n31QLTzBn",
        "outputId": "7fa586c6-b5e1-4bf6-b121-1071a80cccdb"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1461, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej2xFZKVTzES"
      },
      "source": [
        "train_percent = int(0.3*len(data))\r\n",
        "valid_percent = int(0.1*len(data))\r\n",
        "test_percent = int(0.6*len(data))\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUsmrsDXTzG2"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "train_data = list(data.iloc[:train_percent,0])\r\n",
        "valid_data = list(data.iloc[train_percent:train_percent+valid_percent,0])\r\n",
        "test_data = list(data.iloc[train_percent+valid_percent:,0])\r\n",
        "\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAxHloThgkDJ"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "116g8CtJTzJX"
      },
      "source": [
        "# define parameters\r\n",
        "w = 35\r\n",
        "pred_window = 1\r\n",
        "filter1_size = 32\r\n",
        "filter2_size = 32\r\n",
        "kernel_size = 2\r\n",
        "stride = 1\r\n",
        "pool_size = 2"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDcFFhiUXwMJ",
        "outputId": "fa43d2be-7151-44aa-9387-cc601f25b5f9"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1461"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zEl8h15TzMA",
        "outputId": "684eefa2-ad5a-47a3-a3d8-1c7300db380e"
      },
      "source": [
        "def get_subsequences(data):\r\n",
        "    X = []\r\n",
        "    Y = []\r\n",
        "    \r\n",
        "    for i in range(len(data) - w -pred_window):\r\n",
        "        X.append(data[i:i+w])\r\n",
        "        Y.append(data[i+w:i+w+pred_window])\r\n",
        "    return np.array(X),np.array(Y)\r\n",
        "\r\n",
        "trainX,trainY = get_subsequences(train_data)\r\n",
        "print(\"trainx\",trainX.shape)\r\n",
        "print(\"trainy\",trainY.shape)\r\n",
        "\r\n",
        "trainX = np.reshape(trainX,(trainX.shape[0],1,trainX.shape[1]))\r\n",
        "\r\n",
        "\r\n",
        "validX,validY = get_subsequences(valid_data)\r\n",
        "validX = np.reshape(validX,(validX.shape[0],1,validX.shape[1]))\r\n",
        "\r\n",
        "testX,testY = get_subsequences(test_data)\r\n",
        "testX = np.reshape(testX,(testX.shape[0],1,testX.shape[1]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainx (402, 35)\n",
            "trainy (402, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a7mC670Dm-A"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cif11t3VTzPF"
      },
      "source": [
        "#  CNN architecture\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Net, self).__init__()\r\n",
        "        ## layers of a CNN\r\n",
        "        \r\n",
        "        self.conv1 = nn.Conv1d(1,filter1_size,kernel_size,stride,padding = 0)\r\n",
        "        \r\n",
        "        self.conv2 = nn.Conv1d(filter1_size,filter2_size,kernel_size,stride,padding = 0)\r\n",
        "\r\n",
        "        self.maxpool = nn.MaxPool1d(pool_size)\r\n",
        "        \r\n",
        "        self.dim1 = int(0.5*(0.5*(w-1)-1)) * filter2_size\r\n",
        "        \r\n",
        "        self.lin1 = nn.Linear(self.dim1,pred_window )\r\n",
        "        self.dropout = nn.Dropout(0.25)\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        #convolution layer 1\r\n",
        "        x = (F.relu(self.conv1(x)))\r\n",
        "        x = self.maxpool(x)\r\n",
        "        #print(x.shape)\r\n",
        "        #x = self.dropout(x)\r\n",
        "\r\n",
        "        #convolution layer 2\r\n",
        "        x = (F.relu(self.conv2(x)))\r\n",
        "        x = self.maxpool(x)\r\n",
        "    \r\n",
        "\r\n",
        "        \r\n",
        "        x = x.view(-1,self.dim1) ## Flatten layer\r\n",
        "        x = self.dropout(x) ## Dropout layers are important in training CNNs because they prevent overfitting on the training data.\r\n",
        "        x = self.lin1(x) ## Dense layer\r\n",
        "       \r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeBDj8nkDmTh"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqCTzFYPd3by",
        "outputId": "e5ed9eac-f3cf-49ea-a8aa-e38a801d5651"
      },
      "source": [
        "\r\n",
        "# define CNN model\r\n",
        "\r\n",
        "model_A1 = Net()\r\n",
        "print(model_A1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv1d(1, 32, kernel_size=(2,), stride=(1,))\n",
            "  (conv2): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
            "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (lin1): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtY0AfnTd-ZJ"
      },
      "source": [
        "criterion_scratch = nn.L1Loss()\r\n",
        "optimizer_scratch = optim.SGD(model_A1.parameters(), lr = 1e-5,weight_decay=1e-6)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L8WLYHbMQMG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddnorbDHTzRa"
      },
      "source": [
        "# function for training the model (also checks on validation data)\r\n",
        "\r\n",
        "def train_valid(n_epochs, trainX,trainY, validX,validY,model, optimizer, criterion,save_path,freq = 5):\r\n",
        "    \"\"\"returns trained model\"\"\"\r\n",
        "\r\n",
        "    target_train = torch.tensor(trainY).type('torch.FloatTensor')\r\n",
        "    data_train = torch.tensor(trainX).type('torch.FloatTensor')\r\n",
        "    \r\n",
        "    target_valid = torch.tensor(validY).type('torch.FloatTensor')\r\n",
        "    data_valid = torch.tensor(validX).type('torch.FloatTensor')\r\n",
        "    \r\n",
        "    train_loss_min = np.Inf\r\n",
        "    valid_loss_min = np.Inf\r\n",
        "    last_valid_loss= 0\r\n",
        "    \r\n",
        "    for epoch in range(1, n_epochs+1):\r\n",
        "        \r\n",
        "        ###################\r\n",
        "        # training the model #\r\n",
        "        ###################\r\n",
        "        model.train()\r\n",
        "\r\n",
        "        #print(data.shape)\r\n",
        "\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        output = model(data_train)\r\n",
        "        loss = criterion(output, target_train)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        train_loss = loss.item()\r\n",
        "        \r\n",
        "        ###################\r\n",
        "        # Validation #\r\n",
        "        ###################\r\n",
        "        model.eval()\r\n",
        "        output_valid = model(data_valid)\r\n",
        "        \r\n",
        "        loss_valid = criterion(output_valid, target_valid)\r\n",
        "        valid_loss = loss_valid.item()\r\n",
        "        if(valid_loss == last_valid_loss):\r\n",
        "            print('problem')\r\n",
        "            \r\n",
        "        last_valid_loss = valid_loss\r\n",
        "        if(epoch%freq == 0):\r\n",
        "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\r\n",
        "                epoch, \r\n",
        "                train_loss,\r\n",
        "                valid_loss\r\n",
        "                ))\r\n",
        "            \r\n",
        "        if valid_loss < valid_loss_min:\r\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\r\n",
        "            valid_loss_min,\r\n",
        "            valid_loss))\r\n",
        "            torch.save(model.state_dict(), save_path)\r\n",
        "            valid_loss_min = valid_loss\r\n",
        "\r\n",
        "    return model,output"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME8Th-GY1mFe",
        "outputId": "5fb90baf-efe0-4d73-8432-b42aad61a254"
      },
      "source": [
        "type(model_A1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.Net"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1dCTeWRTzUK",
        "outputId": "fa6edfcd-1099-46d0-a679-43639092d035"
      },
      "source": [
        "model_A1,out = train_valid(3000, trainX,trainY,validX,validY, model_A1, optimizer_scratch, \r\n",
        "                      criterion_scratch, 'sinwave_DeepAnT_2.h5',freq = 10)\r\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased (inf --> 0.906656).  Saving model ...\n",
            "Validation loss decreased (0.906656 --> 0.906501).  Saving model ...\n",
            "Validation loss decreased (0.906501 --> 0.906346).  Saving model ...\n",
            "Validation loss decreased (0.906346 --> 0.906190).  Saving model ...\n",
            "Validation loss decreased (0.906190 --> 0.906035).  Saving model ...\n",
            "Validation loss decreased (0.906035 --> 0.905880).  Saving model ...\n",
            "Validation loss decreased (0.905880 --> 0.905725).  Saving model ...\n",
            "Validation loss decreased (0.905725 --> 0.905570).  Saving model ...\n",
            "Validation loss decreased (0.905570 --> 0.905416).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.917063 \tValidation Loss: 0.905262\n",
            "Validation loss decreased (0.905416 --> 0.905262).  Saving model ...\n",
            "Validation loss decreased (0.905262 --> 0.905108).  Saving model ...\n",
            "Validation loss decreased (0.905108 --> 0.904953).  Saving model ...\n",
            "Validation loss decreased (0.904953 --> 0.904799).  Saving model ...\n",
            "Validation loss decreased (0.904799 --> 0.904645).  Saving model ...\n",
            "Validation loss decreased (0.904645 --> 0.904489).  Saving model ...\n",
            "Validation loss decreased (0.904489 --> 0.904334).  Saving model ...\n",
            "Validation loss decreased (0.904334 --> 0.904178).  Saving model ...\n",
            "Validation loss decreased (0.904178 --> 0.904023).  Saving model ...\n",
            "Validation loss decreased (0.904023 --> 0.903868).  Saving model ...\n",
            "Epoch: 20 \tTraining Loss: 0.921135 \tValidation Loss: 0.903713\n",
            "Validation loss decreased (0.903868 --> 0.903713).  Saving model ...\n",
            "Validation loss decreased (0.903713 --> 0.903558).  Saving model ...\n",
            "Validation loss decreased (0.903558 --> 0.903404).  Saving model ...\n",
            "Validation loss decreased (0.903404 --> 0.903249).  Saving model ...\n",
            "Validation loss decreased (0.903249 --> 0.903095).  Saving model ...\n",
            "Validation loss decreased (0.903095 --> 0.902940).  Saving model ...\n",
            "Validation loss decreased (0.902940 --> 0.902785).  Saving model ...\n",
            "Validation loss decreased (0.902785 --> 0.902631).  Saving model ...\n",
            "Validation loss decreased (0.902631 --> 0.902476).  Saving model ...\n",
            "Validation loss decreased (0.902476 --> 0.902321).  Saving model ...\n",
            "Epoch: 30 \tTraining Loss: 0.918385 \tValidation Loss: 0.902166\n",
            "Validation loss decreased (0.902321 --> 0.902166).  Saving model ...\n",
            "Validation loss decreased (0.902166 --> 0.902012).  Saving model ...\n",
            "Validation loss decreased (0.902012 --> 0.901857).  Saving model ...\n",
            "Validation loss decreased (0.901857 --> 0.901703).  Saving model ...\n",
            "Validation loss decreased (0.901703 --> 0.901547).  Saving model ...\n",
            "Validation loss decreased (0.901547 --> 0.901394).  Saving model ...\n",
            "Validation loss decreased (0.901394 --> 0.901238).  Saving model ...\n",
            "Validation loss decreased (0.901238 --> 0.901084).  Saving model ...\n",
            "Validation loss decreased (0.901084 --> 0.900928).  Saving model ...\n",
            "Validation loss decreased (0.900928 --> 0.900774).  Saving model ...\n",
            "Epoch: 40 \tTraining Loss: 0.918799 \tValidation Loss: 0.900620\n",
            "Validation loss decreased (0.900774 --> 0.900620).  Saving model ...\n",
            "Validation loss decreased (0.900620 --> 0.900465).  Saving model ...\n",
            "Validation loss decreased (0.900465 --> 0.900310).  Saving model ...\n",
            "Validation loss decreased (0.900310 --> 0.900156).  Saving model ...\n",
            "Validation loss decreased (0.900156 --> 0.900001).  Saving model ...\n",
            "Validation loss decreased (0.900001 --> 0.899845).  Saving model ...\n",
            "Validation loss decreased (0.899845 --> 0.899690).  Saving model ...\n",
            "Validation loss decreased (0.899690 --> 0.899534).  Saving model ...\n",
            "Validation loss decreased (0.899534 --> 0.899380).  Saving model ...\n",
            "Validation loss decreased (0.899380 --> 0.899225).  Saving model ...\n",
            "Epoch: 50 \tTraining Loss: 0.915932 \tValidation Loss: 0.899070\n",
            "Validation loss decreased (0.899225 --> 0.899070).  Saving model ...\n",
            "Validation loss decreased (0.899070 --> 0.898916).  Saving model ...\n",
            "Validation loss decreased (0.898916 --> 0.898761).  Saving model ...\n",
            "Validation loss decreased (0.898761 --> 0.898606).  Saving model ...\n",
            "Validation loss decreased (0.898606 --> 0.898451).  Saving model ...\n",
            "Validation loss decreased (0.898451 --> 0.898297).  Saving model ...\n",
            "Validation loss decreased (0.898297 --> 0.898142).  Saving model ...\n",
            "Validation loss decreased (0.898142 --> 0.897987).  Saving model ...\n",
            "Validation loss decreased (0.897987 --> 0.897832).  Saving model ...\n",
            "Validation loss decreased (0.897832 --> 0.897679).  Saving model ...\n",
            "Epoch: 60 \tTraining Loss: 0.916326 \tValidation Loss: 0.897525\n",
            "Validation loss decreased (0.897679 --> 0.897525).  Saving model ...\n",
            "Validation loss decreased (0.897525 --> 0.897370).  Saving model ...\n",
            "Validation loss decreased (0.897370 --> 0.897215).  Saving model ...\n",
            "Validation loss decreased (0.897215 --> 0.897060).  Saving model ...\n",
            "Validation loss decreased (0.897060 --> 0.896906).  Saving model ...\n",
            "Validation loss decreased (0.896906 --> 0.896751).  Saving model ...\n",
            "Validation loss decreased (0.896751 --> 0.896596).  Saving model ...\n",
            "Validation loss decreased (0.896596 --> 0.896441).  Saving model ...\n",
            "Validation loss decreased (0.896441 --> 0.896287).  Saving model ...\n",
            "Validation loss decreased (0.896287 --> 0.896132).  Saving model ...\n",
            "Epoch: 70 \tTraining Loss: 0.912657 \tValidation Loss: 0.895977\n",
            "Validation loss decreased (0.896132 --> 0.895977).  Saving model ...\n",
            "Validation loss decreased (0.895977 --> 0.895822).  Saving model ...\n",
            "Validation loss decreased (0.895822 --> 0.895668).  Saving model ...\n",
            "Validation loss decreased (0.895668 --> 0.895513).  Saving model ...\n",
            "Validation loss decreased (0.895513 --> 0.895359).  Saving model ...\n",
            "Validation loss decreased (0.895359 --> 0.895203).  Saving model ...\n",
            "Validation loss decreased (0.895203 --> 0.895050).  Saving model ...\n",
            "Validation loss decreased (0.895050 --> 0.894896).  Saving model ...\n",
            "Validation loss decreased (0.894896 --> 0.894741).  Saving model ...\n",
            "Validation loss decreased (0.894741 --> 0.894587).  Saving model ...\n",
            "Epoch: 80 \tTraining Loss: 0.910882 \tValidation Loss: 0.894432\n",
            "Validation loss decreased (0.894587 --> 0.894432).  Saving model ...\n",
            "Validation loss decreased (0.894432 --> 0.894277).  Saving model ...\n",
            "Validation loss decreased (0.894277 --> 0.894123).  Saving model ...\n",
            "Validation loss decreased (0.894123 --> 0.893968).  Saving model ...\n",
            "Validation loss decreased (0.893968 --> 0.893813).  Saving model ...\n",
            "Validation loss decreased (0.893813 --> 0.893659).  Saving model ...\n",
            "Validation loss decreased (0.893659 --> 0.893504).  Saving model ...\n",
            "Validation loss decreased (0.893504 --> 0.893349).  Saving model ...\n",
            "Validation loss decreased (0.893349 --> 0.893193).  Saving model ...\n",
            "Validation loss decreased (0.893193 --> 0.893038).  Saving model ...\n",
            "Epoch: 90 \tTraining Loss: 0.909473 \tValidation Loss: 0.892883\n",
            "Validation loss decreased (0.893038 --> 0.892883).  Saving model ...\n",
            "Validation loss decreased (0.892883 --> 0.892729).  Saving model ...\n",
            "Validation loss decreased (0.892729 --> 0.892573).  Saving model ...\n",
            "Validation loss decreased (0.892573 --> 0.892419).  Saving model ...\n",
            "Validation loss decreased (0.892419 --> 0.892264).  Saving model ...\n",
            "Validation loss decreased (0.892264 --> 0.892110).  Saving model ...\n",
            "Validation loss decreased (0.892110 --> 0.891956).  Saving model ...\n",
            "Validation loss decreased (0.891956 --> 0.891801).  Saving model ...\n",
            "Validation loss decreased (0.891801 --> 0.891645).  Saving model ...\n",
            "Validation loss decreased (0.891645 --> 0.891491).  Saving model ...\n",
            "Epoch: 100 \tTraining Loss: 0.907290 \tValidation Loss: 0.891336\n",
            "Validation loss decreased (0.891491 --> 0.891336).  Saving model ...\n",
            "Validation loss decreased (0.891336 --> 0.891181).  Saving model ...\n",
            "Validation loss decreased (0.891181 --> 0.891026).  Saving model ...\n",
            "Validation loss decreased (0.891026 --> 0.890872).  Saving model ...\n",
            "Validation loss decreased (0.890872 --> 0.890717).  Saving model ...\n",
            "Validation loss decreased (0.890717 --> 0.890562).  Saving model ...\n",
            "Validation loss decreased (0.890562 --> 0.890407).  Saving model ...\n",
            "Validation loss decreased (0.890407 --> 0.890253).  Saving model ...\n",
            "Validation loss decreased (0.890253 --> 0.890098).  Saving model ...\n",
            "Validation loss decreased (0.890098 --> 0.889944).  Saving model ...\n",
            "Epoch: 110 \tTraining Loss: 0.911676 \tValidation Loss: 0.889790\n",
            "Validation loss decreased (0.889944 --> 0.889790).  Saving model ...\n",
            "Validation loss decreased (0.889790 --> 0.889635).  Saving model ...\n",
            "Validation loss decreased (0.889635 --> 0.889480).  Saving model ...\n",
            "Validation loss decreased (0.889480 --> 0.889325).  Saving model ...\n",
            "Validation loss decreased (0.889325 --> 0.889170).  Saving model ...\n",
            "Validation loss decreased (0.889170 --> 0.889015).  Saving model ...\n",
            "Validation loss decreased (0.889015 --> 0.888859).  Saving model ...\n",
            "Validation loss decreased (0.888859 --> 0.888705).  Saving model ...\n",
            "Validation loss decreased (0.888705 --> 0.888552).  Saving model ...\n",
            "Validation loss decreased (0.888552 --> 0.888398).  Saving model ...\n",
            "Epoch: 120 \tTraining Loss: 0.904146 \tValidation Loss: 0.888244\n",
            "Validation loss decreased (0.888398 --> 0.888244).  Saving model ...\n",
            "Validation loss decreased (0.888244 --> 0.888088).  Saving model ...\n",
            "Validation loss decreased (0.888088 --> 0.887934).  Saving model ...\n",
            "Validation loss decreased (0.887934 --> 0.887780).  Saving model ...\n",
            "Validation loss decreased (0.887780 --> 0.887626).  Saving model ...\n",
            "Validation loss decreased (0.887626 --> 0.887473).  Saving model ...\n",
            "Validation loss decreased (0.887473 --> 0.887319).  Saving model ...\n",
            "Validation loss decreased (0.887319 --> 0.887164).  Saving model ...\n",
            "Validation loss decreased (0.887164 --> 0.887009).  Saving model ...\n",
            "Validation loss decreased (0.887009 --> 0.886854).  Saving model ...\n",
            "Epoch: 130 \tTraining Loss: 0.902990 \tValidation Loss: 0.886699\n",
            "Validation loss decreased (0.886854 --> 0.886699).  Saving model ...\n",
            "Validation loss decreased (0.886699 --> 0.886544).  Saving model ...\n",
            "Validation loss decreased (0.886544 --> 0.886391).  Saving model ...\n",
            "Validation loss decreased (0.886391 --> 0.886236).  Saving model ...\n",
            "Validation loss decreased (0.886236 --> 0.886082).  Saving model ...\n",
            "Validation loss decreased (0.886082 --> 0.885928).  Saving model ...\n",
            "Validation loss decreased (0.885928 --> 0.885774).  Saving model ...\n",
            "Validation loss decreased (0.885774 --> 0.885620).  Saving model ...\n",
            "Validation loss decreased (0.885620 --> 0.885467).  Saving model ...\n",
            "Validation loss decreased (0.885467 --> 0.885313).  Saving model ...\n",
            "Epoch: 140 \tTraining Loss: 0.906049 \tValidation Loss: 0.885158\n",
            "Validation loss decreased (0.885313 --> 0.885158).  Saving model ...\n",
            "Validation loss decreased (0.885158 --> 0.885003).  Saving model ...\n",
            "Validation loss decreased (0.885003 --> 0.884848).  Saving model ...\n",
            "Validation loss decreased (0.884848 --> 0.884694).  Saving model ...\n",
            "Validation loss decreased (0.884694 --> 0.884540).  Saving model ...\n",
            "Validation loss decreased (0.884540 --> 0.884385).  Saving model ...\n",
            "Validation loss decreased (0.884385 --> 0.884231).  Saving model ...\n",
            "Validation loss decreased (0.884231 --> 0.884076).  Saving model ...\n",
            "Validation loss decreased (0.884076 --> 0.883922).  Saving model ...\n",
            "Validation loss decreased (0.883922 --> 0.883766).  Saving model ...\n",
            "Epoch: 150 \tTraining Loss: 0.903640 \tValidation Loss: 0.883612\n",
            "Validation loss decreased (0.883766 --> 0.883612).  Saving model ...\n",
            "Validation loss decreased (0.883612 --> 0.883457).  Saving model ...\n",
            "Validation loss decreased (0.883457 --> 0.883303).  Saving model ...\n",
            "Validation loss decreased (0.883303 --> 0.883149).  Saving model ...\n",
            "Validation loss decreased (0.883149 --> 0.882994).  Saving model ...\n",
            "Validation loss decreased (0.882994 --> 0.882840).  Saving model ...\n",
            "Validation loss decreased (0.882840 --> 0.882685).  Saving model ...\n",
            "Validation loss decreased (0.882685 --> 0.882530).  Saving model ...\n",
            "Validation loss decreased (0.882530 --> 0.882375).  Saving model ...\n",
            "Validation loss decreased (0.882375 --> 0.882221).  Saving model ...\n",
            "Epoch: 160 \tTraining Loss: 0.898232 \tValidation Loss: 0.882067\n",
            "Validation loss decreased (0.882221 --> 0.882067).  Saving model ...\n",
            "Validation loss decreased (0.882067 --> 0.881912).  Saving model ...\n",
            "Validation loss decreased (0.881912 --> 0.881758).  Saving model ...\n",
            "Validation loss decreased (0.881758 --> 0.881603).  Saving model ...\n",
            "Validation loss decreased (0.881603 --> 0.881448).  Saving model ...\n",
            "Validation loss decreased (0.881448 --> 0.881293).  Saving model ...\n",
            "Validation loss decreased (0.881293 --> 0.881138).  Saving model ...\n",
            "Validation loss decreased (0.881138 --> 0.880983).  Saving model ...\n",
            "Validation loss decreased (0.880983 --> 0.880828).  Saving model ...\n",
            "Validation loss decreased (0.880828 --> 0.880673).  Saving model ...\n",
            "Epoch: 170 \tTraining Loss: 0.894014 \tValidation Loss: 0.880518\n",
            "Validation loss decreased (0.880673 --> 0.880518).  Saving model ...\n",
            "Validation loss decreased (0.880518 --> 0.880363).  Saving model ...\n",
            "Validation loss decreased (0.880363 --> 0.880209).  Saving model ...\n",
            "Validation loss decreased (0.880209 --> 0.880054).  Saving model ...\n",
            "Validation loss decreased (0.880054 --> 0.879899).  Saving model ...\n",
            "Validation loss decreased (0.879899 --> 0.879745).  Saving model ...\n",
            "Validation loss decreased (0.879745 --> 0.879591).  Saving model ...\n",
            "Validation loss decreased (0.879591 --> 0.879436).  Saving model ...\n",
            "Validation loss decreased (0.879436 --> 0.879282).  Saving model ...\n",
            "Validation loss decreased (0.879282 --> 0.879127).  Saving model ...\n",
            "Epoch: 180 \tTraining Loss: 0.895778 \tValidation Loss: 0.878972\n",
            "Validation loss decreased (0.879127 --> 0.878972).  Saving model ...\n",
            "Validation loss decreased (0.878972 --> 0.878818).  Saving model ...\n",
            "Validation loss decreased (0.878818 --> 0.878663).  Saving model ...\n",
            "Validation loss decreased (0.878663 --> 0.878508).  Saving model ...\n",
            "Validation loss decreased (0.878508 --> 0.878354).  Saving model ...\n",
            "Validation loss decreased (0.878354 --> 0.878200).  Saving model ...\n",
            "Validation loss decreased (0.878200 --> 0.878045).  Saving model ...\n",
            "Validation loss decreased (0.878045 --> 0.877889).  Saving model ...\n",
            "Validation loss decreased (0.877889 --> 0.877735).  Saving model ...\n",
            "Validation loss decreased (0.877735 --> 0.877580).  Saving model ...\n",
            "Epoch: 190 \tTraining Loss: 0.894403 \tValidation Loss: 0.877425\n",
            "Validation loss decreased (0.877580 --> 0.877425).  Saving model ...\n",
            "Validation loss decreased (0.877425 --> 0.877269).  Saving model ...\n",
            "Validation loss decreased (0.877269 --> 0.877115).  Saving model ...\n",
            "Validation loss decreased (0.877115 --> 0.876960).  Saving model ...\n",
            "Validation loss decreased (0.876960 --> 0.876806).  Saving model ...\n",
            "Validation loss decreased (0.876806 --> 0.876652).  Saving model ...\n",
            "Validation loss decreased (0.876652 --> 0.876497).  Saving model ...\n",
            "Validation loss decreased (0.876497 --> 0.876342).  Saving model ...\n",
            "Validation loss decreased (0.876342 --> 0.876187).  Saving model ...\n",
            "Validation loss decreased (0.876187 --> 0.876033).  Saving model ...\n",
            "Epoch: 200 \tTraining Loss: 0.895368 \tValidation Loss: 0.875879\n",
            "Validation loss decreased (0.876033 --> 0.875879).  Saving model ...\n",
            "Validation loss decreased (0.875879 --> 0.875724).  Saving model ...\n",
            "Validation loss decreased (0.875724 --> 0.875570).  Saving model ...\n",
            "Validation loss decreased (0.875570 --> 0.875416).  Saving model ...\n",
            "Validation loss decreased (0.875416 --> 0.875261).  Saving model ...\n",
            "Validation loss decreased (0.875261 --> 0.875106).  Saving model ...\n",
            "Validation loss decreased (0.875106 --> 0.874952).  Saving model ...\n",
            "Validation loss decreased (0.874952 --> 0.874797).  Saving model ...\n",
            "Validation loss decreased (0.874797 --> 0.874643).  Saving model ...\n",
            "Validation loss decreased (0.874643 --> 0.874488).  Saving model ...\n",
            "Epoch: 210 \tTraining Loss: 0.900201 \tValidation Loss: 0.874334\n",
            "Validation loss decreased (0.874488 --> 0.874334).  Saving model ...\n",
            "Validation loss decreased (0.874334 --> 0.874180).  Saving model ...\n",
            "Validation loss decreased (0.874180 --> 0.874026).  Saving model ...\n",
            "Validation loss decreased (0.874026 --> 0.873871).  Saving model ...\n",
            "Validation loss decreased (0.873871 --> 0.873716).  Saving model ...\n",
            "Validation loss decreased (0.873716 --> 0.873562).  Saving model ...\n",
            "Validation loss decreased (0.873562 --> 0.873406).  Saving model ...\n",
            "Validation loss decreased (0.873406 --> 0.873251).  Saving model ...\n",
            "Validation loss decreased (0.873251 --> 0.873097).  Saving model ...\n",
            "Validation loss decreased (0.873097 --> 0.872943).  Saving model ...\n",
            "Epoch: 220 \tTraining Loss: 0.892027 \tValidation Loss: 0.872789\n",
            "Validation loss decreased (0.872943 --> 0.872789).  Saving model ...\n",
            "Validation loss decreased (0.872789 --> 0.872635).  Saving model ...\n",
            "Validation loss decreased (0.872635 --> 0.872479).  Saving model ...\n",
            "Validation loss decreased (0.872479 --> 0.872325).  Saving model ...\n",
            "Validation loss decreased (0.872325 --> 0.872170).  Saving model ...\n",
            "Validation loss decreased (0.872170 --> 0.872015).  Saving model ...\n",
            "Validation loss decreased (0.872015 --> 0.871860).  Saving model ...\n",
            "Validation loss decreased (0.871860 --> 0.871705).  Saving model ...\n",
            "Validation loss decreased (0.871705 --> 0.871553).  Saving model ...\n",
            "Validation loss decreased (0.871553 --> 0.871398).  Saving model ...\n",
            "Epoch: 230 \tTraining Loss: 0.889244 \tValidation Loss: 0.871244\n",
            "Validation loss decreased (0.871398 --> 0.871244).  Saving model ...\n",
            "Validation loss decreased (0.871244 --> 0.871090).  Saving model ...\n",
            "Validation loss decreased (0.871090 --> 0.870937).  Saving model ...\n",
            "Validation loss decreased (0.870937 --> 0.870781).  Saving model ...\n",
            "Validation loss decreased (0.870781 --> 0.870627).  Saving model ...\n",
            "Validation loss decreased (0.870627 --> 0.870471).  Saving model ...\n",
            "Validation loss decreased (0.870471 --> 0.870317).  Saving model ...\n",
            "Validation loss decreased (0.870317 --> 0.870163).  Saving model ...\n",
            "Validation loss decreased (0.870163 --> 0.870009).  Saving model ...\n",
            "Validation loss decreased (0.870009 --> 0.869854).  Saving model ...\n",
            "Epoch: 240 \tTraining Loss: 0.887388 \tValidation Loss: 0.869700\n",
            "Validation loss decreased (0.869854 --> 0.869700).  Saving model ...\n",
            "Validation loss decreased (0.869700 --> 0.869546).  Saving model ...\n",
            "Validation loss decreased (0.869546 --> 0.869391).  Saving model ...\n",
            "Validation loss decreased (0.869391 --> 0.869237).  Saving model ...\n",
            "Validation loss decreased (0.869237 --> 0.869082).  Saving model ...\n",
            "Validation loss decreased (0.869082 --> 0.868926).  Saving model ...\n",
            "Validation loss decreased (0.868926 --> 0.868772).  Saving model ...\n",
            "Validation loss decreased (0.868772 --> 0.868619).  Saving model ...\n",
            "Validation loss decreased (0.868619 --> 0.868465).  Saving model ...\n",
            "Validation loss decreased (0.868465 --> 0.868311).  Saving model ...\n",
            "Epoch: 250 \tTraining Loss: 0.881011 \tValidation Loss: 0.868156\n",
            "Validation loss decreased (0.868311 --> 0.868156).  Saving model ...\n",
            "Validation loss decreased (0.868156 --> 0.868002).  Saving model ...\n",
            "Validation loss decreased (0.868002 --> 0.867848).  Saving model ...\n",
            "Validation loss decreased (0.867848 --> 0.867693).  Saving model ...\n",
            "Validation loss decreased (0.867693 --> 0.867538).  Saving model ...\n",
            "Validation loss decreased (0.867538 --> 0.867383).  Saving model ...\n",
            "Validation loss decreased (0.867383 --> 0.867228).  Saving model ...\n",
            "Validation loss decreased (0.867228 --> 0.867074).  Saving model ...\n",
            "Validation loss decreased (0.867074 --> 0.866920).  Saving model ...\n",
            "Validation loss decreased (0.866920 --> 0.866765).  Saving model ...\n",
            "Epoch: 260 \tTraining Loss: 0.884529 \tValidation Loss: 0.866611\n",
            "Validation loss decreased (0.866765 --> 0.866611).  Saving model ...\n",
            "Validation loss decreased (0.866611 --> 0.866456).  Saving model ...\n",
            "Validation loss decreased (0.866456 --> 0.866301).  Saving model ...\n",
            "Validation loss decreased (0.866301 --> 0.866147).  Saving model ...\n",
            "Validation loss decreased (0.866147 --> 0.865992).  Saving model ...\n",
            "Validation loss decreased (0.865992 --> 0.865838).  Saving model ...\n",
            "Validation loss decreased (0.865838 --> 0.865684).  Saving model ...\n",
            "Validation loss decreased (0.865684 --> 0.865529).  Saving model ...\n",
            "Validation loss decreased (0.865529 --> 0.865375).  Saving model ...\n",
            "Validation loss decreased (0.865375 --> 0.865220).  Saving model ...\n",
            "Epoch: 270 \tTraining Loss: 0.881633 \tValidation Loss: 0.865066\n",
            "Validation loss decreased (0.865220 --> 0.865066).  Saving model ...\n",
            "Validation loss decreased (0.865066 --> 0.864912).  Saving model ...\n",
            "Validation loss decreased (0.864912 --> 0.864758).  Saving model ...\n",
            "Validation loss decreased (0.864758 --> 0.864603).  Saving model ...\n",
            "Validation loss decreased (0.864603 --> 0.864449).  Saving model ...\n",
            "Validation loss decreased (0.864449 --> 0.864296).  Saving model ...\n",
            "Validation loss decreased (0.864296 --> 0.864141).  Saving model ...\n",
            "Validation loss decreased (0.864141 --> 0.863986).  Saving model ...\n",
            "Validation loss decreased (0.863986 --> 0.863831).  Saving model ...\n",
            "Validation loss decreased (0.863831 --> 0.863677).  Saving model ...\n",
            "Epoch: 280 \tTraining Loss: 0.881239 \tValidation Loss: 0.863522\n",
            "Validation loss decreased (0.863677 --> 0.863522).  Saving model ...\n",
            "Validation loss decreased (0.863522 --> 0.863367).  Saving model ...\n",
            "Validation loss decreased (0.863367 --> 0.863212).  Saving model ...\n",
            "Validation loss decreased (0.863212 --> 0.863057).  Saving model ...\n",
            "Validation loss decreased (0.863057 --> 0.862902).  Saving model ...\n",
            "Validation loss decreased (0.862902 --> 0.862747).  Saving model ...\n",
            "Validation loss decreased (0.862747 --> 0.862593).  Saving model ...\n",
            "Validation loss decreased (0.862593 --> 0.862438).  Saving model ...\n",
            "Validation loss decreased (0.862438 --> 0.862285).  Saving model ...\n",
            "Validation loss decreased (0.862285 --> 0.862130).  Saving model ...\n",
            "Epoch: 290 \tTraining Loss: 0.880961 \tValidation Loss: 0.861976\n",
            "Validation loss decreased (0.862130 --> 0.861976).  Saving model ...\n",
            "Validation loss decreased (0.861976 --> 0.861821).  Saving model ...\n",
            "Validation loss decreased (0.861821 --> 0.861666).  Saving model ...\n",
            "Validation loss decreased (0.861666 --> 0.861512).  Saving model ...\n",
            "Validation loss decreased (0.861512 --> 0.861358).  Saving model ...\n",
            "Validation loss decreased (0.861358 --> 0.861204).  Saving model ...\n",
            "Validation loss decreased (0.861204 --> 0.861050).  Saving model ...\n",
            "Validation loss decreased (0.861050 --> 0.860895).  Saving model ...\n",
            "Validation loss decreased (0.860895 --> 0.860741).  Saving model ...\n",
            "Validation loss decreased (0.860741 --> 0.860585).  Saving model ...\n",
            "Epoch: 300 \tTraining Loss: 0.881018 \tValidation Loss: 0.860431\n",
            "Validation loss decreased (0.860585 --> 0.860431).  Saving model ...\n",
            "Validation loss decreased (0.860431 --> 0.860277).  Saving model ...\n",
            "Validation loss decreased (0.860277 --> 0.860124).  Saving model ...\n",
            "Validation loss decreased (0.860124 --> 0.859969).  Saving model ...\n",
            "Validation loss decreased (0.859969 --> 0.859815).  Saving model ...\n",
            "Validation loss decreased (0.859815 --> 0.859660).  Saving model ...\n",
            "Validation loss decreased (0.859660 --> 0.859505).  Saving model ...\n",
            "Validation loss decreased (0.859505 --> 0.859351).  Saving model ...\n",
            "Validation loss decreased (0.859351 --> 0.859196).  Saving model ...\n",
            "Validation loss decreased (0.859196 --> 0.859041).  Saving model ...\n",
            "Epoch: 310 \tTraining Loss: 0.873064 \tValidation Loss: 0.858887\n",
            "Validation loss decreased (0.859041 --> 0.858887).  Saving model ...\n",
            "Validation loss decreased (0.858887 --> 0.858733).  Saving model ...\n",
            "Validation loss decreased (0.858733 --> 0.858578).  Saving model ...\n",
            "Validation loss decreased (0.858578 --> 0.858423).  Saving model ...\n",
            "Validation loss decreased (0.858423 --> 0.858269).  Saving model ...\n",
            "Validation loss decreased (0.858269 --> 0.858114).  Saving model ...\n",
            "Validation loss decreased (0.858114 --> 0.857959).  Saving model ...\n",
            "Validation loss decreased (0.857959 --> 0.857804).  Saving model ...\n",
            "Validation loss decreased (0.857804 --> 0.857649).  Saving model ...\n",
            "Validation loss decreased (0.857649 --> 0.857494).  Saving model ...\n",
            "Epoch: 320 \tTraining Loss: 0.879009 \tValidation Loss: 0.857340\n",
            "Validation loss decreased (0.857494 --> 0.857340).  Saving model ...\n",
            "Validation loss decreased (0.857340 --> 0.857186).  Saving model ...\n",
            "Validation loss decreased (0.857186 --> 0.857031).  Saving model ...\n",
            "Validation loss decreased (0.857031 --> 0.856877).  Saving model ...\n",
            "Validation loss decreased (0.856877 --> 0.856722).  Saving model ...\n",
            "Validation loss decreased (0.856722 --> 0.856568).  Saving model ...\n",
            "Validation loss decreased (0.856568 --> 0.856413).  Saving model ...\n",
            "Validation loss decreased (0.856413 --> 0.856258).  Saving model ...\n",
            "Validation loss decreased (0.856258 --> 0.856103).  Saving model ...\n",
            "Validation loss decreased (0.856103 --> 0.855948).  Saving model ...\n",
            "Epoch: 330 \tTraining Loss: 0.872190 \tValidation Loss: 0.855794\n",
            "Validation loss decreased (0.855948 --> 0.855794).  Saving model ...\n",
            "Validation loss decreased (0.855794 --> 0.855640).  Saving model ...\n",
            "Validation loss decreased (0.855640 --> 0.855485).  Saving model ...\n",
            "Validation loss decreased (0.855485 --> 0.855331).  Saving model ...\n",
            "Validation loss decreased (0.855331 --> 0.855176).  Saving model ...\n",
            "Validation loss decreased (0.855176 --> 0.855021).  Saving model ...\n",
            "Validation loss decreased (0.855021 --> 0.854866).  Saving model ...\n",
            "Validation loss decreased (0.854866 --> 0.854712).  Saving model ...\n",
            "Validation loss decreased (0.854712 --> 0.854556).  Saving model ...\n",
            "Validation loss decreased (0.854556 --> 0.854401).  Saving model ...\n",
            "Epoch: 340 \tTraining Loss: 0.873036 \tValidation Loss: 0.854247\n",
            "Validation loss decreased (0.854401 --> 0.854247).  Saving model ...\n",
            "Validation loss decreased (0.854247 --> 0.854092).  Saving model ...\n",
            "Validation loss decreased (0.854092 --> 0.853937).  Saving model ...\n",
            "Validation loss decreased (0.853937 --> 0.853784).  Saving model ...\n",
            "Validation loss decreased (0.853784 --> 0.853630).  Saving model ...\n",
            "Validation loss decreased (0.853630 --> 0.853476).  Saving model ...\n",
            "Validation loss decreased (0.853476 --> 0.853322).  Saving model ...\n",
            "Validation loss decreased (0.853322 --> 0.853168).  Saving model ...\n",
            "Validation loss decreased (0.853168 --> 0.853013).  Saving model ...\n",
            "Validation loss decreased (0.853013 --> 0.852859).  Saving model ...\n",
            "Epoch: 350 \tTraining Loss: 0.866708 \tValidation Loss: 0.852704\n",
            "Validation loss decreased (0.852859 --> 0.852704).  Saving model ...\n",
            "Validation loss decreased (0.852704 --> 0.852550).  Saving model ...\n",
            "Validation loss decreased (0.852550 --> 0.852395).  Saving model ...\n",
            "Validation loss decreased (0.852395 --> 0.852241).  Saving model ...\n",
            "Validation loss decreased (0.852241 --> 0.852086).  Saving model ...\n",
            "Validation loss decreased (0.852086 --> 0.851931).  Saving model ...\n",
            "Validation loss decreased (0.851931 --> 0.851777).  Saving model ...\n",
            "Validation loss decreased (0.851777 --> 0.851623).  Saving model ...\n",
            "Validation loss decreased (0.851623 --> 0.851467).  Saving model ...\n",
            "Validation loss decreased (0.851467 --> 0.851311).  Saving model ...\n",
            "Epoch: 360 \tTraining Loss: 0.870840 \tValidation Loss: 0.851157\n",
            "Validation loss decreased (0.851311 --> 0.851157).  Saving model ...\n",
            "Validation loss decreased (0.851157 --> 0.851002).  Saving model ...\n",
            "Validation loss decreased (0.851002 --> 0.850848).  Saving model ...\n",
            "Validation loss decreased (0.850848 --> 0.850694).  Saving model ...\n",
            "Validation loss decreased (0.850694 --> 0.850539).  Saving model ...\n",
            "Validation loss decreased (0.850539 --> 0.850384).  Saving model ...\n",
            "Validation loss decreased (0.850384 --> 0.850229).  Saving model ...\n",
            "Validation loss decreased (0.850229 --> 0.850075).  Saving model ...\n",
            "Validation loss decreased (0.850075 --> 0.849920).  Saving model ...\n",
            "Validation loss decreased (0.849920 --> 0.849766).  Saving model ...\n",
            "Epoch: 370 \tTraining Loss: 0.863265 \tValidation Loss: 0.849611\n",
            "Validation loss decreased (0.849766 --> 0.849611).  Saving model ...\n",
            "Validation loss decreased (0.849611 --> 0.849457).  Saving model ...\n",
            "Validation loss decreased (0.849457 --> 0.849301).  Saving model ...\n",
            "Validation loss decreased (0.849301 --> 0.849147).  Saving model ...\n",
            "Validation loss decreased (0.849147 --> 0.848992).  Saving model ...\n",
            "Validation loss decreased (0.848992 --> 0.848837).  Saving model ...\n",
            "Validation loss decreased (0.848837 --> 0.848682).  Saving model ...\n",
            "Validation loss decreased (0.848682 --> 0.848528).  Saving model ...\n",
            "Validation loss decreased (0.848528 --> 0.848373).  Saving model ...\n",
            "Validation loss decreased (0.848373 --> 0.848218).  Saving model ...\n",
            "Epoch: 380 \tTraining Loss: 0.863853 \tValidation Loss: 0.848064\n",
            "Validation loss decreased (0.848218 --> 0.848064).  Saving model ...\n",
            "Validation loss decreased (0.848064 --> 0.847908).  Saving model ...\n",
            "Validation loss decreased (0.847908 --> 0.847755).  Saving model ...\n",
            "Validation loss decreased (0.847755 --> 0.847601).  Saving model ...\n",
            "Validation loss decreased (0.847601 --> 0.847447).  Saving model ...\n",
            "Validation loss decreased (0.847447 --> 0.847293).  Saving model ...\n",
            "Validation loss decreased (0.847293 --> 0.847138).  Saving model ...\n",
            "Validation loss decreased (0.847138 --> 0.846984).  Saving model ...\n",
            "Validation loss decreased (0.846984 --> 0.846830).  Saving model ...\n",
            "Validation loss decreased (0.846830 --> 0.846675).  Saving model ...\n",
            "Epoch: 390 \tTraining Loss: 0.863793 \tValidation Loss: 0.846521\n",
            "Validation loss decreased (0.846675 --> 0.846521).  Saving model ...\n",
            "Validation loss decreased (0.846521 --> 0.846366).  Saving model ...\n",
            "Validation loss decreased (0.846366 --> 0.846211).  Saving model ...\n",
            "Validation loss decreased (0.846211 --> 0.846058).  Saving model ...\n",
            "Validation loss decreased (0.846058 --> 0.845903).  Saving model ...\n",
            "Validation loss decreased (0.845903 --> 0.845748).  Saving model ...\n",
            "Validation loss decreased (0.845748 --> 0.845595).  Saving model ...\n",
            "Validation loss decreased (0.845595 --> 0.845441).  Saving model ...\n",
            "Validation loss decreased (0.845441 --> 0.845286).  Saving model ...\n",
            "Validation loss decreased (0.845286 --> 0.845131).  Saving model ...\n",
            "Epoch: 400 \tTraining Loss: 0.857482 \tValidation Loss: 0.844977\n",
            "Validation loss decreased (0.845131 --> 0.844977).  Saving model ...\n",
            "Validation loss decreased (0.844977 --> 0.844823).  Saving model ...\n",
            "Validation loss decreased (0.844823 --> 0.844668).  Saving model ...\n",
            "Validation loss decreased (0.844668 --> 0.844513).  Saving model ...\n",
            "Validation loss decreased (0.844513 --> 0.844359).  Saving model ...\n",
            "Validation loss decreased (0.844359 --> 0.844204).  Saving model ...\n",
            "Validation loss decreased (0.844204 --> 0.844048).  Saving model ...\n",
            "Validation loss decreased (0.844048 --> 0.843894).  Saving model ...\n",
            "Validation loss decreased (0.843894 --> 0.843740).  Saving model ...\n",
            "Validation loss decreased (0.843740 --> 0.843585).  Saving model ...\n",
            "Epoch: 410 \tTraining Loss: 0.861767 \tValidation Loss: 0.843432\n",
            "Validation loss decreased (0.843585 --> 0.843432).  Saving model ...\n",
            "Validation loss decreased (0.843432 --> 0.843278).  Saving model ...\n",
            "Validation loss decreased (0.843278 --> 0.843122).  Saving model ...\n",
            "Validation loss decreased (0.843122 --> 0.842967).  Saving model ...\n",
            "Validation loss decreased (0.842967 --> 0.842812).  Saving model ...\n",
            "Validation loss decreased (0.842812 --> 0.842657).  Saving model ...\n",
            "Validation loss decreased (0.842657 --> 0.842503).  Saving model ...\n",
            "Validation loss decreased (0.842503 --> 0.842348).  Saving model ...\n",
            "Validation loss decreased (0.842348 --> 0.842194).  Saving model ...\n",
            "Validation loss decreased (0.842194 --> 0.842039).  Saving model ...\n",
            "Epoch: 420 \tTraining Loss: 0.858686 \tValidation Loss: 0.841885\n",
            "Validation loss decreased (0.842039 --> 0.841885).  Saving model ...\n",
            "Validation loss decreased (0.841885 --> 0.841730).  Saving model ...\n",
            "Validation loss decreased (0.841730 --> 0.841576).  Saving model ...\n",
            "Validation loss decreased (0.841576 --> 0.841422).  Saving model ...\n",
            "Validation loss decreased (0.841422 --> 0.841268).  Saving model ...\n",
            "Validation loss decreased (0.841268 --> 0.841113).  Saving model ...\n",
            "Validation loss decreased (0.841113 --> 0.840958).  Saving model ...\n",
            "Validation loss decreased (0.840958 --> 0.840803).  Saving model ...\n",
            "Validation loss decreased (0.840803 --> 0.840649).  Saving model ...\n",
            "Validation loss decreased (0.840649 --> 0.840494).  Saving model ...\n",
            "Epoch: 430 \tTraining Loss: 0.855589 \tValidation Loss: 0.840339\n",
            "Validation loss decreased (0.840494 --> 0.840339).  Saving model ...\n",
            "Validation loss decreased (0.840339 --> 0.840184).  Saving model ...\n",
            "Validation loss decreased (0.840184 --> 0.840030).  Saving model ...\n",
            "Validation loss decreased (0.840030 --> 0.839876).  Saving model ...\n",
            "Validation loss decreased (0.839876 --> 0.839722).  Saving model ...\n",
            "Validation loss decreased (0.839722 --> 0.839567).  Saving model ...\n",
            "Validation loss decreased (0.839567 --> 0.839412).  Saving model ...\n",
            "Validation loss decreased (0.839412 --> 0.839259).  Saving model ...\n",
            "Validation loss decreased (0.839259 --> 0.839104).  Saving model ...\n",
            "Validation loss decreased (0.839104 --> 0.838950).  Saving model ...\n",
            "Epoch: 440 \tTraining Loss: 0.857144 \tValidation Loss: 0.838795\n",
            "Validation loss decreased (0.838950 --> 0.838795).  Saving model ...\n",
            "Validation loss decreased (0.838795 --> 0.838641).  Saving model ...\n",
            "Validation loss decreased (0.838641 --> 0.838486).  Saving model ...\n",
            "Validation loss decreased (0.838486 --> 0.838330).  Saving model ...\n",
            "Validation loss decreased (0.838330 --> 0.838176).  Saving model ...\n",
            "Validation loss decreased (0.838176 --> 0.838021).  Saving model ...\n",
            "Validation loss decreased (0.838021 --> 0.837867).  Saving model ...\n",
            "Validation loss decreased (0.837867 --> 0.837712).  Saving model ...\n",
            "Validation loss decreased (0.837712 --> 0.837557).  Saving model ...\n",
            "Validation loss decreased (0.837557 --> 0.837403).  Saving model ...\n",
            "Epoch: 450 \tTraining Loss: 0.856741 \tValidation Loss: 0.837249\n",
            "Validation loss decreased (0.837403 --> 0.837249).  Saving model ...\n",
            "Validation loss decreased (0.837249 --> 0.837094).  Saving model ...\n",
            "Validation loss decreased (0.837094 --> 0.836940).  Saving model ...\n",
            "Validation loss decreased (0.836940 --> 0.836785).  Saving model ...\n",
            "Validation loss decreased (0.836785 --> 0.836631).  Saving model ...\n",
            "Validation loss decreased (0.836631 --> 0.836478).  Saving model ...\n",
            "Validation loss decreased (0.836478 --> 0.836323).  Saving model ...\n",
            "Validation loss decreased (0.836323 --> 0.836169).  Saving model ...\n",
            "Validation loss decreased (0.836169 --> 0.836013).  Saving model ...\n",
            "Validation loss decreased (0.836013 --> 0.835859).  Saving model ...\n",
            "Epoch: 460 \tTraining Loss: 0.854733 \tValidation Loss: 0.835705\n",
            "Validation loss decreased (0.835859 --> 0.835705).  Saving model ...\n",
            "Validation loss decreased (0.835705 --> 0.835551).  Saving model ...\n",
            "Validation loss decreased (0.835551 --> 0.835397).  Saving model ...\n",
            "Validation loss decreased (0.835397 --> 0.835242).  Saving model ...\n",
            "Validation loss decreased (0.835242 --> 0.835087).  Saving model ...\n",
            "Validation loss decreased (0.835087 --> 0.834933).  Saving model ...\n",
            "Validation loss decreased (0.834933 --> 0.834778).  Saving model ...\n",
            "Validation loss decreased (0.834778 --> 0.834625).  Saving model ...\n",
            "Validation loss decreased (0.834625 --> 0.834469).  Saving model ...\n",
            "Validation loss decreased (0.834469 --> 0.834315).  Saving model ...\n",
            "Epoch: 470 \tTraining Loss: 0.852310 \tValidation Loss: 0.834159\n",
            "Validation loss decreased (0.834315 --> 0.834159).  Saving model ...\n",
            "Validation loss decreased (0.834159 --> 0.834004).  Saving model ...\n",
            "Validation loss decreased (0.834004 --> 0.833851).  Saving model ...\n",
            "Validation loss decreased (0.833851 --> 0.833696).  Saving model ...\n",
            "Validation loss decreased (0.833696 --> 0.833542).  Saving model ...\n",
            "Validation loss decreased (0.833542 --> 0.833387).  Saving model ...\n",
            "Validation loss decreased (0.833387 --> 0.833232).  Saving model ...\n",
            "Validation loss decreased (0.833232 --> 0.833078).  Saving model ...\n",
            "Validation loss decreased (0.833078 --> 0.832925).  Saving model ...\n",
            "Validation loss decreased (0.832925 --> 0.832770).  Saving model ...\n",
            "Epoch: 480 \tTraining Loss: 0.863262 \tValidation Loss: 0.832617\n",
            "Validation loss decreased (0.832770 --> 0.832617).  Saving model ...\n",
            "Validation loss decreased (0.832617 --> 0.832461).  Saving model ...\n",
            "Validation loss decreased (0.832461 --> 0.832306).  Saving model ...\n",
            "Validation loss decreased (0.832306 --> 0.832152).  Saving model ...\n",
            "Validation loss decreased (0.832152 --> 0.831998).  Saving model ...\n",
            "Validation loss decreased (0.831998 --> 0.831844).  Saving model ...\n",
            "Validation loss decreased (0.831844 --> 0.831690).  Saving model ...\n",
            "Validation loss decreased (0.831690 --> 0.831535).  Saving model ...\n",
            "Validation loss decreased (0.831535 --> 0.831380).  Saving model ...\n",
            "Validation loss decreased (0.831380 --> 0.831226).  Saving model ...\n",
            "Epoch: 490 \tTraining Loss: 0.849427 \tValidation Loss: 0.831071\n",
            "Validation loss decreased (0.831226 --> 0.831071).  Saving model ...\n",
            "Validation loss decreased (0.831071 --> 0.830916).  Saving model ...\n",
            "Validation loss decreased (0.830916 --> 0.830761).  Saving model ...\n",
            "Validation loss decreased (0.830761 --> 0.830606).  Saving model ...\n",
            "Validation loss decreased (0.830606 --> 0.830451).  Saving model ...\n",
            "Validation loss decreased (0.830451 --> 0.830296).  Saving model ...\n",
            "Validation loss decreased (0.830296 --> 0.830142).  Saving model ...\n",
            "Validation loss decreased (0.830142 --> 0.829987).  Saving model ...\n",
            "Validation loss decreased (0.829987 --> 0.829834).  Saving model ...\n",
            "Validation loss decreased (0.829834 --> 0.829678).  Saving model ...\n",
            "Epoch: 500 \tTraining Loss: 0.850081 \tValidation Loss: 0.829524\n",
            "Validation loss decreased (0.829678 --> 0.829524).  Saving model ...\n",
            "Validation loss decreased (0.829524 --> 0.829369).  Saving model ...\n",
            "Validation loss decreased (0.829369 --> 0.829214).  Saving model ...\n",
            "Validation loss decreased (0.829214 --> 0.829059).  Saving model ...\n",
            "Validation loss decreased (0.829059 --> 0.828905).  Saving model ...\n",
            "Validation loss decreased (0.828905 --> 0.828750).  Saving model ...\n",
            "Validation loss decreased (0.828750 --> 0.828595).  Saving model ...\n",
            "Validation loss decreased (0.828595 --> 0.828441).  Saving model ...\n",
            "Validation loss decreased (0.828441 --> 0.828287).  Saving model ...\n",
            "Validation loss decreased (0.828287 --> 0.828131).  Saving model ...\n",
            "Epoch: 510 \tTraining Loss: 0.848419 \tValidation Loss: 0.827976\n",
            "Validation loss decreased (0.828131 --> 0.827976).  Saving model ...\n",
            "Validation loss decreased (0.827976 --> 0.827822).  Saving model ...\n",
            "Validation loss decreased (0.827822 --> 0.827666).  Saving model ...\n",
            "Validation loss decreased (0.827666 --> 0.827511).  Saving model ...\n",
            "Validation loss decreased (0.827511 --> 0.827355).  Saving model ...\n",
            "Validation loss decreased (0.827355 --> 0.827201).  Saving model ...\n",
            "Validation loss decreased (0.827201 --> 0.827047).  Saving model ...\n",
            "Validation loss decreased (0.827047 --> 0.826892).  Saving model ...\n",
            "Validation loss decreased (0.826892 --> 0.826737).  Saving model ...\n",
            "Validation loss decreased (0.826737 --> 0.826582).  Saving model ...\n",
            "Epoch: 520 \tTraining Loss: 0.840233 \tValidation Loss: 0.826427\n",
            "Validation loss decreased (0.826582 --> 0.826427).  Saving model ...\n",
            "Validation loss decreased (0.826427 --> 0.826274).  Saving model ...\n",
            "Validation loss decreased (0.826274 --> 0.826119).  Saving model ...\n",
            "Validation loss decreased (0.826119 --> 0.825964).  Saving model ...\n",
            "Validation loss decreased (0.825964 --> 0.825809).  Saving model ...\n",
            "Validation loss decreased (0.825809 --> 0.825654).  Saving model ...\n",
            "Validation loss decreased (0.825654 --> 0.825500).  Saving model ...\n",
            "Validation loss decreased (0.825500 --> 0.825345).  Saving model ...\n",
            "Validation loss decreased (0.825345 --> 0.825191).  Saving model ...\n",
            "Validation loss decreased (0.825191 --> 0.825036).  Saving model ...\n",
            "Epoch: 530 \tTraining Loss: 0.839369 \tValidation Loss: 0.824881\n",
            "Validation loss decreased (0.825036 --> 0.824881).  Saving model ...\n",
            "Validation loss decreased (0.824881 --> 0.824727).  Saving model ...\n",
            "Validation loss decreased (0.824727 --> 0.824572).  Saving model ...\n",
            "Validation loss decreased (0.824572 --> 0.824417).  Saving model ...\n",
            "Validation loss decreased (0.824417 --> 0.824262).  Saving model ...\n",
            "Validation loss decreased (0.824262 --> 0.824107).  Saving model ...\n",
            "Validation loss decreased (0.824107 --> 0.823953).  Saving model ...\n",
            "Validation loss decreased (0.823953 --> 0.823799).  Saving model ...\n",
            "Validation loss decreased (0.823799 --> 0.823643).  Saving model ...\n",
            "Validation loss decreased (0.823643 --> 0.823487).  Saving model ...\n",
            "Epoch: 540 \tTraining Loss: 0.841600 \tValidation Loss: 0.823332\n",
            "Validation loss decreased (0.823487 --> 0.823332).  Saving model ...\n",
            "Validation loss decreased (0.823332 --> 0.823177).  Saving model ...\n",
            "Validation loss decreased (0.823177 --> 0.823022).  Saving model ...\n",
            "Validation loss decreased (0.823022 --> 0.822866).  Saving model ...\n",
            "Validation loss decreased (0.822866 --> 0.822712).  Saving model ...\n",
            "Validation loss decreased (0.822712 --> 0.822557).  Saving model ...\n",
            "Validation loss decreased (0.822557 --> 0.822401).  Saving model ...\n",
            "Validation loss decreased (0.822401 --> 0.822246).  Saving model ...\n",
            "Validation loss decreased (0.822246 --> 0.822090).  Saving model ...\n",
            "Validation loss decreased (0.822090 --> 0.821936).  Saving model ...\n",
            "Epoch: 550 \tTraining Loss: 0.843898 \tValidation Loss: 0.821780\n",
            "Validation loss decreased (0.821936 --> 0.821780).  Saving model ...\n",
            "Validation loss decreased (0.821780 --> 0.821625).  Saving model ...\n",
            "Validation loss decreased (0.821625 --> 0.821470).  Saving model ...\n",
            "Validation loss decreased (0.821470 --> 0.821316).  Saving model ...\n",
            "Validation loss decreased (0.821316 --> 0.821159).  Saving model ...\n",
            "Validation loss decreased (0.821159 --> 0.821003).  Saving model ...\n",
            "Validation loss decreased (0.821003 --> 0.820849).  Saving model ...\n",
            "Validation loss decreased (0.820849 --> 0.820694).  Saving model ...\n",
            "Validation loss decreased (0.820694 --> 0.820538).  Saving model ...\n",
            "Validation loss decreased (0.820538 --> 0.820383).  Saving model ...\n",
            "Epoch: 560 \tTraining Loss: 0.844070 \tValidation Loss: 0.820228\n",
            "Validation loss decreased (0.820383 --> 0.820228).  Saving model ...\n",
            "Validation loss decreased (0.820228 --> 0.820072).  Saving model ...\n",
            "Validation loss decreased (0.820072 --> 0.819917).  Saving model ...\n",
            "Validation loss decreased (0.819917 --> 0.819761).  Saving model ...\n",
            "Validation loss decreased (0.819761 --> 0.819606).  Saving model ...\n",
            "Validation loss decreased (0.819606 --> 0.819452).  Saving model ...\n",
            "Validation loss decreased (0.819452 --> 0.819297).  Saving model ...\n",
            "Validation loss decreased (0.819297 --> 0.819144).  Saving model ...\n",
            "Validation loss decreased (0.819144 --> 0.818989).  Saving model ...\n",
            "Validation loss decreased (0.818989 --> 0.818833).  Saving model ...\n",
            "Epoch: 570 \tTraining Loss: 0.840936 \tValidation Loss: 0.818678\n",
            "Validation loss decreased (0.818833 --> 0.818678).  Saving model ...\n",
            "Validation loss decreased (0.818678 --> 0.818523).  Saving model ...\n",
            "Validation loss decreased (0.818523 --> 0.818369).  Saving model ...\n",
            "Validation loss decreased (0.818369 --> 0.818213).  Saving model ...\n",
            "Validation loss decreased (0.818213 --> 0.818059).  Saving model ...\n",
            "Validation loss decreased (0.818059 --> 0.817904).  Saving model ...\n",
            "Validation loss decreased (0.817904 --> 0.817749).  Saving model ...\n",
            "Validation loss decreased (0.817749 --> 0.817593).  Saving model ...\n",
            "Validation loss decreased (0.817593 --> 0.817439).  Saving model ...\n",
            "Validation loss decreased (0.817439 --> 0.817285).  Saving model ...\n",
            "Epoch: 580 \tTraining Loss: 0.838899 \tValidation Loss: 0.817130\n",
            "Validation loss decreased (0.817285 --> 0.817130).  Saving model ...\n",
            "Validation loss decreased (0.817130 --> 0.816975).  Saving model ...\n",
            "Validation loss decreased (0.816975 --> 0.816821).  Saving model ...\n",
            "Validation loss decreased (0.816821 --> 0.816666).  Saving model ...\n",
            "Validation loss decreased (0.816666 --> 0.816511).  Saving model ...\n",
            "Validation loss decreased (0.816511 --> 0.816357).  Saving model ...\n",
            "Validation loss decreased (0.816357 --> 0.816202).  Saving model ...\n",
            "Validation loss decreased (0.816202 --> 0.816047).  Saving model ...\n",
            "Validation loss decreased (0.816047 --> 0.815893).  Saving model ...\n",
            "Validation loss decreased (0.815893 --> 0.815738).  Saving model ...\n",
            "Epoch: 590 \tTraining Loss: 0.831544 \tValidation Loss: 0.815582\n",
            "Validation loss decreased (0.815738 --> 0.815582).  Saving model ...\n",
            "Validation loss decreased (0.815582 --> 0.815428).  Saving model ...\n",
            "Validation loss decreased (0.815428 --> 0.815274).  Saving model ...\n",
            "Validation loss decreased (0.815274 --> 0.815119).  Saving model ...\n",
            "Validation loss decreased (0.815119 --> 0.814964).  Saving model ...\n",
            "Validation loss decreased (0.814964 --> 0.814809).  Saving model ...\n",
            "Validation loss decreased (0.814809 --> 0.814654).  Saving model ...\n",
            "Validation loss decreased (0.814654 --> 0.814500).  Saving model ...\n",
            "Validation loss decreased (0.814500 --> 0.814345).  Saving model ...\n",
            "Validation loss decreased (0.814345 --> 0.814190).  Saving model ...\n",
            "Epoch: 600 \tTraining Loss: 0.842619 \tValidation Loss: 0.814036\n",
            "Validation loss decreased (0.814190 --> 0.814036).  Saving model ...\n",
            "Validation loss decreased (0.814036 --> 0.813879).  Saving model ...\n",
            "Validation loss decreased (0.813879 --> 0.813725).  Saving model ...\n",
            "Validation loss decreased (0.813725 --> 0.813570).  Saving model ...\n",
            "Validation loss decreased (0.813570 --> 0.813415).  Saving model ...\n",
            "Validation loss decreased (0.813415 --> 0.813261).  Saving model ...\n",
            "Validation loss decreased (0.813261 --> 0.813105).  Saving model ...\n",
            "Validation loss decreased (0.813105 --> 0.812950).  Saving model ...\n",
            "Validation loss decreased (0.812950 --> 0.812795).  Saving model ...\n",
            "Validation loss decreased (0.812795 --> 0.812641).  Saving model ...\n",
            "Epoch: 610 \tTraining Loss: 0.833185 \tValidation Loss: 0.812486\n",
            "Validation loss decreased (0.812641 --> 0.812486).  Saving model ...\n",
            "Validation loss decreased (0.812486 --> 0.812330).  Saving model ...\n",
            "Validation loss decreased (0.812330 --> 0.812175).  Saving model ...\n",
            "Validation loss decreased (0.812175 --> 0.812020).  Saving model ...\n",
            "Validation loss decreased (0.812020 --> 0.811865).  Saving model ...\n",
            "Validation loss decreased (0.811865 --> 0.811709).  Saving model ...\n",
            "Validation loss decreased (0.811709 --> 0.811555).  Saving model ...\n",
            "Validation loss decreased (0.811555 --> 0.811399).  Saving model ...\n",
            "Validation loss decreased (0.811399 --> 0.811244).  Saving model ...\n",
            "Validation loss decreased (0.811244 --> 0.811088).  Saving model ...\n",
            "Epoch: 620 \tTraining Loss: 0.831537 \tValidation Loss: 0.810933\n",
            "Validation loss decreased (0.811088 --> 0.810933).  Saving model ...\n",
            "Validation loss decreased (0.810933 --> 0.810777).  Saving model ...\n",
            "Validation loss decreased (0.810777 --> 0.810621).  Saving model ...\n",
            "Validation loss decreased (0.810621 --> 0.810466).  Saving model ...\n",
            "Validation loss decreased (0.810466 --> 0.810311).  Saving model ...\n",
            "Validation loss decreased (0.810311 --> 0.810156).  Saving model ...\n",
            "Validation loss decreased (0.810156 --> 0.810001).  Saving model ...\n",
            "Validation loss decreased (0.810001 --> 0.809845).  Saving model ...\n",
            "Validation loss decreased (0.809845 --> 0.809691).  Saving model ...\n",
            "Validation loss decreased (0.809691 --> 0.809536).  Saving model ...\n",
            "Epoch: 630 \tTraining Loss: 0.831570 \tValidation Loss: 0.809380\n",
            "Validation loss decreased (0.809536 --> 0.809380).  Saving model ...\n",
            "Validation loss decreased (0.809380 --> 0.809225).  Saving model ...\n",
            "Validation loss decreased (0.809225 --> 0.809070).  Saving model ...\n",
            "Validation loss decreased (0.809070 --> 0.808914).  Saving model ...\n",
            "Validation loss decreased (0.808914 --> 0.808757).  Saving model ...\n",
            "Validation loss decreased (0.808757 --> 0.808601).  Saving model ...\n",
            "Validation loss decreased (0.808601 --> 0.808446).  Saving model ...\n",
            "Validation loss decreased (0.808446 --> 0.808290).  Saving model ...\n",
            "Validation loss decreased (0.808290 --> 0.808135).  Saving model ...\n",
            "Validation loss decreased (0.808135 --> 0.807979).  Saving model ...\n",
            "Epoch: 640 \tTraining Loss: 0.829156 \tValidation Loss: 0.807824\n",
            "Validation loss decreased (0.807979 --> 0.807824).  Saving model ...\n",
            "Validation loss decreased (0.807824 --> 0.807669).  Saving model ...\n",
            "Validation loss decreased (0.807669 --> 0.807514).  Saving model ...\n",
            "Validation loss decreased (0.807514 --> 0.807359).  Saving model ...\n",
            "Validation loss decreased (0.807359 --> 0.807204).  Saving model ...\n",
            "Validation loss decreased (0.807204 --> 0.807048).  Saving model ...\n",
            "Validation loss decreased (0.807048 --> 0.806892).  Saving model ...\n",
            "Validation loss decreased (0.806892 --> 0.806736).  Saving model ...\n",
            "Validation loss decreased (0.806736 --> 0.806582).  Saving model ...\n",
            "Validation loss decreased (0.806582 --> 0.806426).  Saving model ...\n",
            "Epoch: 650 \tTraining Loss: 0.825044 \tValidation Loss: 0.806270\n",
            "Validation loss decreased (0.806426 --> 0.806270).  Saving model ...\n",
            "Validation loss decreased (0.806270 --> 0.806114).  Saving model ...\n",
            "Validation loss decreased (0.806114 --> 0.805959).  Saving model ...\n",
            "Validation loss decreased (0.805959 --> 0.805804).  Saving model ...\n",
            "Validation loss decreased (0.805804 --> 0.805648).  Saving model ...\n",
            "Validation loss decreased (0.805648 --> 0.805492).  Saving model ...\n",
            "Validation loss decreased (0.805492 --> 0.805339).  Saving model ...\n",
            "Validation loss decreased (0.805339 --> 0.805182).  Saving model ...\n",
            "Validation loss decreased (0.805182 --> 0.805028).  Saving model ...\n",
            "Validation loss decreased (0.805028 --> 0.804871).  Saving model ...\n",
            "Epoch: 660 \tTraining Loss: 0.823463 \tValidation Loss: 0.804715\n",
            "Validation loss decreased (0.804871 --> 0.804715).  Saving model ...\n",
            "Validation loss decreased (0.804715 --> 0.804560).  Saving model ...\n",
            "Validation loss decreased (0.804560 --> 0.804404).  Saving model ...\n",
            "Validation loss decreased (0.804404 --> 0.804249).  Saving model ...\n",
            "Validation loss decreased (0.804249 --> 0.804093).  Saving model ...\n",
            "Validation loss decreased (0.804093 --> 0.803938).  Saving model ...\n",
            "Validation loss decreased (0.803938 --> 0.803783).  Saving model ...\n",
            "Validation loss decreased (0.803783 --> 0.803628).  Saving model ...\n",
            "Validation loss decreased (0.803628 --> 0.803473).  Saving model ...\n",
            "Validation loss decreased (0.803473 --> 0.803318).  Saving model ...\n",
            "Epoch: 670 \tTraining Loss: 0.829251 \tValidation Loss: 0.803163\n",
            "Validation loss decreased (0.803318 --> 0.803163).  Saving model ...\n",
            "Validation loss decreased (0.803163 --> 0.803008).  Saving model ...\n",
            "Validation loss decreased (0.803008 --> 0.802852).  Saving model ...\n",
            "Validation loss decreased (0.802852 --> 0.802697).  Saving model ...\n",
            "Validation loss decreased (0.802697 --> 0.802543).  Saving model ...\n",
            "Validation loss decreased (0.802543 --> 0.802386).  Saving model ...\n",
            "Validation loss decreased (0.802386 --> 0.802232).  Saving model ...\n",
            "Validation loss decreased (0.802232 --> 0.802076).  Saving model ...\n",
            "Validation loss decreased (0.802076 --> 0.801921).  Saving model ...\n",
            "Validation loss decreased (0.801921 --> 0.801765).  Saving model ...\n",
            "Epoch: 680 \tTraining Loss: 0.816332 \tValidation Loss: 0.801609\n",
            "Validation loss decreased (0.801765 --> 0.801609).  Saving model ...\n",
            "Validation loss decreased (0.801609 --> 0.801453).  Saving model ...\n",
            "Validation loss decreased (0.801453 --> 0.801297).  Saving model ...\n",
            "Validation loss decreased (0.801297 --> 0.801141).  Saving model ...\n",
            "Validation loss decreased (0.801141 --> 0.800985).  Saving model ...\n",
            "Validation loss decreased (0.800985 --> 0.800830).  Saving model ...\n",
            "Validation loss decreased (0.800830 --> 0.800673).  Saving model ...\n",
            "Validation loss decreased (0.800673 --> 0.800517).  Saving model ...\n",
            "Validation loss decreased (0.800517 --> 0.800362).  Saving model ...\n",
            "Validation loss decreased (0.800362 --> 0.800206).  Saving model ...\n",
            "Epoch: 690 \tTraining Loss: 0.820699 \tValidation Loss: 0.800050\n",
            "Validation loss decreased (0.800206 --> 0.800050).  Saving model ...\n",
            "Validation loss decreased (0.800050 --> 0.799894).  Saving model ...\n",
            "Validation loss decreased (0.799894 --> 0.799737).  Saving model ...\n",
            "Validation loss decreased (0.799737 --> 0.799582).  Saving model ...\n",
            "Validation loss decreased (0.799582 --> 0.799426).  Saving model ...\n",
            "Validation loss decreased (0.799426 --> 0.799271).  Saving model ...\n",
            "Validation loss decreased (0.799271 --> 0.799115).  Saving model ...\n",
            "Validation loss decreased (0.799115 --> 0.798959).  Saving model ...\n",
            "Validation loss decreased (0.798959 --> 0.798803).  Saving model ...\n",
            "Validation loss decreased (0.798803 --> 0.798646).  Saving model ...\n",
            "Epoch: 700 \tTraining Loss: 0.818947 \tValidation Loss: 0.798490\n",
            "Validation loss decreased (0.798646 --> 0.798490).  Saving model ...\n",
            "Validation loss decreased (0.798490 --> 0.798335).  Saving model ...\n",
            "Validation loss decreased (0.798335 --> 0.798179).  Saving model ...\n",
            "Validation loss decreased (0.798179 --> 0.798023).  Saving model ...\n",
            "Validation loss decreased (0.798023 --> 0.797868).  Saving model ...\n",
            "Validation loss decreased (0.797868 --> 0.797712).  Saving model ...\n",
            "Validation loss decreased (0.797712 --> 0.797555).  Saving model ...\n",
            "Validation loss decreased (0.797555 --> 0.797399).  Saving model ...\n",
            "Validation loss decreased (0.797399 --> 0.797243).  Saving model ...\n",
            "Validation loss decreased (0.797243 --> 0.797087).  Saving model ...\n",
            "Epoch: 710 \tTraining Loss: 0.820880 \tValidation Loss: 0.796932\n",
            "Validation loss decreased (0.797087 --> 0.796932).  Saving model ...\n",
            "Validation loss decreased (0.796932 --> 0.796775).  Saving model ...\n",
            "Validation loss decreased (0.796775 --> 0.796619).  Saving model ...\n",
            "Validation loss decreased (0.796619 --> 0.796463).  Saving model ...\n",
            "Validation loss decreased (0.796463 --> 0.796308).  Saving model ...\n",
            "Validation loss decreased (0.796308 --> 0.796152).  Saving model ...\n",
            "Validation loss decreased (0.796152 --> 0.795997).  Saving model ...\n",
            "Validation loss decreased (0.795997 --> 0.795842).  Saving model ...\n",
            "Validation loss decreased (0.795842 --> 0.795686).  Saving model ...\n",
            "Validation loss decreased (0.795686 --> 0.795530).  Saving model ...\n",
            "Epoch: 720 \tTraining Loss: 0.814084 \tValidation Loss: 0.795374\n",
            "Validation loss decreased (0.795530 --> 0.795374).  Saving model ...\n",
            "Validation loss decreased (0.795374 --> 0.795218).  Saving model ...\n",
            "Validation loss decreased (0.795218 --> 0.795062).  Saving model ...\n",
            "Validation loss decreased (0.795062 --> 0.794907).  Saving model ...\n",
            "Validation loss decreased (0.794907 --> 0.794751).  Saving model ...\n",
            "Validation loss decreased (0.794751 --> 0.794594).  Saving model ...\n",
            "Validation loss decreased (0.794594 --> 0.794437).  Saving model ...\n",
            "Validation loss decreased (0.794437 --> 0.794282).  Saving model ...\n",
            "Validation loss decreased (0.794282 --> 0.794126).  Saving model ...\n",
            "Validation loss decreased (0.794126 --> 0.793971).  Saving model ...\n",
            "Epoch: 730 \tTraining Loss: 0.818197 \tValidation Loss: 0.793816\n",
            "Validation loss decreased (0.793971 --> 0.793816).  Saving model ...\n",
            "Validation loss decreased (0.793816 --> 0.793660).  Saving model ...\n",
            "Validation loss decreased (0.793660 --> 0.793505).  Saving model ...\n",
            "Validation loss decreased (0.793505 --> 0.793349).  Saving model ...\n",
            "Validation loss decreased (0.793349 --> 0.793192).  Saving model ...\n",
            "Validation loss decreased (0.793192 --> 0.793037).  Saving model ...\n",
            "Validation loss decreased (0.793037 --> 0.792881).  Saving model ...\n",
            "Validation loss decreased (0.792881 --> 0.792725).  Saving model ...\n",
            "Validation loss decreased (0.792725 --> 0.792569).  Saving model ...\n",
            "Validation loss decreased (0.792569 --> 0.792413).  Saving model ...\n",
            "Epoch: 740 \tTraining Loss: 0.811490 \tValidation Loss: 0.792258\n",
            "Validation loss decreased (0.792413 --> 0.792258).  Saving model ...\n",
            "Validation loss decreased (0.792258 --> 0.792100).  Saving model ...\n",
            "Validation loss decreased (0.792100 --> 0.791945).  Saving model ...\n",
            "Validation loss decreased (0.791945 --> 0.791790).  Saving model ...\n",
            "Validation loss decreased (0.791790 --> 0.791635).  Saving model ...\n",
            "Validation loss decreased (0.791635 --> 0.791479).  Saving model ...\n",
            "Validation loss decreased (0.791479 --> 0.791323).  Saving model ...\n",
            "Validation loss decreased (0.791323 --> 0.791168).  Saving model ...\n",
            "Validation loss decreased (0.791168 --> 0.791012).  Saving model ...\n",
            "Validation loss decreased (0.791012 --> 0.790856).  Saving model ...\n",
            "Epoch: 750 \tTraining Loss: 0.803794 \tValidation Loss: 0.790699\n",
            "Validation loss decreased (0.790856 --> 0.790699).  Saving model ...\n",
            "Validation loss decreased (0.790699 --> 0.790543).  Saving model ...\n",
            "Validation loss decreased (0.790543 --> 0.790388).  Saving model ...\n",
            "Validation loss decreased (0.790388 --> 0.790232).  Saving model ...\n",
            "Validation loss decreased (0.790232 --> 0.790075).  Saving model ...\n",
            "Validation loss decreased (0.790075 --> 0.789919).  Saving model ...\n",
            "Validation loss decreased (0.789919 --> 0.789763).  Saving model ...\n",
            "Validation loss decreased (0.789763 --> 0.789606).  Saving model ...\n",
            "Validation loss decreased (0.789606 --> 0.789450).  Saving model ...\n",
            "Validation loss decreased (0.789450 --> 0.789293).  Saving model ...\n",
            "Epoch: 760 \tTraining Loss: 0.813073 \tValidation Loss: 0.789137\n",
            "Validation loss decreased (0.789293 --> 0.789137).  Saving model ...\n",
            "Validation loss decreased (0.789137 --> 0.788981).  Saving model ...\n",
            "Validation loss decreased (0.788981 --> 0.788825).  Saving model ...\n",
            "Validation loss decreased (0.788825 --> 0.788669).  Saving model ...\n",
            "Validation loss decreased (0.788669 --> 0.788513).  Saving model ...\n",
            "Validation loss decreased (0.788513 --> 0.788357).  Saving model ...\n",
            "Validation loss decreased (0.788357 --> 0.788200).  Saving model ...\n",
            "Validation loss decreased (0.788200 --> 0.788044).  Saving model ...\n",
            "Validation loss decreased (0.788044 --> 0.787888).  Saving model ...\n",
            "Validation loss decreased (0.787888 --> 0.787732).  Saving model ...\n",
            "Epoch: 770 \tTraining Loss: 0.805936 \tValidation Loss: 0.787575\n",
            "Validation loss decreased (0.787732 --> 0.787575).  Saving model ...\n",
            "Validation loss decreased (0.787575 --> 0.787418).  Saving model ...\n",
            "Validation loss decreased (0.787418 --> 0.787262).  Saving model ...\n",
            "Validation loss decreased (0.787262 --> 0.787106).  Saving model ...\n",
            "Validation loss decreased (0.787106 --> 0.786950).  Saving model ...\n",
            "Validation loss decreased (0.786950 --> 0.786795).  Saving model ...\n",
            "Validation loss decreased (0.786795 --> 0.786638).  Saving model ...\n",
            "Validation loss decreased (0.786638 --> 0.786482).  Saving model ...\n",
            "Validation loss decreased (0.786482 --> 0.786326).  Saving model ...\n",
            "Validation loss decreased (0.786326 --> 0.786170).  Saving model ...\n",
            "Epoch: 780 \tTraining Loss: 0.809397 \tValidation Loss: 0.786013\n",
            "Validation loss decreased (0.786170 --> 0.786013).  Saving model ...\n",
            "Validation loss decreased (0.786013 --> 0.785857).  Saving model ...\n",
            "Validation loss decreased (0.785857 --> 0.785700).  Saving model ...\n",
            "Validation loss decreased (0.785700 --> 0.785544).  Saving model ...\n",
            "Validation loss decreased (0.785544 --> 0.785388).  Saving model ...\n",
            "Validation loss decreased (0.785388 --> 0.785233).  Saving model ...\n",
            "Validation loss decreased (0.785233 --> 0.785076).  Saving model ...\n",
            "Validation loss decreased (0.785076 --> 0.784920).  Saving model ...\n",
            "Validation loss decreased (0.784920 --> 0.784763).  Saving model ...\n",
            "Validation loss decreased (0.784763 --> 0.784604).  Saving model ...\n",
            "Epoch: 790 \tTraining Loss: 0.805573 \tValidation Loss: 0.784448\n",
            "Validation loss decreased (0.784604 --> 0.784448).  Saving model ...\n",
            "Validation loss decreased (0.784448 --> 0.784292).  Saving model ...\n",
            "Validation loss decreased (0.784292 --> 0.784136).  Saving model ...\n",
            "Validation loss decreased (0.784136 --> 0.783981).  Saving model ...\n",
            "Validation loss decreased (0.783981 --> 0.783823).  Saving model ...\n",
            "Validation loss decreased (0.783823 --> 0.783668).  Saving model ...\n",
            "Validation loss decreased (0.783668 --> 0.783511).  Saving model ...\n",
            "Validation loss decreased (0.783511 --> 0.783356).  Saving model ...\n",
            "Validation loss decreased (0.783356 --> 0.783199).  Saving model ...\n",
            "Validation loss decreased (0.783199 --> 0.783043).  Saving model ...\n",
            "Epoch: 800 \tTraining Loss: 0.808680 \tValidation Loss: 0.782887\n",
            "Validation loss decreased (0.783043 --> 0.782887).  Saving model ...\n",
            "Validation loss decreased (0.782887 --> 0.782729).  Saving model ...\n",
            "Validation loss decreased (0.782729 --> 0.782572).  Saving model ...\n",
            "Validation loss decreased (0.782572 --> 0.782415).  Saving model ...\n",
            "Validation loss decreased (0.782415 --> 0.782258).  Saving model ...\n",
            "Validation loss decreased (0.782258 --> 0.782103).  Saving model ...\n",
            "Validation loss decreased (0.782103 --> 0.781947).  Saving model ...\n",
            "Validation loss decreased (0.781947 --> 0.781790).  Saving model ...\n",
            "Validation loss decreased (0.781790 --> 0.781634).  Saving model ...\n",
            "Validation loss decreased (0.781634 --> 0.781479).  Saving model ...\n",
            "Epoch: 810 \tTraining Loss: 0.801671 \tValidation Loss: 0.781322\n",
            "Validation loss decreased (0.781479 --> 0.781322).  Saving model ...\n",
            "Validation loss decreased (0.781322 --> 0.781166).  Saving model ...\n",
            "Validation loss decreased (0.781166 --> 0.781010).  Saving model ...\n",
            "Validation loss decreased (0.781010 --> 0.780853).  Saving model ...\n",
            "Validation loss decreased (0.780853 --> 0.780697).  Saving model ...\n",
            "Validation loss decreased (0.780697 --> 0.780541).  Saving model ...\n",
            "Validation loss decreased (0.780541 --> 0.780385).  Saving model ...\n",
            "Validation loss decreased (0.780385 --> 0.780228).  Saving model ...\n",
            "Validation loss decreased (0.780228 --> 0.780073).  Saving model ...\n",
            "Validation loss decreased (0.780073 --> 0.779916).  Saving model ...\n",
            "Epoch: 820 \tTraining Loss: 0.808957 \tValidation Loss: 0.779761\n",
            "Validation loss decreased (0.779916 --> 0.779761).  Saving model ...\n",
            "Validation loss decreased (0.779761 --> 0.779605).  Saving model ...\n",
            "Validation loss decreased (0.779605 --> 0.779449).  Saving model ...\n",
            "Validation loss decreased (0.779449 --> 0.779293).  Saving model ...\n",
            "Validation loss decreased (0.779293 --> 0.779137).  Saving model ...\n",
            "Validation loss decreased (0.779137 --> 0.778980).  Saving model ...\n",
            "Validation loss decreased (0.778980 --> 0.778824).  Saving model ...\n",
            "Validation loss decreased (0.778824 --> 0.778667).  Saving model ...\n",
            "Validation loss decreased (0.778667 --> 0.778510).  Saving model ...\n",
            "Validation loss decreased (0.778510 --> 0.778353).  Saving model ...\n",
            "Epoch: 830 \tTraining Loss: 0.800692 \tValidation Loss: 0.778197\n",
            "Validation loss decreased (0.778353 --> 0.778197).  Saving model ...\n",
            "Validation loss decreased (0.778197 --> 0.778041).  Saving model ...\n",
            "Validation loss decreased (0.778041 --> 0.777883).  Saving model ...\n",
            "Validation loss decreased (0.777883 --> 0.777727).  Saving model ...\n",
            "Validation loss decreased (0.777727 --> 0.777571).  Saving model ...\n",
            "Validation loss decreased (0.777571 --> 0.777415).  Saving model ...\n",
            "Validation loss decreased (0.777415 --> 0.777259).  Saving model ...\n",
            "Validation loss decreased (0.777259 --> 0.777102).  Saving model ...\n",
            "Validation loss decreased (0.777102 --> 0.776946).  Saving model ...\n",
            "Validation loss decreased (0.776946 --> 0.776789).  Saving model ...\n",
            "Epoch: 840 \tTraining Loss: 0.797583 \tValidation Loss: 0.776633\n",
            "Validation loss decreased (0.776789 --> 0.776633).  Saving model ...\n",
            "Validation loss decreased (0.776633 --> 0.776477).  Saving model ...\n",
            "Validation loss decreased (0.776477 --> 0.776319).  Saving model ...\n",
            "Validation loss decreased (0.776319 --> 0.776163).  Saving model ...\n",
            "Validation loss decreased (0.776163 --> 0.776005).  Saving model ...\n",
            "Validation loss decreased (0.776005 --> 0.775849).  Saving model ...\n",
            "Validation loss decreased (0.775849 --> 0.775693).  Saving model ...\n",
            "Validation loss decreased (0.775693 --> 0.775536).  Saving model ...\n",
            "Validation loss decreased (0.775536 --> 0.775380).  Saving model ...\n",
            "Validation loss decreased (0.775380 --> 0.775224).  Saving model ...\n",
            "Epoch: 850 \tTraining Loss: 0.799313 \tValidation Loss: 0.775067\n",
            "Validation loss decreased (0.775224 --> 0.775067).  Saving model ...\n",
            "Validation loss decreased (0.775067 --> 0.774910).  Saving model ...\n",
            "Validation loss decreased (0.774910 --> 0.774753).  Saving model ...\n",
            "Validation loss decreased (0.774753 --> 0.774596).  Saving model ...\n",
            "Validation loss decreased (0.774596 --> 0.774439).  Saving model ...\n",
            "Validation loss decreased (0.774439 --> 0.774282).  Saving model ...\n",
            "Validation loss decreased (0.774282 --> 0.774126).  Saving model ...\n",
            "Validation loss decreased (0.774126 --> 0.773969).  Saving model ...\n",
            "Validation loss decreased (0.773969 --> 0.773812).  Saving model ...\n",
            "Validation loss decreased (0.773812 --> 0.773656).  Saving model ...\n",
            "Epoch: 860 \tTraining Loss: 0.793068 \tValidation Loss: 0.773500\n",
            "Validation loss decreased (0.773656 --> 0.773500).  Saving model ...\n",
            "Validation loss decreased (0.773500 --> 0.773344).  Saving model ...\n",
            "Validation loss decreased (0.773344 --> 0.773186).  Saving model ...\n",
            "Validation loss decreased (0.773186 --> 0.773030).  Saving model ...\n",
            "Validation loss decreased (0.773030 --> 0.772874).  Saving model ...\n",
            "Validation loss decreased (0.772874 --> 0.772717).  Saving model ...\n",
            "Validation loss decreased (0.772717 --> 0.772559).  Saving model ...\n",
            "Validation loss decreased (0.772559 --> 0.772402).  Saving model ...\n",
            "Validation loss decreased (0.772402 --> 0.772247).  Saving model ...\n",
            "Validation loss decreased (0.772247 --> 0.772089).  Saving model ...\n",
            "Epoch: 870 \tTraining Loss: 0.788227 \tValidation Loss: 0.771933\n",
            "Validation loss decreased (0.772089 --> 0.771933).  Saving model ...\n",
            "Validation loss decreased (0.771933 --> 0.771776).  Saving model ...\n",
            "Validation loss decreased (0.771776 --> 0.771619).  Saving model ...\n",
            "Validation loss decreased (0.771619 --> 0.771462).  Saving model ...\n",
            "Validation loss decreased (0.771462 --> 0.771306).  Saving model ...\n",
            "Validation loss decreased (0.771306 --> 0.771149).  Saving model ...\n",
            "Validation loss decreased (0.771149 --> 0.770992).  Saving model ...\n",
            "Validation loss decreased (0.770992 --> 0.770835).  Saving model ...\n",
            "Validation loss decreased (0.770835 --> 0.770679).  Saving model ...\n",
            "Validation loss decreased (0.770679 --> 0.770521).  Saving model ...\n",
            "Epoch: 880 \tTraining Loss: 0.791611 \tValidation Loss: 0.770364\n",
            "Validation loss decreased (0.770521 --> 0.770364).  Saving model ...\n",
            "Validation loss decreased (0.770364 --> 0.770208).  Saving model ...\n",
            "Validation loss decreased (0.770208 --> 0.770051).  Saving model ...\n",
            "Validation loss decreased (0.770051 --> 0.769895).  Saving model ...\n",
            "Validation loss decreased (0.769895 --> 0.769738).  Saving model ...\n",
            "Validation loss decreased (0.769738 --> 0.769580).  Saving model ...\n",
            "Validation loss decreased (0.769580 --> 0.769423).  Saving model ...\n",
            "Validation loss decreased (0.769423 --> 0.769266).  Saving model ...\n",
            "Validation loss decreased (0.769266 --> 0.769108).  Saving model ...\n",
            "Validation loss decreased (0.769108 --> 0.768952).  Saving model ...\n",
            "Epoch: 890 \tTraining Loss: 0.788218 \tValidation Loss: 0.768794\n",
            "Validation loss decreased (0.768952 --> 0.768794).  Saving model ...\n",
            "Validation loss decreased (0.768794 --> 0.768637).  Saving model ...\n",
            "Validation loss decreased (0.768637 --> 0.768479).  Saving model ...\n",
            "Validation loss decreased (0.768479 --> 0.768322).  Saving model ...\n",
            "Validation loss decreased (0.768322 --> 0.768165).  Saving model ...\n",
            "Validation loss decreased (0.768165 --> 0.768008).  Saving model ...\n",
            "Validation loss decreased (0.768008 --> 0.767850).  Saving model ...\n",
            "Validation loss decreased (0.767850 --> 0.767692).  Saving model ...\n",
            "Validation loss decreased (0.767692 --> 0.767536).  Saving model ...\n",
            "Validation loss decreased (0.767536 --> 0.767378).  Saving model ...\n",
            "Epoch: 900 \tTraining Loss: 0.789350 \tValidation Loss: 0.767220\n",
            "Validation loss decreased (0.767378 --> 0.767220).  Saving model ...\n",
            "Validation loss decreased (0.767220 --> 0.767063).  Saving model ...\n",
            "Validation loss decreased (0.767063 --> 0.766906).  Saving model ...\n",
            "Validation loss decreased (0.766906 --> 0.766749).  Saving model ...\n",
            "Validation loss decreased (0.766749 --> 0.766593).  Saving model ...\n",
            "Validation loss decreased (0.766593 --> 0.766435).  Saving model ...\n",
            "Validation loss decreased (0.766435 --> 0.766278).  Saving model ...\n",
            "Validation loss decreased (0.766278 --> 0.766121).  Saving model ...\n",
            "Validation loss decreased (0.766121 --> 0.765964).  Saving model ...\n",
            "Validation loss decreased (0.765964 --> 0.765806).  Saving model ...\n",
            "Epoch: 910 \tTraining Loss: 0.790093 \tValidation Loss: 0.765648\n",
            "Validation loss decreased (0.765806 --> 0.765648).  Saving model ...\n",
            "Validation loss decreased (0.765648 --> 0.765489).  Saving model ...\n",
            "Validation loss decreased (0.765489 --> 0.765332).  Saving model ...\n",
            "Validation loss decreased (0.765332 --> 0.765175).  Saving model ...\n",
            "Validation loss decreased (0.765175 --> 0.765018).  Saving model ...\n",
            "Validation loss decreased (0.765018 --> 0.764861).  Saving model ...\n",
            "Validation loss decreased (0.764861 --> 0.764703).  Saving model ...\n",
            "Validation loss decreased (0.764703 --> 0.764546).  Saving model ...\n",
            "Validation loss decreased (0.764546 --> 0.764387).  Saving model ...\n",
            "Validation loss decreased (0.764387 --> 0.764230).  Saving model ...\n",
            "Epoch: 920 \tTraining Loss: 0.788858 \tValidation Loss: 0.764072\n",
            "Validation loss decreased (0.764230 --> 0.764072).  Saving model ...\n",
            "Validation loss decreased (0.764072 --> 0.763915).  Saving model ...\n",
            "Validation loss decreased (0.763915 --> 0.763757).  Saving model ...\n",
            "Validation loss decreased (0.763757 --> 0.763600).  Saving model ...\n",
            "Validation loss decreased (0.763600 --> 0.763442).  Saving model ...\n",
            "Validation loss decreased (0.763442 --> 0.763285).  Saving model ...\n",
            "Validation loss decreased (0.763285 --> 0.763127).  Saving model ...\n",
            "Validation loss decreased (0.763127 --> 0.762971).  Saving model ...\n",
            "Validation loss decreased (0.762971 --> 0.762814).  Saving model ...\n",
            "Validation loss decreased (0.762814 --> 0.762658).  Saving model ...\n",
            "Epoch: 930 \tTraining Loss: 0.784835 \tValidation Loss: 0.762499\n",
            "Validation loss decreased (0.762658 --> 0.762499).  Saving model ...\n",
            "Validation loss decreased (0.762499 --> 0.762342).  Saving model ...\n",
            "Validation loss decreased (0.762342 --> 0.762185).  Saving model ...\n",
            "Validation loss decreased (0.762185 --> 0.762027).  Saving model ...\n",
            "Validation loss decreased (0.762027 --> 0.761870).  Saving model ...\n",
            "Validation loss decreased (0.761870 --> 0.761712).  Saving model ...\n",
            "Validation loss decreased (0.761712 --> 0.761554).  Saving model ...\n",
            "Validation loss decreased (0.761554 --> 0.761396).  Saving model ...\n",
            "Validation loss decreased (0.761396 --> 0.761238).  Saving model ...\n",
            "Validation loss decreased (0.761238 --> 0.761080).  Saving model ...\n",
            "Epoch: 940 \tTraining Loss: 0.788424 \tValidation Loss: 0.760922\n",
            "Validation loss decreased (0.761080 --> 0.760922).  Saving model ...\n",
            "Validation loss decreased (0.760922 --> 0.760764).  Saving model ...\n",
            "Validation loss decreased (0.760764 --> 0.760606).  Saving model ...\n",
            "Validation loss decreased (0.760606 --> 0.760448).  Saving model ...\n",
            "Validation loss decreased (0.760448 --> 0.760291).  Saving model ...\n",
            "Validation loss decreased (0.760291 --> 0.760134).  Saving model ...\n",
            "Validation loss decreased (0.760134 --> 0.759975).  Saving model ...\n",
            "Validation loss decreased (0.759975 --> 0.759818).  Saving model ...\n",
            "Validation loss decreased (0.759818 --> 0.759660).  Saving model ...\n",
            "Validation loss decreased (0.759660 --> 0.759504).  Saving model ...\n",
            "Epoch: 950 \tTraining Loss: 0.790496 \tValidation Loss: 0.759347\n",
            "Validation loss decreased (0.759504 --> 0.759347).  Saving model ...\n",
            "Validation loss decreased (0.759347 --> 0.759190).  Saving model ...\n",
            "Validation loss decreased (0.759190 --> 0.759032).  Saving model ...\n",
            "Validation loss decreased (0.759032 --> 0.758875).  Saving model ...\n",
            "Validation loss decreased (0.758875 --> 0.758716).  Saving model ...\n",
            "Validation loss decreased (0.758716 --> 0.758557).  Saving model ...\n",
            "Validation loss decreased (0.758557 --> 0.758399).  Saving model ...\n",
            "Validation loss decreased (0.758399 --> 0.758242).  Saving model ...\n",
            "Validation loss decreased (0.758242 --> 0.758085).  Saving model ...\n",
            "Validation loss decreased (0.758085 --> 0.757926).  Saving model ...\n",
            "Epoch: 960 \tTraining Loss: 0.783136 \tValidation Loss: 0.757770\n",
            "Validation loss decreased (0.757926 --> 0.757770).  Saving model ...\n",
            "Validation loss decreased (0.757770 --> 0.757612).  Saving model ...\n",
            "Validation loss decreased (0.757612 --> 0.757454).  Saving model ...\n",
            "Validation loss decreased (0.757454 --> 0.757296).  Saving model ...\n",
            "Validation loss decreased (0.757296 --> 0.757138).  Saving model ...\n",
            "Validation loss decreased (0.757138 --> 0.756980).  Saving model ...\n",
            "Validation loss decreased (0.756980 --> 0.756823).  Saving model ...\n",
            "Validation loss decreased (0.756823 --> 0.756665).  Saving model ...\n",
            "Validation loss decreased (0.756665 --> 0.756508).  Saving model ...\n",
            "Validation loss decreased (0.756508 --> 0.756350).  Saving model ...\n",
            "Epoch: 970 \tTraining Loss: 0.778440 \tValidation Loss: 0.756193\n",
            "Validation loss decreased (0.756350 --> 0.756193).  Saving model ...\n",
            "Validation loss decreased (0.756193 --> 0.756035).  Saving model ...\n",
            "Validation loss decreased (0.756035 --> 0.755878).  Saving model ...\n",
            "Validation loss decreased (0.755878 --> 0.755719).  Saving model ...\n",
            "Validation loss decreased (0.755719 --> 0.755561).  Saving model ...\n",
            "Validation loss decreased (0.755561 --> 0.755404).  Saving model ...\n",
            "Validation loss decreased (0.755404 --> 0.755246).  Saving model ...\n",
            "Validation loss decreased (0.755246 --> 0.755088).  Saving model ...\n",
            "Validation loss decreased (0.755088 --> 0.754930).  Saving model ...\n",
            "Validation loss decreased (0.754930 --> 0.754772).  Saving model ...\n",
            "Epoch: 980 \tTraining Loss: 0.778508 \tValidation Loss: 0.754614\n",
            "Validation loss decreased (0.754772 --> 0.754614).  Saving model ...\n",
            "Validation loss decreased (0.754614 --> 0.754456).  Saving model ...\n",
            "Validation loss decreased (0.754456 --> 0.754297).  Saving model ...\n",
            "Validation loss decreased (0.754297 --> 0.754140).  Saving model ...\n",
            "Validation loss decreased (0.754140 --> 0.753982).  Saving model ...\n",
            "Validation loss decreased (0.753982 --> 0.753824).  Saving model ...\n",
            "Validation loss decreased (0.753824 --> 0.753666).  Saving model ...\n",
            "Validation loss decreased (0.753666 --> 0.753509).  Saving model ...\n",
            "Validation loss decreased (0.753509 --> 0.753350).  Saving model ...\n",
            "Validation loss decreased (0.753350 --> 0.753192).  Saving model ...\n",
            "Epoch: 990 \tTraining Loss: 0.773623 \tValidation Loss: 0.753034\n",
            "Validation loss decreased (0.753192 --> 0.753034).  Saving model ...\n",
            "Validation loss decreased (0.753034 --> 0.752876).  Saving model ...\n",
            "Validation loss decreased (0.752876 --> 0.752718).  Saving model ...\n",
            "Validation loss decreased (0.752718 --> 0.752560).  Saving model ...\n",
            "Validation loss decreased (0.752560 --> 0.752402).  Saving model ...\n",
            "Validation loss decreased (0.752402 --> 0.752244).  Saving model ...\n",
            "Validation loss decreased (0.752244 --> 0.752086).  Saving model ...\n",
            "Validation loss decreased (0.752086 --> 0.751928).  Saving model ...\n",
            "Validation loss decreased (0.751928 --> 0.751770).  Saving model ...\n",
            "Validation loss decreased (0.751770 --> 0.751612).  Saving model ...\n",
            "Epoch: 1000 \tTraining Loss: 0.773398 \tValidation Loss: 0.751454\n",
            "Validation loss decreased (0.751612 --> 0.751454).  Saving model ...\n",
            "Validation loss decreased (0.751454 --> 0.751297).  Saving model ...\n",
            "Validation loss decreased (0.751297 --> 0.751139).  Saving model ...\n",
            "Validation loss decreased (0.751139 --> 0.750981).  Saving model ...\n",
            "Validation loss decreased (0.750981 --> 0.750822).  Saving model ...\n",
            "Validation loss decreased (0.750822 --> 0.750664).  Saving model ...\n",
            "Validation loss decreased (0.750664 --> 0.750505).  Saving model ...\n",
            "Validation loss decreased (0.750505 --> 0.750347).  Saving model ...\n",
            "Validation loss decreased (0.750347 --> 0.750189).  Saving model ...\n",
            "Validation loss decreased (0.750189 --> 0.750031).  Saving model ...\n",
            "Epoch: 1010 \tTraining Loss: 0.770699 \tValidation Loss: 0.749873\n",
            "Validation loss decreased (0.750031 --> 0.749873).  Saving model ...\n",
            "Validation loss decreased (0.749873 --> 0.749716).  Saving model ...\n",
            "Validation loss decreased (0.749716 --> 0.749558).  Saving model ...\n",
            "Validation loss decreased (0.749558 --> 0.749401).  Saving model ...\n",
            "Validation loss decreased (0.749401 --> 0.749242).  Saving model ...\n",
            "Validation loss decreased (0.749242 --> 0.749083).  Saving model ...\n",
            "Validation loss decreased (0.749083 --> 0.748925).  Saving model ...\n",
            "Validation loss decreased (0.748925 --> 0.748766).  Saving model ...\n",
            "Validation loss decreased (0.748766 --> 0.748608).  Saving model ...\n",
            "Validation loss decreased (0.748608 --> 0.748450).  Saving model ...\n",
            "Epoch: 1020 \tTraining Loss: 0.772914 \tValidation Loss: 0.748291\n",
            "Validation loss decreased (0.748450 --> 0.748291).  Saving model ...\n",
            "Validation loss decreased (0.748291 --> 0.748132).  Saving model ...\n",
            "Validation loss decreased (0.748132 --> 0.747974).  Saving model ...\n",
            "Validation loss decreased (0.747974 --> 0.747816).  Saving model ...\n",
            "Validation loss decreased (0.747816 --> 0.747659).  Saving model ...\n",
            "Validation loss decreased (0.747659 --> 0.747500).  Saving model ...\n",
            "Validation loss decreased (0.747500 --> 0.747342).  Saving model ...\n",
            "Validation loss decreased (0.747342 --> 0.747184).  Saving model ...\n",
            "Validation loss decreased (0.747184 --> 0.747026).  Saving model ...\n",
            "Validation loss decreased (0.747026 --> 0.746866).  Saving model ...\n",
            "Epoch: 1030 \tTraining Loss: 0.770845 \tValidation Loss: 0.746708\n",
            "Validation loss decreased (0.746866 --> 0.746708).  Saving model ...\n",
            "Validation loss decreased (0.746708 --> 0.746550).  Saving model ...\n",
            "Validation loss decreased (0.746550 --> 0.746391).  Saving model ...\n",
            "Validation loss decreased (0.746391 --> 0.746233).  Saving model ...\n",
            "Validation loss decreased (0.746233 --> 0.746075).  Saving model ...\n",
            "Validation loss decreased (0.746075 --> 0.745917).  Saving model ...\n",
            "Validation loss decreased (0.745917 --> 0.745758).  Saving model ...\n",
            "Validation loss decreased (0.745758 --> 0.745600).  Saving model ...\n",
            "Validation loss decreased (0.745600 --> 0.745442).  Saving model ...\n",
            "Validation loss decreased (0.745442 --> 0.745283).  Saving model ...\n",
            "Epoch: 1040 \tTraining Loss: 0.765763 \tValidation Loss: 0.745125\n",
            "Validation loss decreased (0.745283 --> 0.745125).  Saving model ...\n",
            "Validation loss decreased (0.745125 --> 0.744966).  Saving model ...\n",
            "Validation loss decreased (0.744966 --> 0.744808).  Saving model ...\n",
            "Validation loss decreased (0.744808 --> 0.744650).  Saving model ...\n",
            "Validation loss decreased (0.744650 --> 0.744492).  Saving model ...\n",
            "Validation loss decreased (0.744492 --> 0.744334).  Saving model ...\n",
            "Validation loss decreased (0.744334 --> 0.744175).  Saving model ...\n",
            "Validation loss decreased (0.744175 --> 0.744017).  Saving model ...\n",
            "Validation loss decreased (0.744017 --> 0.743858).  Saving model ...\n",
            "Validation loss decreased (0.743858 --> 0.743700).  Saving model ...\n",
            "Epoch: 1050 \tTraining Loss: 0.767223 \tValidation Loss: 0.743542\n",
            "Validation loss decreased (0.743700 --> 0.743542).  Saving model ...\n",
            "Validation loss decreased (0.743542 --> 0.743383).  Saving model ...\n",
            "Validation loss decreased (0.743383 --> 0.743225).  Saving model ...\n",
            "Validation loss decreased (0.743225 --> 0.743066).  Saving model ...\n",
            "Validation loss decreased (0.743066 --> 0.742907).  Saving model ...\n",
            "Validation loss decreased (0.742907 --> 0.742749).  Saving model ...\n",
            "Validation loss decreased (0.742749 --> 0.742592).  Saving model ...\n",
            "Validation loss decreased (0.742592 --> 0.742433).  Saving model ...\n",
            "Validation loss decreased (0.742433 --> 0.742274).  Saving model ...\n",
            "Validation loss decreased (0.742274 --> 0.742114).  Saving model ...\n",
            "Epoch: 1060 \tTraining Loss: 0.766243 \tValidation Loss: 0.741957\n",
            "Validation loss decreased (0.742114 --> 0.741957).  Saving model ...\n",
            "Validation loss decreased (0.741957 --> 0.741798).  Saving model ...\n",
            "Validation loss decreased (0.741798 --> 0.741640).  Saving model ...\n",
            "Validation loss decreased (0.741640 --> 0.741483).  Saving model ...\n",
            "Validation loss decreased (0.741483 --> 0.741324).  Saving model ...\n",
            "Validation loss decreased (0.741324 --> 0.741166).  Saving model ...\n",
            "Validation loss decreased (0.741166 --> 0.741007).  Saving model ...\n",
            "Validation loss decreased (0.741007 --> 0.740847).  Saving model ...\n",
            "Validation loss decreased (0.740847 --> 0.740689).  Saving model ...\n",
            "Validation loss decreased (0.740689 --> 0.740531).  Saving model ...\n",
            "Epoch: 1070 \tTraining Loss: 0.769574 \tValidation Loss: 0.740372\n",
            "Validation loss decreased (0.740531 --> 0.740372).  Saving model ...\n",
            "Validation loss decreased (0.740372 --> 0.740215).  Saving model ...\n",
            "Validation loss decreased (0.740215 --> 0.740057).  Saving model ...\n",
            "Validation loss decreased (0.740057 --> 0.739898).  Saving model ...\n",
            "Validation loss decreased (0.739898 --> 0.739739).  Saving model ...\n",
            "Validation loss decreased (0.739739 --> 0.739583).  Saving model ...\n",
            "Validation loss decreased (0.739583 --> 0.739424).  Saving model ...\n",
            "Validation loss decreased (0.739424 --> 0.739267).  Saving model ...\n",
            "Validation loss decreased (0.739267 --> 0.739109).  Saving model ...\n",
            "Validation loss decreased (0.739109 --> 0.738950).  Saving model ...\n",
            "Epoch: 1080 \tTraining Loss: 0.766948 \tValidation Loss: 0.738793\n",
            "Validation loss decreased (0.738950 --> 0.738793).  Saving model ...\n",
            "Validation loss decreased (0.738793 --> 0.738634).  Saving model ...\n",
            "Validation loss decreased (0.738634 --> 0.738476).  Saving model ...\n",
            "Validation loss decreased (0.738476 --> 0.738316).  Saving model ...\n",
            "Validation loss decreased (0.738316 --> 0.738158).  Saving model ...\n",
            "Validation loss decreased (0.738158 --> 0.737999).  Saving model ...\n",
            "Validation loss decreased (0.737999 --> 0.737840).  Saving model ...\n",
            "Validation loss decreased (0.737840 --> 0.737682).  Saving model ...\n",
            "Validation loss decreased (0.737682 --> 0.737523).  Saving model ...\n",
            "Validation loss decreased (0.737523 --> 0.737364).  Saving model ...\n",
            "Epoch: 1090 \tTraining Loss: 0.759346 \tValidation Loss: 0.737206\n",
            "Validation loss decreased (0.737364 --> 0.737206).  Saving model ...\n",
            "Validation loss decreased (0.737206 --> 0.737047).  Saving model ...\n",
            "Validation loss decreased (0.737047 --> 0.736887).  Saving model ...\n",
            "Validation loss decreased (0.736887 --> 0.736729).  Saving model ...\n",
            "Validation loss decreased (0.736729 --> 0.736570).  Saving model ...\n",
            "Validation loss decreased (0.736570 --> 0.736412).  Saving model ...\n",
            "Validation loss decreased (0.736412 --> 0.736254).  Saving model ...\n",
            "Validation loss decreased (0.736254 --> 0.736095).  Saving model ...\n",
            "Validation loss decreased (0.736095 --> 0.735935).  Saving model ...\n",
            "Validation loss decreased (0.735935 --> 0.735776).  Saving model ...\n",
            "Epoch: 1100 \tTraining Loss: 0.762458 \tValidation Loss: 0.735618\n",
            "Validation loss decreased (0.735776 --> 0.735618).  Saving model ...\n",
            "Validation loss decreased (0.735618 --> 0.735459).  Saving model ...\n",
            "Validation loss decreased (0.735459 --> 0.735300).  Saving model ...\n",
            "Validation loss decreased (0.735300 --> 0.735141).  Saving model ...\n",
            "Validation loss decreased (0.735141 --> 0.734983).  Saving model ...\n",
            "Validation loss decreased (0.734983 --> 0.734824).  Saving model ...\n",
            "Validation loss decreased (0.734824 --> 0.734666).  Saving model ...\n",
            "Validation loss decreased (0.734666 --> 0.734507).  Saving model ...\n",
            "Validation loss decreased (0.734507 --> 0.734348).  Saving model ...\n",
            "Validation loss decreased (0.734348 --> 0.734189).  Saving model ...\n",
            "Epoch: 1110 \tTraining Loss: 0.761801 \tValidation Loss: 0.734031\n",
            "Validation loss decreased (0.734189 --> 0.734031).  Saving model ...\n",
            "Validation loss decreased (0.734031 --> 0.733871).  Saving model ...\n",
            "Validation loss decreased (0.733871 --> 0.733712).  Saving model ...\n",
            "Validation loss decreased (0.733712 --> 0.733553).  Saving model ...\n",
            "Validation loss decreased (0.733553 --> 0.733395).  Saving model ...\n",
            "Validation loss decreased (0.733395 --> 0.733237).  Saving model ...\n",
            "Validation loss decreased (0.733237 --> 0.733077).  Saving model ...\n",
            "Validation loss decreased (0.733077 --> 0.732918).  Saving model ...\n",
            "Validation loss decreased (0.732918 --> 0.732758).  Saving model ...\n",
            "Validation loss decreased (0.732758 --> 0.732599).  Saving model ...\n",
            "Epoch: 1120 \tTraining Loss: 0.749820 \tValidation Loss: 0.732439\n",
            "Validation loss decreased (0.732599 --> 0.732439).  Saving model ...\n",
            "Validation loss decreased (0.732439 --> 0.732281).  Saving model ...\n",
            "Validation loss decreased (0.732281 --> 0.732122).  Saving model ...\n",
            "Validation loss decreased (0.732122 --> 0.731964).  Saving model ...\n",
            "Validation loss decreased (0.731964 --> 0.731804).  Saving model ...\n",
            "Validation loss decreased (0.731804 --> 0.731645).  Saving model ...\n",
            "Validation loss decreased (0.731645 --> 0.731486).  Saving model ...\n",
            "Validation loss decreased (0.731486 --> 0.731328).  Saving model ...\n",
            "Validation loss decreased (0.731328 --> 0.731169).  Saving model ...\n",
            "Validation loss decreased (0.731169 --> 0.731009).  Saving model ...\n",
            "Epoch: 1130 \tTraining Loss: 0.754430 \tValidation Loss: 0.730850\n",
            "Validation loss decreased (0.731009 --> 0.730850).  Saving model ...\n",
            "Validation loss decreased (0.730850 --> 0.730690).  Saving model ...\n",
            "Validation loss decreased (0.730690 --> 0.730530).  Saving model ...\n",
            "Validation loss decreased (0.730530 --> 0.730371).  Saving model ...\n",
            "Validation loss decreased (0.730371 --> 0.730212).  Saving model ...\n",
            "Validation loss decreased (0.730212 --> 0.730053).  Saving model ...\n",
            "Validation loss decreased (0.730053 --> 0.729893).  Saving model ...\n",
            "Validation loss decreased (0.729893 --> 0.729734).  Saving model ...\n",
            "Validation loss decreased (0.729734 --> 0.729576).  Saving model ...\n",
            "Validation loss decreased (0.729576 --> 0.729416).  Saving model ...\n",
            "Epoch: 1140 \tTraining Loss: 0.754458 \tValidation Loss: 0.729257\n",
            "Validation loss decreased (0.729416 --> 0.729257).  Saving model ...\n",
            "Validation loss decreased (0.729257 --> 0.729098).  Saving model ...\n",
            "Validation loss decreased (0.729098 --> 0.728938).  Saving model ...\n",
            "Validation loss decreased (0.728938 --> 0.728780).  Saving model ...\n",
            "Validation loss decreased (0.728780 --> 0.728621).  Saving model ...\n",
            "Validation loss decreased (0.728621 --> 0.728462).  Saving model ...\n",
            "Validation loss decreased (0.728462 --> 0.728302).  Saving model ...\n",
            "Validation loss decreased (0.728302 --> 0.728143).  Saving model ...\n",
            "Validation loss decreased (0.728143 --> 0.727983).  Saving model ...\n",
            "Validation loss decreased (0.727983 --> 0.727823).  Saving model ...\n",
            "Epoch: 1150 \tTraining Loss: 0.752953 \tValidation Loss: 0.727662\n",
            "Validation loss decreased (0.727823 --> 0.727662).  Saving model ...\n",
            "Validation loss decreased (0.727662 --> 0.727502).  Saving model ...\n",
            "Validation loss decreased (0.727502 --> 0.727342).  Saving model ...\n",
            "Validation loss decreased (0.727342 --> 0.727182).  Saving model ...\n",
            "Validation loss decreased (0.727182 --> 0.727022).  Saving model ...\n",
            "Validation loss decreased (0.727022 --> 0.726863).  Saving model ...\n",
            "Validation loss decreased (0.726863 --> 0.726703).  Saving model ...\n",
            "Validation loss decreased (0.726703 --> 0.726542).  Saving model ...\n",
            "Validation loss decreased (0.726542 --> 0.726383).  Saving model ...\n",
            "Validation loss decreased (0.726383 --> 0.726223).  Saving model ...\n",
            "Epoch: 1160 \tTraining Loss: 0.753194 \tValidation Loss: 0.726063\n",
            "Validation loss decreased (0.726223 --> 0.726063).  Saving model ...\n",
            "Validation loss decreased (0.726063 --> 0.725904).  Saving model ...\n",
            "Validation loss decreased (0.725904 --> 0.725745).  Saving model ...\n",
            "Validation loss decreased (0.725745 --> 0.725585).  Saving model ...\n",
            "Validation loss decreased (0.725585 --> 0.725425).  Saving model ...\n",
            "Validation loss decreased (0.725425 --> 0.725265).  Saving model ...\n",
            "Validation loss decreased (0.725265 --> 0.725106).  Saving model ...\n",
            "Validation loss decreased (0.725106 --> 0.724946).  Saving model ...\n",
            "Validation loss decreased (0.724946 --> 0.724786).  Saving model ...\n",
            "Validation loss decreased (0.724786 --> 0.724625).  Saving model ...\n",
            "Epoch: 1170 \tTraining Loss: 0.746158 \tValidation Loss: 0.724466\n",
            "Validation loss decreased (0.724625 --> 0.724466).  Saving model ...\n",
            "Validation loss decreased (0.724466 --> 0.724307).  Saving model ...\n",
            "Validation loss decreased (0.724307 --> 0.724148).  Saving model ...\n",
            "Validation loss decreased (0.724148 --> 0.723988).  Saving model ...\n",
            "Validation loss decreased (0.723988 --> 0.723828).  Saving model ...\n",
            "Validation loss decreased (0.723828 --> 0.723669).  Saving model ...\n",
            "Validation loss decreased (0.723669 --> 0.723509).  Saving model ...\n",
            "Validation loss decreased (0.723509 --> 0.723349).  Saving model ...\n",
            "Validation loss decreased (0.723349 --> 0.723189).  Saving model ...\n",
            "Validation loss decreased (0.723189 --> 0.723030).  Saving model ...\n",
            "Epoch: 1180 \tTraining Loss: 0.743824 \tValidation Loss: 0.722870\n",
            "Validation loss decreased (0.723030 --> 0.722870).  Saving model ...\n",
            "Validation loss decreased (0.722870 --> 0.722711).  Saving model ...\n",
            "Validation loss decreased (0.722711 --> 0.722551).  Saving model ...\n",
            "Validation loss decreased (0.722551 --> 0.722390).  Saving model ...\n",
            "Validation loss decreased (0.722390 --> 0.722230).  Saving model ...\n",
            "Validation loss decreased (0.722230 --> 0.722071).  Saving model ...\n",
            "Validation loss decreased (0.722071 --> 0.721911).  Saving model ...\n",
            "Validation loss decreased (0.721911 --> 0.721751).  Saving model ...\n",
            "Validation loss decreased (0.721751 --> 0.721591).  Saving model ...\n",
            "Validation loss decreased (0.721591 --> 0.721430).  Saving model ...\n",
            "Epoch: 1190 \tTraining Loss: 0.747924 \tValidation Loss: 0.721270\n",
            "Validation loss decreased (0.721430 --> 0.721270).  Saving model ...\n",
            "Validation loss decreased (0.721270 --> 0.721110).  Saving model ...\n",
            "Validation loss decreased (0.721110 --> 0.720951).  Saving model ...\n",
            "Validation loss decreased (0.720951 --> 0.720791).  Saving model ...\n",
            "Validation loss decreased (0.720791 --> 0.720631).  Saving model ...\n",
            "Validation loss decreased (0.720631 --> 0.720470).  Saving model ...\n",
            "Validation loss decreased (0.720470 --> 0.720310).  Saving model ...\n",
            "Validation loss decreased (0.720310 --> 0.720150).  Saving model ...\n",
            "Validation loss decreased (0.720150 --> 0.719991).  Saving model ...\n",
            "Validation loss decreased (0.719991 --> 0.719831).  Saving model ...\n",
            "Epoch: 1200 \tTraining Loss: 0.747666 \tValidation Loss: 0.719671\n",
            "Validation loss decreased (0.719831 --> 0.719671).  Saving model ...\n",
            "Validation loss decreased (0.719671 --> 0.719511).  Saving model ...\n",
            "Validation loss decreased (0.719511 --> 0.719351).  Saving model ...\n",
            "Validation loss decreased (0.719351 --> 0.719190).  Saving model ...\n",
            "Validation loss decreased (0.719190 --> 0.719030).  Saving model ...\n",
            "Validation loss decreased (0.719030 --> 0.718871).  Saving model ...\n",
            "Validation loss decreased (0.718871 --> 0.718712).  Saving model ...\n",
            "Validation loss decreased (0.718712 --> 0.718552).  Saving model ...\n",
            "Validation loss decreased (0.718552 --> 0.718392).  Saving model ...\n",
            "Validation loss decreased (0.718392 --> 0.718232).  Saving model ...\n",
            "Epoch: 1210 \tTraining Loss: 0.745673 \tValidation Loss: 0.718072\n",
            "Validation loss decreased (0.718232 --> 0.718072).  Saving model ...\n",
            "Validation loss decreased (0.718072 --> 0.717911).  Saving model ...\n",
            "Validation loss decreased (0.717911 --> 0.717751).  Saving model ...\n",
            "Validation loss decreased (0.717751 --> 0.717591).  Saving model ...\n",
            "Validation loss decreased (0.717591 --> 0.717431).  Saving model ...\n",
            "Validation loss decreased (0.717431 --> 0.717271).  Saving model ...\n",
            "Validation loss decreased (0.717271 --> 0.717113).  Saving model ...\n",
            "Validation loss decreased (0.717113 --> 0.716953).  Saving model ...\n",
            "Validation loss decreased (0.716953 --> 0.716792).  Saving model ...\n",
            "Validation loss decreased (0.716792 --> 0.716632).  Saving model ...\n",
            "Epoch: 1220 \tTraining Loss: 0.741497 \tValidation Loss: 0.716473\n",
            "Validation loss decreased (0.716632 --> 0.716473).  Saving model ...\n",
            "Validation loss decreased (0.716473 --> 0.716313).  Saving model ...\n",
            "Validation loss decreased (0.716313 --> 0.716153).  Saving model ...\n",
            "Validation loss decreased (0.716153 --> 0.715993).  Saving model ...\n",
            "Validation loss decreased (0.715993 --> 0.715833).  Saving model ...\n",
            "Validation loss decreased (0.715833 --> 0.715673).  Saving model ...\n",
            "Validation loss decreased (0.715673 --> 0.715513).  Saving model ...\n",
            "Validation loss decreased (0.715513 --> 0.715353).  Saving model ...\n",
            "Validation loss decreased (0.715353 --> 0.715192).  Saving model ...\n",
            "Validation loss decreased (0.715192 --> 0.715032).  Saving model ...\n",
            "Epoch: 1230 \tTraining Loss: 0.737952 \tValidation Loss: 0.714872\n",
            "Validation loss decreased (0.715032 --> 0.714872).  Saving model ...\n",
            "Validation loss decreased (0.714872 --> 0.714711).  Saving model ...\n",
            "Validation loss decreased (0.714711 --> 0.714550).  Saving model ...\n",
            "Validation loss decreased (0.714550 --> 0.714391).  Saving model ...\n",
            "Validation loss decreased (0.714391 --> 0.714231).  Saving model ...\n",
            "Validation loss decreased (0.714231 --> 0.714071).  Saving model ...\n",
            "Validation loss decreased (0.714071 --> 0.713910).  Saving model ...\n",
            "Validation loss decreased (0.713910 --> 0.713748).  Saving model ...\n",
            "Validation loss decreased (0.713748 --> 0.713586).  Saving model ...\n",
            "Validation loss decreased (0.713586 --> 0.713427).  Saving model ...\n",
            "Epoch: 1240 \tTraining Loss: 0.730514 \tValidation Loss: 0.713266\n",
            "Validation loss decreased (0.713427 --> 0.713266).  Saving model ...\n",
            "Validation loss decreased (0.713266 --> 0.713107).  Saving model ...\n",
            "Validation loss decreased (0.713107 --> 0.712946).  Saving model ...\n",
            "Validation loss decreased (0.712946 --> 0.712786).  Saving model ...\n",
            "Validation loss decreased (0.712786 --> 0.712625).  Saving model ...\n",
            "Validation loss decreased (0.712625 --> 0.712464).  Saving model ...\n",
            "Validation loss decreased (0.712464 --> 0.712304).  Saving model ...\n",
            "Validation loss decreased (0.712304 --> 0.712144).  Saving model ...\n",
            "Validation loss decreased (0.712144 --> 0.711983).  Saving model ...\n",
            "Validation loss decreased (0.711983 --> 0.711822).  Saving model ...\n",
            "Epoch: 1250 \tTraining Loss: 0.738910 \tValidation Loss: 0.711662\n",
            "Validation loss decreased (0.711822 --> 0.711662).  Saving model ...\n",
            "Validation loss decreased (0.711662 --> 0.711502).  Saving model ...\n",
            "Validation loss decreased (0.711502 --> 0.711341).  Saving model ...\n",
            "Validation loss decreased (0.711341 --> 0.711181).  Saving model ...\n",
            "Validation loss decreased (0.711181 --> 0.711021).  Saving model ...\n",
            "Validation loss decreased (0.711021 --> 0.710859).  Saving model ...\n",
            "Validation loss decreased (0.710859 --> 0.710699).  Saving model ...\n",
            "Validation loss decreased (0.710699 --> 0.710538).  Saving model ...\n",
            "Validation loss decreased (0.710538 --> 0.710377).  Saving model ...\n",
            "Validation loss decreased (0.710377 --> 0.710216).  Saving model ...\n",
            "Epoch: 1260 \tTraining Loss: 0.739044 \tValidation Loss: 0.710055\n",
            "Validation loss decreased (0.710216 --> 0.710055).  Saving model ...\n",
            "Validation loss decreased (0.710055 --> 0.709894).  Saving model ...\n",
            "Validation loss decreased (0.709894 --> 0.709734).  Saving model ...\n",
            "Validation loss decreased (0.709734 --> 0.709575).  Saving model ...\n",
            "Validation loss decreased (0.709575 --> 0.709414).  Saving model ...\n",
            "Validation loss decreased (0.709414 --> 0.709253).  Saving model ...\n",
            "Validation loss decreased (0.709253 --> 0.709093).  Saving model ...\n",
            "Validation loss decreased (0.709093 --> 0.708932).  Saving model ...\n",
            "Validation loss decreased (0.708932 --> 0.708771).  Saving model ...\n",
            "Validation loss decreased (0.708771 --> 0.708610).  Saving model ...\n",
            "Epoch: 1270 \tTraining Loss: 0.741966 \tValidation Loss: 0.708450\n",
            "Validation loss decreased (0.708610 --> 0.708450).  Saving model ...\n",
            "Validation loss decreased (0.708450 --> 0.708289).  Saving model ...\n",
            "Validation loss decreased (0.708289 --> 0.708128).  Saving model ...\n",
            "Validation loss decreased (0.708128 --> 0.707968).  Saving model ...\n",
            "Validation loss decreased (0.707968 --> 0.707806).  Saving model ...\n",
            "Validation loss decreased (0.707806 --> 0.707646).  Saving model ...\n",
            "Validation loss decreased (0.707646 --> 0.707484).  Saving model ...\n",
            "Validation loss decreased (0.707484 --> 0.707324).  Saving model ...\n",
            "Validation loss decreased (0.707324 --> 0.707163).  Saving model ...\n",
            "Validation loss decreased (0.707163 --> 0.707002).  Saving model ...\n",
            "Epoch: 1280 \tTraining Loss: 0.729195 \tValidation Loss: 0.706841\n",
            "Validation loss decreased (0.707002 --> 0.706841).  Saving model ...\n",
            "Validation loss decreased (0.706841 --> 0.706679).  Saving model ...\n",
            "Validation loss decreased (0.706679 --> 0.706519).  Saving model ...\n",
            "Validation loss decreased (0.706519 --> 0.706358).  Saving model ...\n",
            "Validation loss decreased (0.706358 --> 0.706197).  Saving model ...\n",
            "Validation loss decreased (0.706197 --> 0.706037).  Saving model ...\n",
            "Validation loss decreased (0.706037 --> 0.705876).  Saving model ...\n",
            "Validation loss decreased (0.705876 --> 0.705716).  Saving model ...\n",
            "Validation loss decreased (0.705716 --> 0.705556).  Saving model ...\n",
            "Validation loss decreased (0.705556 --> 0.705394).  Saving model ...\n",
            "Epoch: 1290 \tTraining Loss: 0.731222 \tValidation Loss: 0.705234\n",
            "Validation loss decreased (0.705394 --> 0.705234).  Saving model ...\n",
            "Validation loss decreased (0.705234 --> 0.705073).  Saving model ...\n",
            "Validation loss decreased (0.705073 --> 0.704913).  Saving model ...\n",
            "Validation loss decreased (0.704913 --> 0.704752).  Saving model ...\n",
            "Validation loss decreased (0.704752 --> 0.704592).  Saving model ...\n",
            "Validation loss decreased (0.704592 --> 0.704430).  Saving model ...\n",
            "Validation loss decreased (0.704430 --> 0.704269).  Saving model ...\n",
            "Validation loss decreased (0.704269 --> 0.704108).  Saving model ...\n",
            "Validation loss decreased (0.704108 --> 0.703947).  Saving model ...\n",
            "Validation loss decreased (0.703947 --> 0.703786).  Saving model ...\n",
            "Epoch: 1300 \tTraining Loss: 0.733380 \tValidation Loss: 0.703626\n",
            "Validation loss decreased (0.703786 --> 0.703626).  Saving model ...\n",
            "Validation loss decreased (0.703626 --> 0.703465).  Saving model ...\n",
            "Validation loss decreased (0.703465 --> 0.703303).  Saving model ...\n",
            "Validation loss decreased (0.703303 --> 0.703144).  Saving model ...\n",
            "Validation loss decreased (0.703144 --> 0.702983).  Saving model ...\n",
            "Validation loss decreased (0.702983 --> 0.702821).  Saving model ...\n",
            "Validation loss decreased (0.702821 --> 0.702661).  Saving model ...\n",
            "Validation loss decreased (0.702661 --> 0.702500).  Saving model ...\n",
            "Validation loss decreased (0.702500 --> 0.702339).  Saving model ...\n",
            "Validation loss decreased (0.702339 --> 0.702178).  Saving model ...\n",
            "Epoch: 1310 \tTraining Loss: 0.722131 \tValidation Loss: 0.702016\n",
            "Validation loss decreased (0.702178 --> 0.702016).  Saving model ...\n",
            "Validation loss decreased (0.702016 --> 0.701855).  Saving model ...\n",
            "Validation loss decreased (0.701855 --> 0.701695).  Saving model ...\n",
            "Validation loss decreased (0.701695 --> 0.701533).  Saving model ...\n",
            "Validation loss decreased (0.701533 --> 0.701373).  Saving model ...\n",
            "Validation loss decreased (0.701373 --> 0.701210).  Saving model ...\n",
            "Validation loss decreased (0.701210 --> 0.701049).  Saving model ...\n",
            "Validation loss decreased (0.701049 --> 0.700888).  Saving model ...\n",
            "Validation loss decreased (0.700888 --> 0.700727).  Saving model ...\n",
            "Validation loss decreased (0.700727 --> 0.700565).  Saving model ...\n",
            "Epoch: 1320 \tTraining Loss: 0.731407 \tValidation Loss: 0.700404\n",
            "Validation loss decreased (0.700565 --> 0.700404).  Saving model ...\n",
            "Validation loss decreased (0.700404 --> 0.700243).  Saving model ...\n",
            "Validation loss decreased (0.700243 --> 0.700082).  Saving model ...\n",
            "Validation loss decreased (0.700082 --> 0.699920).  Saving model ...\n",
            "Validation loss decreased (0.699920 --> 0.699759).  Saving model ...\n",
            "Validation loss decreased (0.699759 --> 0.699597).  Saving model ...\n",
            "Validation loss decreased (0.699597 --> 0.699436).  Saving model ...\n",
            "Validation loss decreased (0.699436 --> 0.699274).  Saving model ...\n",
            "Validation loss decreased (0.699274 --> 0.699112).  Saving model ...\n",
            "Validation loss decreased (0.699112 --> 0.698951).  Saving model ...\n",
            "Epoch: 1330 \tTraining Loss: 0.724003 \tValidation Loss: 0.698790\n",
            "Validation loss decreased (0.698951 --> 0.698790).  Saving model ...\n",
            "Validation loss decreased (0.698790 --> 0.698628).  Saving model ...\n",
            "Validation loss decreased (0.698628 --> 0.698467).  Saving model ...\n",
            "Validation loss decreased (0.698467 --> 0.698306).  Saving model ...\n",
            "Validation loss decreased (0.698306 --> 0.698145).  Saving model ...\n",
            "Validation loss decreased (0.698145 --> 0.697984).  Saving model ...\n",
            "Validation loss decreased (0.697984 --> 0.697823).  Saving model ...\n",
            "Validation loss decreased (0.697823 --> 0.697662).  Saving model ...\n",
            "Validation loss decreased (0.697662 --> 0.697499).  Saving model ...\n",
            "Validation loss decreased (0.697499 --> 0.697337).  Saving model ...\n",
            "Epoch: 1340 \tTraining Loss: 0.729541 \tValidation Loss: 0.697176\n",
            "Validation loss decreased (0.697337 --> 0.697176).  Saving model ...\n",
            "Validation loss decreased (0.697176 --> 0.697014).  Saving model ...\n",
            "Validation loss decreased (0.697014 --> 0.696853).  Saving model ...\n",
            "Validation loss decreased (0.696853 --> 0.696692).  Saving model ...\n",
            "Validation loss decreased (0.696692 --> 0.696530).  Saving model ...\n",
            "Validation loss decreased (0.696530 --> 0.696368).  Saving model ...\n",
            "Validation loss decreased (0.696368 --> 0.696206).  Saving model ...\n",
            "Validation loss decreased (0.696206 --> 0.696045).  Saving model ...\n",
            "Validation loss decreased (0.696045 --> 0.695884).  Saving model ...\n",
            "Validation loss decreased (0.695884 --> 0.695723).  Saving model ...\n",
            "Epoch: 1350 \tTraining Loss: 0.717480 \tValidation Loss: 0.695561\n",
            "Validation loss decreased (0.695723 --> 0.695561).  Saving model ...\n",
            "Validation loss decreased (0.695561 --> 0.695400).  Saving model ...\n",
            "Validation loss decreased (0.695400 --> 0.695238).  Saving model ...\n",
            "Validation loss decreased (0.695238 --> 0.695076).  Saving model ...\n",
            "Validation loss decreased (0.695076 --> 0.694915).  Saving model ...\n",
            "Validation loss decreased (0.694915 --> 0.694753).  Saving model ...\n",
            "Validation loss decreased (0.694753 --> 0.694592).  Saving model ...\n",
            "Validation loss decreased (0.694592 --> 0.694431).  Saving model ...\n",
            "Validation loss decreased (0.694431 --> 0.694269).  Saving model ...\n",
            "Validation loss decreased (0.694269 --> 0.694108).  Saving model ...\n",
            "Epoch: 1360 \tTraining Loss: 0.720105 \tValidation Loss: 0.693948\n",
            "Validation loss decreased (0.694108 --> 0.693948).  Saving model ...\n",
            "Validation loss decreased (0.693948 --> 0.693786).  Saving model ...\n",
            "Validation loss decreased (0.693786 --> 0.693624).  Saving model ...\n",
            "Validation loss decreased (0.693624 --> 0.693462).  Saving model ...\n",
            "Validation loss decreased (0.693462 --> 0.693300).  Saving model ...\n",
            "Validation loss decreased (0.693300 --> 0.693138).  Saving model ...\n",
            "Validation loss decreased (0.693138 --> 0.692976).  Saving model ...\n",
            "Validation loss decreased (0.692976 --> 0.692814).  Saving model ...\n",
            "Validation loss decreased (0.692814 --> 0.692652).  Saving model ...\n",
            "Validation loss decreased (0.692652 --> 0.692491).  Saving model ...\n",
            "Epoch: 1370 \tTraining Loss: 0.718694 \tValidation Loss: 0.692328\n",
            "Validation loss decreased (0.692491 --> 0.692328).  Saving model ...\n",
            "Validation loss decreased (0.692328 --> 0.692167).  Saving model ...\n",
            "Validation loss decreased (0.692167 --> 0.692004).  Saving model ...\n",
            "Validation loss decreased (0.692004 --> 0.691842).  Saving model ...\n",
            "Validation loss decreased (0.691842 --> 0.691681).  Saving model ...\n",
            "Validation loss decreased (0.691681 --> 0.691519).  Saving model ...\n",
            "Validation loss decreased (0.691519 --> 0.691357).  Saving model ...\n",
            "Validation loss decreased (0.691357 --> 0.691196).  Saving model ...\n",
            "Validation loss decreased (0.691196 --> 0.691034).  Saving model ...\n",
            "Validation loss decreased (0.691034 --> 0.690872).  Saving model ...\n",
            "Epoch: 1380 \tTraining Loss: 0.713251 \tValidation Loss: 0.690710\n",
            "Validation loss decreased (0.690872 --> 0.690710).  Saving model ...\n",
            "Validation loss decreased (0.690710 --> 0.690548).  Saving model ...\n",
            "Validation loss decreased (0.690548 --> 0.690385).  Saving model ...\n",
            "Validation loss decreased (0.690385 --> 0.690223).  Saving model ...\n",
            "Validation loss decreased (0.690223 --> 0.690061).  Saving model ...\n",
            "Validation loss decreased (0.690061 --> 0.689900).  Saving model ...\n",
            "Validation loss decreased (0.689900 --> 0.689737).  Saving model ...\n",
            "Validation loss decreased (0.689737 --> 0.689575).  Saving model ...\n",
            "Validation loss decreased (0.689575 --> 0.689413).  Saving model ...\n",
            "Validation loss decreased (0.689413 --> 0.689251).  Saving model ...\n",
            "Epoch: 1390 \tTraining Loss: 0.720241 \tValidation Loss: 0.689089\n",
            "Validation loss decreased (0.689251 --> 0.689089).  Saving model ...\n",
            "Validation loss decreased (0.689089 --> 0.688928).  Saving model ...\n",
            "Validation loss decreased (0.688928 --> 0.688765).  Saving model ...\n",
            "Validation loss decreased (0.688765 --> 0.688604).  Saving model ...\n",
            "Validation loss decreased (0.688604 --> 0.688443).  Saving model ...\n",
            "Validation loss decreased (0.688443 --> 0.688280).  Saving model ...\n",
            "Validation loss decreased (0.688280 --> 0.688118).  Saving model ...\n",
            "Validation loss decreased (0.688118 --> 0.687956).  Saving model ...\n",
            "Validation loss decreased (0.687956 --> 0.687794).  Saving model ...\n",
            "Validation loss decreased (0.687794 --> 0.687632).  Saving model ...\n",
            "Epoch: 1400 \tTraining Loss: 0.718926 \tValidation Loss: 0.687472\n",
            "Validation loss decreased (0.687632 --> 0.687472).  Saving model ...\n",
            "Validation loss decreased (0.687472 --> 0.687308).  Saving model ...\n",
            "Validation loss decreased (0.687308 --> 0.687146).  Saving model ...\n",
            "Validation loss decreased (0.687146 --> 0.686983).  Saving model ...\n",
            "Validation loss decreased (0.686983 --> 0.686820).  Saving model ...\n",
            "Validation loss decreased (0.686820 --> 0.686657).  Saving model ...\n",
            "Validation loss decreased (0.686657 --> 0.686495).  Saving model ...\n",
            "Validation loss decreased (0.686495 --> 0.686333).  Saving model ...\n",
            "Validation loss decreased (0.686333 --> 0.686172).  Saving model ...\n",
            "Validation loss decreased (0.686172 --> 0.686010).  Saving model ...\n",
            "Epoch: 1410 \tTraining Loss: 0.710396 \tValidation Loss: 0.685847\n",
            "Validation loss decreased (0.686010 --> 0.685847).  Saving model ...\n",
            "Validation loss decreased (0.685847 --> 0.685685).  Saving model ...\n",
            "Validation loss decreased (0.685685 --> 0.685522).  Saving model ...\n",
            "Validation loss decreased (0.685522 --> 0.685359).  Saving model ...\n",
            "Validation loss decreased (0.685359 --> 0.685197).  Saving model ...\n",
            "Validation loss decreased (0.685197 --> 0.685034).  Saving model ...\n",
            "Validation loss decreased (0.685034 --> 0.684871).  Saving model ...\n",
            "Validation loss decreased (0.684871 --> 0.684709).  Saving model ...\n",
            "Validation loss decreased (0.684709 --> 0.684547).  Saving model ...\n",
            "Validation loss decreased (0.684547 --> 0.684384).  Saving model ...\n",
            "Epoch: 1420 \tTraining Loss: 0.711452 \tValidation Loss: 0.684221\n",
            "Validation loss decreased (0.684384 --> 0.684221).  Saving model ...\n",
            "Validation loss decreased (0.684221 --> 0.684059).  Saving model ...\n",
            "Validation loss decreased (0.684059 --> 0.683896).  Saving model ...\n",
            "Validation loss decreased (0.683896 --> 0.683734).  Saving model ...\n",
            "Validation loss decreased (0.683734 --> 0.683571).  Saving model ...\n",
            "Validation loss decreased (0.683571 --> 0.683408).  Saving model ...\n",
            "Validation loss decreased (0.683408 --> 0.683247).  Saving model ...\n",
            "Validation loss decreased (0.683247 --> 0.683083).  Saving model ...\n",
            "Validation loss decreased (0.683083 --> 0.682920).  Saving model ...\n",
            "Validation loss decreased (0.682920 --> 0.682757).  Saving model ...\n",
            "Epoch: 1430 \tTraining Loss: 0.711094 \tValidation Loss: 0.682595\n",
            "Validation loss decreased (0.682757 --> 0.682595).  Saving model ...\n",
            "Validation loss decreased (0.682595 --> 0.682432).  Saving model ...\n",
            "Validation loss decreased (0.682432 --> 0.682271).  Saving model ...\n",
            "Validation loss decreased (0.682271 --> 0.682109).  Saving model ...\n",
            "Validation loss decreased (0.682109 --> 0.681946).  Saving model ...\n",
            "Validation loss decreased (0.681946 --> 0.681784).  Saving model ...\n",
            "Validation loss decreased (0.681784 --> 0.681622).  Saving model ...\n",
            "Validation loss decreased (0.681622 --> 0.681459).  Saving model ...\n",
            "Validation loss decreased (0.681459 --> 0.681295).  Saving model ...\n",
            "Validation loss decreased (0.681295 --> 0.681132).  Saving model ...\n",
            "Epoch: 1440 \tTraining Loss: 0.709112 \tValidation Loss: 0.680968\n",
            "Validation loss decreased (0.681132 --> 0.680968).  Saving model ...\n",
            "Validation loss decreased (0.680968 --> 0.680806).  Saving model ...\n",
            "Validation loss decreased (0.680806 --> 0.680644).  Saving model ...\n",
            "Validation loss decreased (0.680644 --> 0.680481).  Saving model ...\n",
            "Validation loss decreased (0.680481 --> 0.680318).  Saving model ...\n",
            "Validation loss decreased (0.680318 --> 0.680155).  Saving model ...\n",
            "Validation loss decreased (0.680155 --> 0.679993).  Saving model ...\n",
            "Validation loss decreased (0.679993 --> 0.679831).  Saving model ...\n",
            "Validation loss decreased (0.679831 --> 0.679667).  Saving model ...\n",
            "Validation loss decreased (0.679667 --> 0.679504).  Saving model ...\n",
            "Epoch: 1450 \tTraining Loss: 0.712918 \tValidation Loss: 0.679341\n",
            "Validation loss decreased (0.679504 --> 0.679341).  Saving model ...\n",
            "Validation loss decreased (0.679341 --> 0.679177).  Saving model ...\n",
            "Validation loss decreased (0.679177 --> 0.679013).  Saving model ...\n",
            "Validation loss decreased (0.679013 --> 0.678852).  Saving model ...\n",
            "Validation loss decreased (0.678852 --> 0.678689).  Saving model ...\n",
            "Validation loss decreased (0.678689 --> 0.678526).  Saving model ...\n",
            "Validation loss decreased (0.678526 --> 0.678363).  Saving model ...\n",
            "Validation loss decreased (0.678363 --> 0.678200).  Saving model ...\n",
            "Validation loss decreased (0.678200 --> 0.678036).  Saving model ...\n",
            "Validation loss decreased (0.678036 --> 0.677873).  Saving model ...\n",
            "Epoch: 1460 \tTraining Loss: 0.705596 \tValidation Loss: 0.677710\n",
            "Validation loss decreased (0.677873 --> 0.677710).  Saving model ...\n",
            "Validation loss decreased (0.677710 --> 0.677548).  Saving model ...\n",
            "Validation loss decreased (0.677548 --> 0.677384).  Saving model ...\n",
            "Validation loss decreased (0.677384 --> 0.677220).  Saving model ...\n",
            "Validation loss decreased (0.677220 --> 0.677056).  Saving model ...\n",
            "Validation loss decreased (0.677056 --> 0.676893).  Saving model ...\n",
            "Validation loss decreased (0.676893 --> 0.676731).  Saving model ...\n",
            "Validation loss decreased (0.676731 --> 0.676568).  Saving model ...\n",
            "Validation loss decreased (0.676568 --> 0.676404).  Saving model ...\n",
            "Validation loss decreased (0.676404 --> 0.676241).  Saving model ...\n",
            "Epoch: 1470 \tTraining Loss: 0.715071 \tValidation Loss: 0.676078\n",
            "Validation loss decreased (0.676241 --> 0.676078).  Saving model ...\n",
            "Validation loss decreased (0.676078 --> 0.675916).  Saving model ...\n",
            "Validation loss decreased (0.675916 --> 0.675753).  Saving model ...\n",
            "Validation loss decreased (0.675753 --> 0.675590).  Saving model ...\n",
            "Validation loss decreased (0.675590 --> 0.675427).  Saving model ...\n",
            "Validation loss decreased (0.675427 --> 0.675264).  Saving model ...\n",
            "Validation loss decreased (0.675264 --> 0.675100).  Saving model ...\n",
            "Validation loss decreased (0.675100 --> 0.674937).  Saving model ...\n",
            "Validation loss decreased (0.674937 --> 0.674773).  Saving model ...\n",
            "Validation loss decreased (0.674773 --> 0.674609).  Saving model ...\n",
            "Epoch: 1480 \tTraining Loss: 0.703128 \tValidation Loss: 0.674446\n",
            "Validation loss decreased (0.674609 --> 0.674446).  Saving model ...\n",
            "Validation loss decreased (0.674446 --> 0.674281).  Saving model ...\n",
            "Validation loss decreased (0.674281 --> 0.674118).  Saving model ...\n",
            "Validation loss decreased (0.674118 --> 0.673955).  Saving model ...\n",
            "Validation loss decreased (0.673955 --> 0.673791).  Saving model ...\n",
            "Validation loss decreased (0.673791 --> 0.673628).  Saving model ...\n",
            "Validation loss decreased (0.673628 --> 0.673463).  Saving model ...\n",
            "Validation loss decreased (0.673463 --> 0.673300).  Saving model ...\n",
            "Validation loss decreased (0.673300 --> 0.673135).  Saving model ...\n",
            "Validation loss decreased (0.673135 --> 0.672971).  Saving model ...\n",
            "Epoch: 1490 \tTraining Loss: 0.699628 \tValidation Loss: 0.672809\n",
            "Validation loss decreased (0.672971 --> 0.672809).  Saving model ...\n",
            "Validation loss decreased (0.672809 --> 0.672645).  Saving model ...\n",
            "Validation loss decreased (0.672645 --> 0.672482).  Saving model ...\n",
            "Validation loss decreased (0.672482 --> 0.672318).  Saving model ...\n",
            "Validation loss decreased (0.672318 --> 0.672154).  Saving model ...\n",
            "Validation loss decreased (0.672154 --> 0.671991).  Saving model ...\n",
            "Validation loss decreased (0.671991 --> 0.671828).  Saving model ...\n",
            "Validation loss decreased (0.671828 --> 0.671664).  Saving model ...\n",
            "Validation loss decreased (0.671664 --> 0.671501).  Saving model ...\n",
            "Validation loss decreased (0.671501 --> 0.671338).  Saving model ...\n",
            "Epoch: 1500 \tTraining Loss: 0.700095 \tValidation Loss: 0.671174\n",
            "Validation loss decreased (0.671338 --> 0.671174).  Saving model ...\n",
            "Validation loss decreased (0.671174 --> 0.671011).  Saving model ...\n",
            "Validation loss decreased (0.671011 --> 0.670847).  Saving model ...\n",
            "Validation loss decreased (0.670847 --> 0.670684).  Saving model ...\n",
            "Validation loss decreased (0.670684 --> 0.670521).  Saving model ...\n",
            "Validation loss decreased (0.670521 --> 0.670359).  Saving model ...\n",
            "Validation loss decreased (0.670359 --> 0.670195).  Saving model ...\n",
            "Validation loss decreased (0.670195 --> 0.670030).  Saving model ...\n",
            "Validation loss decreased (0.670030 --> 0.669868).  Saving model ...\n",
            "Validation loss decreased (0.669868 --> 0.669704).  Saving model ...\n",
            "Epoch: 1510 \tTraining Loss: 0.705618 \tValidation Loss: 0.669541\n",
            "Validation loss decreased (0.669704 --> 0.669541).  Saving model ...\n",
            "Validation loss decreased (0.669541 --> 0.669377).  Saving model ...\n",
            "Validation loss decreased (0.669377 --> 0.669213).  Saving model ...\n",
            "Validation loss decreased (0.669213 --> 0.669049).  Saving model ...\n",
            "Validation loss decreased (0.669049 --> 0.668886).  Saving model ...\n",
            "Validation loss decreased (0.668886 --> 0.668721).  Saving model ...\n",
            "Validation loss decreased (0.668721 --> 0.668556).  Saving model ...\n",
            "Validation loss decreased (0.668556 --> 0.668392).  Saving model ...\n",
            "Validation loss decreased (0.668392 --> 0.668228).  Saving model ...\n",
            "Validation loss decreased (0.668228 --> 0.668065).  Saving model ...\n",
            "Epoch: 1520 \tTraining Loss: 0.702857 \tValidation Loss: 0.667901\n",
            "Validation loss decreased (0.668065 --> 0.667901).  Saving model ...\n",
            "Validation loss decreased (0.667901 --> 0.667737).  Saving model ...\n",
            "Validation loss decreased (0.667737 --> 0.667574).  Saving model ...\n",
            "Validation loss decreased (0.667574 --> 0.667410).  Saving model ...\n",
            "Validation loss decreased (0.667410 --> 0.667247).  Saving model ...\n",
            "Validation loss decreased (0.667247 --> 0.667082).  Saving model ...\n",
            "Validation loss decreased (0.667082 --> 0.666918).  Saving model ...\n",
            "Validation loss decreased (0.666918 --> 0.666754).  Saving model ...\n",
            "Validation loss decreased (0.666754 --> 0.666589).  Saving model ...\n",
            "Validation loss decreased (0.666589 --> 0.666426).  Saving model ...\n",
            "Epoch: 1530 \tTraining Loss: 0.693691 \tValidation Loss: 0.666261\n",
            "Validation loss decreased (0.666426 --> 0.666261).  Saving model ...\n",
            "Validation loss decreased (0.666261 --> 0.666098).  Saving model ...\n",
            "Validation loss decreased (0.666098 --> 0.665933).  Saving model ...\n",
            "Validation loss decreased (0.665933 --> 0.665769).  Saving model ...\n",
            "Validation loss decreased (0.665769 --> 0.665604).  Saving model ...\n",
            "Validation loss decreased (0.665604 --> 0.665440).  Saving model ...\n",
            "Validation loss decreased (0.665440 --> 0.665275).  Saving model ...\n",
            "Validation loss decreased (0.665275 --> 0.665111).  Saving model ...\n",
            "Validation loss decreased (0.665111 --> 0.664946).  Saving model ...\n",
            "Validation loss decreased (0.664946 --> 0.664782).  Saving model ...\n",
            "Epoch: 1540 \tTraining Loss: 0.689957 \tValidation Loss: 0.664618\n",
            "Validation loss decreased (0.664782 --> 0.664618).  Saving model ...\n",
            "Validation loss decreased (0.664618 --> 0.664454).  Saving model ...\n",
            "Validation loss decreased (0.664454 --> 0.664290).  Saving model ...\n",
            "Validation loss decreased (0.664290 --> 0.664127).  Saving model ...\n",
            "Validation loss decreased (0.664127 --> 0.663963).  Saving model ...\n",
            "Validation loss decreased (0.663963 --> 0.663799).  Saving model ...\n",
            "Validation loss decreased (0.663799 --> 0.663634).  Saving model ...\n",
            "Validation loss decreased (0.663634 --> 0.663469).  Saving model ...\n",
            "Validation loss decreased (0.663469 --> 0.663305).  Saving model ...\n",
            "Validation loss decreased (0.663305 --> 0.663141).  Saving model ...\n",
            "Epoch: 1550 \tTraining Loss: 0.692869 \tValidation Loss: 0.662977\n",
            "Validation loss decreased (0.663141 --> 0.662977).  Saving model ...\n",
            "Validation loss decreased (0.662977 --> 0.662812).  Saving model ...\n",
            "Validation loss decreased (0.662812 --> 0.662648).  Saving model ...\n",
            "Validation loss decreased (0.662648 --> 0.662484).  Saving model ...\n",
            "Validation loss decreased (0.662484 --> 0.662320).  Saving model ...\n",
            "Validation loss decreased (0.662320 --> 0.662155).  Saving model ...\n",
            "Validation loss decreased (0.662155 --> 0.661992).  Saving model ...\n",
            "Validation loss decreased (0.661992 --> 0.661827).  Saving model ...\n",
            "Validation loss decreased (0.661827 --> 0.661662).  Saving model ...\n",
            "Validation loss decreased (0.661662 --> 0.661496).  Saving model ...\n",
            "Epoch: 1560 \tTraining Loss: 0.688528 \tValidation Loss: 0.661331\n",
            "Validation loss decreased (0.661496 --> 0.661331).  Saving model ...\n",
            "Validation loss decreased (0.661331 --> 0.661165).  Saving model ...\n",
            "Validation loss decreased (0.661165 --> 0.661000).  Saving model ...\n",
            "Validation loss decreased (0.661000 --> 0.660835).  Saving model ...\n",
            "Validation loss decreased (0.660835 --> 0.660670).  Saving model ...\n",
            "Validation loss decreased (0.660670 --> 0.660505).  Saving model ...\n",
            "Validation loss decreased (0.660505 --> 0.660341).  Saving model ...\n",
            "Validation loss decreased (0.660341 --> 0.660176).  Saving model ...\n",
            "Validation loss decreased (0.660176 --> 0.660011).  Saving model ...\n",
            "Validation loss decreased (0.660011 --> 0.659845).  Saving model ...\n",
            "Epoch: 1570 \tTraining Loss: 0.684851 \tValidation Loss: 0.659680\n",
            "Validation loss decreased (0.659845 --> 0.659680).  Saving model ...\n",
            "Validation loss decreased (0.659680 --> 0.659515).  Saving model ...\n",
            "Validation loss decreased (0.659515 --> 0.659350).  Saving model ...\n",
            "Validation loss decreased (0.659350 --> 0.659187).  Saving model ...\n",
            "Validation loss decreased (0.659187 --> 0.659023).  Saving model ...\n",
            "Validation loss decreased (0.659023 --> 0.658858).  Saving model ...\n",
            "Validation loss decreased (0.658858 --> 0.658692).  Saving model ...\n",
            "Validation loss decreased (0.658692 --> 0.658528).  Saving model ...\n",
            "Validation loss decreased (0.658528 --> 0.658362).  Saving model ...\n",
            "Validation loss decreased (0.658362 --> 0.658198).  Saving model ...\n",
            "Epoch: 1580 \tTraining Loss: 0.683457 \tValidation Loss: 0.658032\n",
            "Validation loss decreased (0.658198 --> 0.658032).  Saving model ...\n",
            "Validation loss decreased (0.658032 --> 0.657867).  Saving model ...\n",
            "Validation loss decreased (0.657867 --> 0.657701).  Saving model ...\n",
            "Validation loss decreased (0.657701 --> 0.657537).  Saving model ...\n",
            "Validation loss decreased (0.657537 --> 0.657372).  Saving model ...\n",
            "Validation loss decreased (0.657372 --> 0.657208).  Saving model ...\n",
            "Validation loss decreased (0.657208 --> 0.657043).  Saving model ...\n",
            "Validation loss decreased (0.657043 --> 0.656878).  Saving model ...\n",
            "Validation loss decreased (0.656878 --> 0.656714).  Saving model ...\n",
            "Validation loss decreased (0.656714 --> 0.656549).  Saving model ...\n",
            "Epoch: 1590 \tTraining Loss: 0.685235 \tValidation Loss: 0.656383\n",
            "Validation loss decreased (0.656549 --> 0.656383).  Saving model ...\n",
            "Validation loss decreased (0.656383 --> 0.656218).  Saving model ...\n",
            "Validation loss decreased (0.656218 --> 0.656053).  Saving model ...\n",
            "Validation loss decreased (0.656053 --> 0.655887).  Saving model ...\n",
            "Validation loss decreased (0.655887 --> 0.655722).  Saving model ...\n",
            "Validation loss decreased (0.655722 --> 0.655558).  Saving model ...\n",
            "Validation loss decreased (0.655558 --> 0.655393).  Saving model ...\n",
            "Validation loss decreased (0.655393 --> 0.655230).  Saving model ...\n",
            "Validation loss decreased (0.655230 --> 0.655066).  Saving model ...\n",
            "Validation loss decreased (0.655066 --> 0.654901).  Saving model ...\n",
            "Epoch: 1600 \tTraining Loss: 0.681899 \tValidation Loss: 0.654735\n",
            "Validation loss decreased (0.654901 --> 0.654735).  Saving model ...\n",
            "Validation loss decreased (0.654735 --> 0.654569).  Saving model ...\n",
            "Validation loss decreased (0.654569 --> 0.654404).  Saving model ...\n",
            "Validation loss decreased (0.654404 --> 0.654239).  Saving model ...\n",
            "Validation loss decreased (0.654239 --> 0.654073).  Saving model ...\n",
            "Validation loss decreased (0.654073 --> 0.653908).  Saving model ...\n",
            "Validation loss decreased (0.653908 --> 0.653742).  Saving model ...\n",
            "Validation loss decreased (0.653742 --> 0.653576).  Saving model ...\n",
            "Validation loss decreased (0.653576 --> 0.653411).  Saving model ...\n",
            "Validation loss decreased (0.653411 --> 0.653245).  Saving model ...\n",
            "Epoch: 1610 \tTraining Loss: 0.678934 \tValidation Loss: 0.653079\n",
            "Validation loss decreased (0.653245 --> 0.653079).  Saving model ...\n",
            "Validation loss decreased (0.653079 --> 0.652913).  Saving model ...\n",
            "Validation loss decreased (0.652913 --> 0.652748).  Saving model ...\n",
            "Validation loss decreased (0.652748 --> 0.652583).  Saving model ...\n",
            "Validation loss decreased (0.652583 --> 0.652416).  Saving model ...\n",
            "Validation loss decreased (0.652416 --> 0.652250).  Saving model ...\n",
            "Validation loss decreased (0.652250 --> 0.652084).  Saving model ...\n",
            "Validation loss decreased (0.652084 --> 0.651920).  Saving model ...\n",
            "Validation loss decreased (0.651920 --> 0.651755).  Saving model ...\n",
            "Validation loss decreased (0.651755 --> 0.651589).  Saving model ...\n",
            "Epoch: 1620 \tTraining Loss: 0.681156 \tValidation Loss: 0.651424\n",
            "Validation loss decreased (0.651589 --> 0.651424).  Saving model ...\n",
            "Validation loss decreased (0.651424 --> 0.651259).  Saving model ...\n",
            "Validation loss decreased (0.651259 --> 0.651093).  Saving model ...\n",
            "Validation loss decreased (0.651093 --> 0.650928).  Saving model ...\n",
            "Validation loss decreased (0.650928 --> 0.650762).  Saving model ...\n",
            "Validation loss decreased (0.650762 --> 0.650596).  Saving model ...\n",
            "Validation loss decreased (0.650596 --> 0.650430).  Saving model ...\n",
            "Validation loss decreased (0.650430 --> 0.650264).  Saving model ...\n",
            "Validation loss decreased (0.650264 --> 0.650099).  Saving model ...\n",
            "Validation loss decreased (0.650099 --> 0.649934).  Saving model ...\n",
            "Epoch: 1630 \tTraining Loss: 0.681687 \tValidation Loss: 0.649769\n",
            "Validation loss decreased (0.649934 --> 0.649769).  Saving model ...\n",
            "Validation loss decreased (0.649769 --> 0.649604).  Saving model ...\n",
            "Validation loss decreased (0.649604 --> 0.649438).  Saving model ...\n",
            "Validation loss decreased (0.649438 --> 0.649272).  Saving model ...\n",
            "Validation loss decreased (0.649272 --> 0.649105).  Saving model ...\n",
            "Validation loss decreased (0.649105 --> 0.648940).  Saving model ...\n",
            "Validation loss decreased (0.648940 --> 0.648775).  Saving model ...\n",
            "Validation loss decreased (0.648775 --> 0.648609).  Saving model ...\n",
            "Validation loss decreased (0.648609 --> 0.648444).  Saving model ...\n",
            "Validation loss decreased (0.648444 --> 0.648279).  Saving model ...\n",
            "Epoch: 1640 \tTraining Loss: 0.677704 \tValidation Loss: 0.648114\n",
            "Validation loss decreased (0.648279 --> 0.648114).  Saving model ...\n",
            "Validation loss decreased (0.648114 --> 0.647948).  Saving model ...\n",
            "Validation loss decreased (0.647948 --> 0.647783).  Saving model ...\n",
            "Validation loss decreased (0.647783 --> 0.647617).  Saving model ...\n",
            "Validation loss decreased (0.647617 --> 0.647450).  Saving model ...\n",
            "Validation loss decreased (0.647450 --> 0.647283).  Saving model ...\n",
            "Validation loss decreased (0.647283 --> 0.647118).  Saving model ...\n",
            "Validation loss decreased (0.647118 --> 0.646952).  Saving model ...\n",
            "Validation loss decreased (0.646952 --> 0.646786).  Saving model ...\n",
            "Validation loss decreased (0.646786 --> 0.646621).  Saving model ...\n",
            "Epoch: 1650 \tTraining Loss: 0.678104 \tValidation Loss: 0.646455\n",
            "Validation loss decreased (0.646621 --> 0.646455).  Saving model ...\n",
            "Validation loss decreased (0.646455 --> 0.646289).  Saving model ...\n",
            "Validation loss decreased (0.646289 --> 0.646123).  Saving model ...\n",
            "Validation loss decreased (0.646123 --> 0.645957).  Saving model ...\n",
            "Validation loss decreased (0.645957 --> 0.645790).  Saving model ...\n",
            "Validation loss decreased (0.645790 --> 0.645624).  Saving model ...\n",
            "Validation loss decreased (0.645624 --> 0.645457).  Saving model ...\n",
            "Validation loss decreased (0.645457 --> 0.645292).  Saving model ...\n",
            "Validation loss decreased (0.645292 --> 0.645125).  Saving model ...\n",
            "Validation loss decreased (0.645125 --> 0.644957).  Saving model ...\n",
            "Epoch: 1660 \tTraining Loss: 0.681068 \tValidation Loss: 0.644792\n",
            "Validation loss decreased (0.644957 --> 0.644792).  Saving model ...\n",
            "Validation loss decreased (0.644792 --> 0.644625).  Saving model ...\n",
            "Validation loss decreased (0.644625 --> 0.644460).  Saving model ...\n",
            "Validation loss decreased (0.644460 --> 0.644295).  Saving model ...\n",
            "Validation loss decreased (0.644295 --> 0.644129).  Saving model ...\n",
            "Validation loss decreased (0.644129 --> 0.643962).  Saving model ...\n",
            "Validation loss decreased (0.643962 --> 0.643796).  Saving model ...\n",
            "Validation loss decreased (0.643796 --> 0.643629).  Saving model ...\n",
            "Validation loss decreased (0.643629 --> 0.643463).  Saving model ...\n",
            "Validation loss decreased (0.643463 --> 0.643297).  Saving model ...\n",
            "Epoch: 1670 \tTraining Loss: 0.680690 \tValidation Loss: 0.643130\n",
            "Validation loss decreased (0.643297 --> 0.643130).  Saving model ...\n",
            "Validation loss decreased (0.643130 --> 0.642964).  Saving model ...\n",
            "Validation loss decreased (0.642964 --> 0.642798).  Saving model ...\n",
            "Validation loss decreased (0.642798 --> 0.642632).  Saving model ...\n",
            "Validation loss decreased (0.642632 --> 0.642467).  Saving model ...\n",
            "Validation loss decreased (0.642467 --> 0.642301).  Saving model ...\n",
            "Validation loss decreased (0.642301 --> 0.642134).  Saving model ...\n",
            "Validation loss decreased (0.642134 --> 0.641969).  Saving model ...\n",
            "Validation loss decreased (0.641969 --> 0.641802).  Saving model ...\n",
            "Validation loss decreased (0.641802 --> 0.641636).  Saving model ...\n",
            "Epoch: 1680 \tTraining Loss: 0.666337 \tValidation Loss: 0.641468\n",
            "Validation loss decreased (0.641636 --> 0.641468).  Saving model ...\n",
            "Validation loss decreased (0.641468 --> 0.641301).  Saving model ...\n",
            "Validation loss decreased (0.641301 --> 0.641134).  Saving model ...\n",
            "Validation loss decreased (0.641134 --> 0.640967).  Saving model ...\n",
            "Validation loss decreased (0.640967 --> 0.640801).  Saving model ...\n",
            "Validation loss decreased (0.640801 --> 0.640633).  Saving model ...\n",
            "Validation loss decreased (0.640633 --> 0.640466).  Saving model ...\n",
            "Validation loss decreased (0.640466 --> 0.640299).  Saving model ...\n",
            "Validation loss decreased (0.640299 --> 0.640132).  Saving model ...\n",
            "Validation loss decreased (0.640132 --> 0.639965).  Saving model ...\n",
            "Epoch: 1690 \tTraining Loss: 0.671131 \tValidation Loss: 0.639799\n",
            "Validation loss decreased (0.639965 --> 0.639799).  Saving model ...\n",
            "Validation loss decreased (0.639799 --> 0.639632).  Saving model ...\n",
            "Validation loss decreased (0.639632 --> 0.639466).  Saving model ...\n",
            "Validation loss decreased (0.639466 --> 0.639298).  Saving model ...\n",
            "Validation loss decreased (0.639298 --> 0.639131).  Saving model ...\n",
            "Validation loss decreased (0.639131 --> 0.638964).  Saving model ...\n",
            "Validation loss decreased (0.638964 --> 0.638797).  Saving model ...\n",
            "Validation loss decreased (0.638797 --> 0.638631).  Saving model ...\n",
            "Validation loss decreased (0.638631 --> 0.638464).  Saving model ...\n",
            "Validation loss decreased (0.638464 --> 0.638296).  Saving model ...\n",
            "Epoch: 1700 \tTraining Loss: 0.663177 \tValidation Loss: 0.638128\n",
            "Validation loss decreased (0.638296 --> 0.638128).  Saving model ...\n",
            "Validation loss decreased (0.638128 --> 0.637960).  Saving model ...\n",
            "Validation loss decreased (0.637960 --> 0.637793).  Saving model ...\n",
            "Validation loss decreased (0.637793 --> 0.637628).  Saving model ...\n",
            "Validation loss decreased (0.637628 --> 0.637460).  Saving model ...\n",
            "Validation loss decreased (0.637460 --> 0.637293).  Saving model ...\n",
            "Validation loss decreased (0.637293 --> 0.637126).  Saving model ...\n",
            "Validation loss decreased (0.637126 --> 0.636958).  Saving model ...\n",
            "Validation loss decreased (0.636958 --> 0.636792).  Saving model ...\n",
            "Validation loss decreased (0.636792 --> 0.636624).  Saving model ...\n",
            "Epoch: 1710 \tTraining Loss: 0.661344 \tValidation Loss: 0.636456\n",
            "Validation loss decreased (0.636624 --> 0.636456).  Saving model ...\n",
            "Validation loss decreased (0.636456 --> 0.636288).  Saving model ...\n",
            "Validation loss decreased (0.636288 --> 0.636120).  Saving model ...\n",
            "Validation loss decreased (0.636120 --> 0.635952).  Saving model ...\n",
            "Validation loss decreased (0.635952 --> 0.635786).  Saving model ...\n",
            "Validation loss decreased (0.635786 --> 0.635619).  Saving model ...\n",
            "Validation loss decreased (0.635619 --> 0.635452).  Saving model ...\n",
            "Validation loss decreased (0.635452 --> 0.635285).  Saving model ...\n",
            "Validation loss decreased (0.635285 --> 0.635117).  Saving model ...\n",
            "Validation loss decreased (0.635117 --> 0.634951).  Saving model ...\n",
            "Epoch: 1720 \tTraining Loss: 0.668600 \tValidation Loss: 0.634783\n",
            "Validation loss decreased (0.634951 --> 0.634783).  Saving model ...\n",
            "Validation loss decreased (0.634783 --> 0.634616).  Saving model ...\n",
            "Validation loss decreased (0.634616 --> 0.634448).  Saving model ...\n",
            "Validation loss decreased (0.634448 --> 0.634280).  Saving model ...\n",
            "Validation loss decreased (0.634280 --> 0.634112).  Saving model ...\n",
            "Validation loss decreased (0.634112 --> 0.633945).  Saving model ...\n",
            "Validation loss decreased (0.633945 --> 0.633778).  Saving model ...\n",
            "Validation loss decreased (0.633778 --> 0.633610).  Saving model ...\n",
            "Validation loss decreased (0.633610 --> 0.633443).  Saving model ...\n",
            "Validation loss decreased (0.633443 --> 0.633276).  Saving model ...\n",
            "Epoch: 1730 \tTraining Loss: 0.660547 \tValidation Loss: 0.633108\n",
            "Validation loss decreased (0.633276 --> 0.633108).  Saving model ...\n",
            "Validation loss decreased (0.633108 --> 0.632940).  Saving model ...\n",
            "Validation loss decreased (0.632940 --> 0.632773).  Saving model ...\n",
            "Validation loss decreased (0.632773 --> 0.632606).  Saving model ...\n",
            "Validation loss decreased (0.632606 --> 0.632439).  Saving model ...\n",
            "Validation loss decreased (0.632439 --> 0.632271).  Saving model ...\n",
            "Validation loss decreased (0.632271 --> 0.632103).  Saving model ...\n",
            "Validation loss decreased (0.632103 --> 0.631935).  Saving model ...\n",
            "Validation loss decreased (0.631935 --> 0.631767).  Saving model ...\n",
            "Validation loss decreased (0.631767 --> 0.631600).  Saving model ...\n",
            "Epoch: 1740 \tTraining Loss: 0.662626 \tValidation Loss: 0.631432\n",
            "Validation loss decreased (0.631600 --> 0.631432).  Saving model ...\n",
            "Validation loss decreased (0.631432 --> 0.631263).  Saving model ...\n",
            "Validation loss decreased (0.631263 --> 0.631095).  Saving model ...\n",
            "Validation loss decreased (0.631095 --> 0.630928).  Saving model ...\n",
            "Validation loss decreased (0.630928 --> 0.630760).  Saving model ...\n",
            "Validation loss decreased (0.630760 --> 0.630592).  Saving model ...\n",
            "Validation loss decreased (0.630592 --> 0.630423).  Saving model ...\n",
            "Validation loss decreased (0.630423 --> 0.630255).  Saving model ...\n",
            "Validation loss decreased (0.630255 --> 0.630087).  Saving model ...\n",
            "Validation loss decreased (0.630087 --> 0.629919).  Saving model ...\n",
            "Epoch: 1750 \tTraining Loss: 0.655587 \tValidation Loss: 0.629751\n",
            "Validation loss decreased (0.629919 --> 0.629751).  Saving model ...\n",
            "Validation loss decreased (0.629751 --> 0.629583).  Saving model ...\n",
            "Validation loss decreased (0.629583 --> 0.629415).  Saving model ...\n",
            "Validation loss decreased (0.629415 --> 0.629248).  Saving model ...\n",
            "Validation loss decreased (0.629248 --> 0.629079).  Saving model ...\n",
            "Validation loss decreased (0.629079 --> 0.628911).  Saving model ...\n",
            "Validation loss decreased (0.628911 --> 0.628742).  Saving model ...\n",
            "Validation loss decreased (0.628742 --> 0.628573).  Saving model ...\n",
            "Validation loss decreased (0.628573 --> 0.628406).  Saving model ...\n",
            "Validation loss decreased (0.628406 --> 0.628237).  Saving model ...\n",
            "Epoch: 1760 \tTraining Loss: 0.663683 \tValidation Loss: 0.628070\n",
            "Validation loss decreased (0.628237 --> 0.628070).  Saving model ...\n",
            "Validation loss decreased (0.628070 --> 0.627902).  Saving model ...\n",
            "Validation loss decreased (0.627902 --> 0.627734).  Saving model ...\n",
            "Validation loss decreased (0.627734 --> 0.627566).  Saving model ...\n",
            "Validation loss decreased (0.627566 --> 0.627398).  Saving model ...\n",
            "Validation loss decreased (0.627398 --> 0.627229).  Saving model ...\n",
            "Validation loss decreased (0.627229 --> 0.627061).  Saving model ...\n",
            "Validation loss decreased (0.627061 --> 0.626893).  Saving model ...\n",
            "Validation loss decreased (0.626893 --> 0.626725).  Saving model ...\n",
            "Validation loss decreased (0.626725 --> 0.626557).  Saving model ...\n",
            "Epoch: 1770 \tTraining Loss: 0.662399 \tValidation Loss: 0.626389\n",
            "Validation loss decreased (0.626557 --> 0.626389).  Saving model ...\n",
            "Validation loss decreased (0.626389 --> 0.626221).  Saving model ...\n",
            "Validation loss decreased (0.626221 --> 0.626053).  Saving model ...\n",
            "Validation loss decreased (0.626053 --> 0.625884).  Saving model ...\n",
            "Validation loss decreased (0.625884 --> 0.625714).  Saving model ...\n",
            "Validation loss decreased (0.625714 --> 0.625546).  Saving model ...\n",
            "Validation loss decreased (0.625546 --> 0.625378).  Saving model ...\n",
            "Validation loss decreased (0.625378 --> 0.625209).  Saving model ...\n",
            "Validation loss decreased (0.625209 --> 0.625041).  Saving model ...\n",
            "Validation loss decreased (0.625041 --> 0.624873).  Saving model ...\n",
            "Epoch: 1780 \tTraining Loss: 0.656559 \tValidation Loss: 0.624704\n",
            "Validation loss decreased (0.624873 --> 0.624704).  Saving model ...\n",
            "Validation loss decreased (0.624704 --> 0.624536).  Saving model ...\n",
            "Validation loss decreased (0.624536 --> 0.624367).  Saving model ...\n",
            "Validation loss decreased (0.624367 --> 0.624198).  Saving model ...\n",
            "Validation loss decreased (0.624198 --> 0.624030).  Saving model ...\n",
            "Validation loss decreased (0.624030 --> 0.623862).  Saving model ...\n",
            "Validation loss decreased (0.623862 --> 0.623694).  Saving model ...\n",
            "Validation loss decreased (0.623694 --> 0.623525).  Saving model ...\n",
            "Validation loss decreased (0.623525 --> 0.623357).  Saving model ...\n",
            "Validation loss decreased (0.623357 --> 0.623190).  Saving model ...\n",
            "Epoch: 1790 \tTraining Loss: 0.655294 \tValidation Loss: 0.623022\n",
            "Validation loss decreased (0.623190 --> 0.623022).  Saving model ...\n",
            "Validation loss decreased (0.623022 --> 0.622853).  Saving model ...\n",
            "Validation loss decreased (0.622853 --> 0.622685).  Saving model ...\n",
            "Validation loss decreased (0.622685 --> 0.622515).  Saving model ...\n",
            "Validation loss decreased (0.622515 --> 0.622347).  Saving model ...\n",
            "Validation loss decreased (0.622347 --> 0.622178).  Saving model ...\n",
            "Validation loss decreased (0.622178 --> 0.622009).  Saving model ...\n",
            "Validation loss decreased (0.622009 --> 0.621840).  Saving model ...\n",
            "Validation loss decreased (0.621840 --> 0.621672).  Saving model ...\n",
            "Validation loss decreased (0.621672 --> 0.621504).  Saving model ...\n",
            "Epoch: 1800 \tTraining Loss: 0.649255 \tValidation Loss: 0.621335\n",
            "Validation loss decreased (0.621504 --> 0.621335).  Saving model ...\n",
            "Validation loss decreased (0.621335 --> 0.621165).  Saving model ...\n",
            "Validation loss decreased (0.621165 --> 0.620997).  Saving model ...\n",
            "Validation loss decreased (0.620997 --> 0.620829).  Saving model ...\n",
            "Validation loss decreased (0.620829 --> 0.620659).  Saving model ...\n",
            "Validation loss decreased (0.620659 --> 0.620490).  Saving model ...\n",
            "Validation loss decreased (0.620490 --> 0.620323).  Saving model ...\n",
            "Validation loss decreased (0.620323 --> 0.620154).  Saving model ...\n",
            "Validation loss decreased (0.620154 --> 0.619986).  Saving model ...\n",
            "Validation loss decreased (0.619986 --> 0.619816).  Saving model ...\n",
            "Epoch: 1810 \tTraining Loss: 0.650885 \tValidation Loss: 0.619646\n",
            "Validation loss decreased (0.619816 --> 0.619646).  Saving model ...\n",
            "Validation loss decreased (0.619646 --> 0.619477).  Saving model ...\n",
            "Validation loss decreased (0.619477 --> 0.619307).  Saving model ...\n",
            "Validation loss decreased (0.619307 --> 0.619137).  Saving model ...\n",
            "Validation loss decreased (0.619137 --> 0.618968).  Saving model ...\n",
            "Validation loss decreased (0.618968 --> 0.618800).  Saving model ...\n",
            "Validation loss decreased (0.618800 --> 0.618631).  Saving model ...\n",
            "Validation loss decreased (0.618631 --> 0.618461).  Saving model ...\n",
            "Validation loss decreased (0.618461 --> 0.618292).  Saving model ...\n",
            "Validation loss decreased (0.618292 --> 0.618122).  Saving model ...\n",
            "Epoch: 1820 \tTraining Loss: 0.651722 \tValidation Loss: 0.617952\n",
            "Validation loss decreased (0.618122 --> 0.617952).  Saving model ...\n",
            "Validation loss decreased (0.617952 --> 0.617783).  Saving model ...\n",
            "Validation loss decreased (0.617783 --> 0.617614).  Saving model ...\n",
            "Validation loss decreased (0.617614 --> 0.617446).  Saving model ...\n",
            "Validation loss decreased (0.617446 --> 0.617277).  Saving model ...\n",
            "Validation loss decreased (0.617277 --> 0.617106).  Saving model ...\n",
            "Validation loss decreased (0.617106 --> 0.616938).  Saving model ...\n",
            "Validation loss decreased (0.616938 --> 0.616769).  Saving model ...\n",
            "Validation loss decreased (0.616769 --> 0.616600).  Saving model ...\n",
            "Validation loss decreased (0.616600 --> 0.616430).  Saving model ...\n",
            "Epoch: 1830 \tTraining Loss: 0.653594 \tValidation Loss: 0.616262\n",
            "Validation loss decreased (0.616430 --> 0.616262).  Saving model ...\n",
            "Validation loss decreased (0.616262 --> 0.616092).  Saving model ...\n",
            "Validation loss decreased (0.616092 --> 0.615923).  Saving model ...\n",
            "Validation loss decreased (0.615923 --> 0.615753).  Saving model ...\n",
            "Validation loss decreased (0.615753 --> 0.615582).  Saving model ...\n",
            "Validation loss decreased (0.615582 --> 0.615412).  Saving model ...\n",
            "Validation loss decreased (0.615412 --> 0.615243).  Saving model ...\n",
            "Validation loss decreased (0.615243 --> 0.615074).  Saving model ...\n",
            "Validation loss decreased (0.615074 --> 0.614904).  Saving model ...\n",
            "Validation loss decreased (0.614904 --> 0.614735).  Saving model ...\n",
            "Epoch: 1840 \tTraining Loss: 0.644674 \tValidation Loss: 0.614565\n",
            "Validation loss decreased (0.614735 --> 0.614565).  Saving model ...\n",
            "Validation loss decreased (0.614565 --> 0.614395).  Saving model ...\n",
            "Validation loss decreased (0.614395 --> 0.614224).  Saving model ...\n",
            "Validation loss decreased (0.614224 --> 0.614054).  Saving model ...\n",
            "Validation loss decreased (0.614054 --> 0.613884).  Saving model ...\n",
            "Validation loss decreased (0.613884 --> 0.613714).  Saving model ...\n",
            "Validation loss decreased (0.613714 --> 0.613544).  Saving model ...\n",
            "Validation loss decreased (0.613544 --> 0.613375).  Saving model ...\n",
            "Validation loss decreased (0.613375 --> 0.613205).  Saving model ...\n",
            "Validation loss decreased (0.613205 --> 0.613035).  Saving model ...\n",
            "Epoch: 1850 \tTraining Loss: 0.649645 \tValidation Loss: 0.612866\n",
            "Validation loss decreased (0.613035 --> 0.612866).  Saving model ...\n",
            "Validation loss decreased (0.612866 --> 0.612695).  Saving model ...\n",
            "Validation loss decreased (0.612695 --> 0.612526).  Saving model ...\n",
            "Validation loss decreased (0.612526 --> 0.612356).  Saving model ...\n",
            "Validation loss decreased (0.612356 --> 0.612186).  Saving model ...\n",
            "Validation loss decreased (0.612186 --> 0.612016).  Saving model ...\n",
            "Validation loss decreased (0.612016 --> 0.611846).  Saving model ...\n",
            "Validation loss decreased (0.611846 --> 0.611676).  Saving model ...\n",
            "Validation loss decreased (0.611676 --> 0.611507).  Saving model ...\n",
            "Validation loss decreased (0.611507 --> 0.611335).  Saving model ...\n",
            "Epoch: 1860 \tTraining Loss: 0.643065 \tValidation Loss: 0.611165\n",
            "Validation loss decreased (0.611335 --> 0.611165).  Saving model ...\n",
            "Validation loss decreased (0.611165 --> 0.610994).  Saving model ...\n",
            "Validation loss decreased (0.610994 --> 0.610824).  Saving model ...\n",
            "Validation loss decreased (0.610824 --> 0.610654).  Saving model ...\n",
            "Validation loss decreased (0.610654 --> 0.610483).  Saving model ...\n",
            "Validation loss decreased (0.610483 --> 0.610312).  Saving model ...\n",
            "Validation loss decreased (0.610312 --> 0.610142).  Saving model ...\n",
            "Validation loss decreased (0.610142 --> 0.609972).  Saving model ...\n",
            "Validation loss decreased (0.609972 --> 0.609802).  Saving model ...\n",
            "Validation loss decreased (0.609802 --> 0.609632).  Saving model ...\n",
            "Epoch: 1870 \tTraining Loss: 0.638288 \tValidation Loss: 0.609462\n",
            "Validation loss decreased (0.609632 --> 0.609462).  Saving model ...\n",
            "Validation loss decreased (0.609462 --> 0.609291).  Saving model ...\n",
            "Validation loss decreased (0.609291 --> 0.609121).  Saving model ...\n",
            "Validation loss decreased (0.609121 --> 0.608952).  Saving model ...\n",
            "Validation loss decreased (0.608952 --> 0.608780).  Saving model ...\n",
            "Validation loss decreased (0.608780 --> 0.608611).  Saving model ...\n",
            "Validation loss decreased (0.608611 --> 0.608440).  Saving model ...\n",
            "Validation loss decreased (0.608440 --> 0.608270).  Saving model ...\n",
            "Validation loss decreased (0.608270 --> 0.608099).  Saving model ...\n",
            "Validation loss decreased (0.608099 --> 0.607928).  Saving model ...\n",
            "Epoch: 1880 \tTraining Loss: 0.641371 \tValidation Loss: 0.607758\n",
            "Validation loss decreased (0.607928 --> 0.607758).  Saving model ...\n",
            "Validation loss decreased (0.607758 --> 0.607588).  Saving model ...\n",
            "Validation loss decreased (0.607588 --> 0.607417).  Saving model ...\n",
            "Validation loss decreased (0.607417 --> 0.607246).  Saving model ...\n",
            "Validation loss decreased (0.607246 --> 0.607075).  Saving model ...\n",
            "Validation loss decreased (0.607075 --> 0.606905).  Saving model ...\n",
            "Validation loss decreased (0.606905 --> 0.606733).  Saving model ...\n",
            "Validation loss decreased (0.606733 --> 0.606561).  Saving model ...\n",
            "Validation loss decreased (0.606561 --> 0.606391).  Saving model ...\n",
            "Validation loss decreased (0.606391 --> 0.606221).  Saving model ...\n",
            "Epoch: 1890 \tTraining Loss: 0.636555 \tValidation Loss: 0.606050\n",
            "Validation loss decreased (0.606221 --> 0.606050).  Saving model ...\n",
            "Validation loss decreased (0.606050 --> 0.605880).  Saving model ...\n",
            "Validation loss decreased (0.605880 --> 0.605709).  Saving model ...\n",
            "Validation loss decreased (0.605709 --> 0.605539).  Saving model ...\n",
            "Validation loss decreased (0.605539 --> 0.605367).  Saving model ...\n",
            "Validation loss decreased (0.605367 --> 0.605197).  Saving model ...\n",
            "Validation loss decreased (0.605197 --> 0.605026).  Saving model ...\n",
            "Validation loss decreased (0.605026 --> 0.604857).  Saving model ...\n",
            "Validation loss decreased (0.604857 --> 0.604685).  Saving model ...\n",
            "Validation loss decreased (0.604685 --> 0.604515).  Saving model ...\n",
            "Epoch: 1900 \tTraining Loss: 0.637741 \tValidation Loss: 0.604345\n",
            "Validation loss decreased (0.604515 --> 0.604345).  Saving model ...\n",
            "Validation loss decreased (0.604345 --> 0.604175).  Saving model ...\n",
            "Validation loss decreased (0.604175 --> 0.604003).  Saving model ...\n",
            "Validation loss decreased (0.604003 --> 0.603833).  Saving model ...\n",
            "Validation loss decreased (0.603833 --> 0.603663).  Saving model ...\n",
            "Validation loss decreased (0.603663 --> 0.603493).  Saving model ...\n",
            "Validation loss decreased (0.603493 --> 0.603322).  Saving model ...\n",
            "Validation loss decreased (0.603322 --> 0.603151).  Saving model ...\n",
            "Validation loss decreased (0.603151 --> 0.602981).  Saving model ...\n",
            "Validation loss decreased (0.602981 --> 0.602811).  Saving model ...\n",
            "Epoch: 1910 \tTraining Loss: 0.637709 \tValidation Loss: 0.602640\n",
            "Validation loss decreased (0.602811 --> 0.602640).  Saving model ...\n",
            "Validation loss decreased (0.602640 --> 0.602468).  Saving model ...\n",
            "Validation loss decreased (0.602468 --> 0.602296).  Saving model ...\n",
            "Validation loss decreased (0.602296 --> 0.602126).  Saving model ...\n",
            "Validation loss decreased (0.602126 --> 0.601954).  Saving model ...\n",
            "Validation loss decreased (0.601954 --> 0.601783).  Saving model ...\n",
            "Validation loss decreased (0.601783 --> 0.601612).  Saving model ...\n",
            "Validation loss decreased (0.601612 --> 0.601440).  Saving model ...\n",
            "Validation loss decreased (0.601440 --> 0.601269).  Saving model ...\n",
            "Validation loss decreased (0.601269 --> 0.601100).  Saving model ...\n",
            "Epoch: 1920 \tTraining Loss: 0.632658 \tValidation Loss: 0.600929\n",
            "Validation loss decreased (0.601100 --> 0.600929).  Saving model ...\n",
            "Validation loss decreased (0.600929 --> 0.600758).  Saving model ...\n",
            "Validation loss decreased (0.600758 --> 0.600585).  Saving model ...\n",
            "Validation loss decreased (0.600585 --> 0.600415).  Saving model ...\n",
            "Validation loss decreased (0.600415 --> 0.600245).  Saving model ...\n",
            "Validation loss decreased (0.600245 --> 0.600073).  Saving model ...\n",
            "Validation loss decreased (0.600073 --> 0.599902).  Saving model ...\n",
            "Validation loss decreased (0.599902 --> 0.599731).  Saving model ...\n",
            "Validation loss decreased (0.599731 --> 0.599560).  Saving model ...\n",
            "Validation loss decreased (0.599560 --> 0.599389).  Saving model ...\n",
            "Epoch: 1930 \tTraining Loss: 0.628919 \tValidation Loss: 0.599218\n",
            "Validation loss decreased (0.599389 --> 0.599218).  Saving model ...\n",
            "Validation loss decreased (0.599218 --> 0.599047).  Saving model ...\n",
            "Validation loss decreased (0.599047 --> 0.598875).  Saving model ...\n",
            "Validation loss decreased (0.598875 --> 0.598705).  Saving model ...\n",
            "Validation loss decreased (0.598705 --> 0.598532).  Saving model ...\n",
            "Validation loss decreased (0.598532 --> 0.598361).  Saving model ...\n",
            "Validation loss decreased (0.598361 --> 0.598190).  Saving model ...\n",
            "Validation loss decreased (0.598190 --> 0.598019).  Saving model ...\n",
            "Validation loss decreased (0.598019 --> 0.597846).  Saving model ...\n",
            "Validation loss decreased (0.597846 --> 0.597675).  Saving model ...\n",
            "Epoch: 1940 \tTraining Loss: 0.639221 \tValidation Loss: 0.597504\n",
            "Validation loss decreased (0.597675 --> 0.597504).  Saving model ...\n",
            "Validation loss decreased (0.597504 --> 0.597333).  Saving model ...\n",
            "Validation loss decreased (0.597333 --> 0.597162).  Saving model ...\n",
            "Validation loss decreased (0.597162 --> 0.596991).  Saving model ...\n",
            "Validation loss decreased (0.596991 --> 0.596820).  Saving model ...\n",
            "Validation loss decreased (0.596820 --> 0.596648).  Saving model ...\n",
            "Validation loss decreased (0.596648 --> 0.596477).  Saving model ...\n",
            "Validation loss decreased (0.596477 --> 0.596306).  Saving model ...\n",
            "Validation loss decreased (0.596306 --> 0.596134).  Saving model ...\n",
            "Validation loss decreased (0.596134 --> 0.595963).  Saving model ...\n",
            "Epoch: 1950 \tTraining Loss: 0.635868 \tValidation Loss: 0.595792\n",
            "Validation loss decreased (0.595963 --> 0.595792).  Saving model ...\n",
            "Validation loss decreased (0.595792 --> 0.595620).  Saving model ...\n",
            "Validation loss decreased (0.595620 --> 0.595448).  Saving model ...\n",
            "Validation loss decreased (0.595448 --> 0.595276).  Saving model ...\n",
            "Validation loss decreased (0.595276 --> 0.595105).  Saving model ...\n",
            "Validation loss decreased (0.595105 --> 0.594932).  Saving model ...\n",
            "Validation loss decreased (0.594932 --> 0.594759).  Saving model ...\n",
            "Validation loss decreased (0.594759 --> 0.594587).  Saving model ...\n",
            "Validation loss decreased (0.594587 --> 0.594414).  Saving model ...\n",
            "Validation loss decreased (0.594414 --> 0.594243).  Saving model ...\n",
            "Epoch: 1960 \tTraining Loss: 0.621512 \tValidation Loss: 0.594070\n",
            "Validation loss decreased (0.594243 --> 0.594070).  Saving model ...\n",
            "Validation loss decreased (0.594070 --> 0.593899).  Saving model ...\n",
            "Validation loss decreased (0.593899 --> 0.593728).  Saving model ...\n",
            "Validation loss decreased (0.593728 --> 0.593556).  Saving model ...\n",
            "Validation loss decreased (0.593556 --> 0.593384).  Saving model ...\n",
            "Validation loss decreased (0.593384 --> 0.593212).  Saving model ...\n",
            "Validation loss decreased (0.593212 --> 0.593040).  Saving model ...\n",
            "Validation loss decreased (0.593040 --> 0.592869).  Saving model ...\n",
            "Validation loss decreased (0.592869 --> 0.592696).  Saving model ...\n",
            "Validation loss decreased (0.592696 --> 0.592523).  Saving model ...\n",
            "Epoch: 1970 \tTraining Loss: 0.628246 \tValidation Loss: 0.592350\n",
            "Validation loss decreased (0.592523 --> 0.592350).  Saving model ...\n",
            "Validation loss decreased (0.592350 --> 0.592178).  Saving model ...\n",
            "Validation loss decreased (0.592178 --> 0.592007).  Saving model ...\n",
            "Validation loss decreased (0.592007 --> 0.591835).  Saving model ...\n",
            "Validation loss decreased (0.591835 --> 0.591662).  Saving model ...\n",
            "Validation loss decreased (0.591662 --> 0.591490).  Saving model ...\n",
            "Validation loss decreased (0.591490 --> 0.591318).  Saving model ...\n",
            "Validation loss decreased (0.591318 --> 0.591146).  Saving model ...\n",
            "Validation loss decreased (0.591146 --> 0.590973).  Saving model ...\n",
            "Validation loss decreased (0.590973 --> 0.590801).  Saving model ...\n",
            "Epoch: 1980 \tTraining Loss: 0.617538 \tValidation Loss: 0.590628\n",
            "Validation loss decreased (0.590801 --> 0.590628).  Saving model ...\n",
            "Validation loss decreased (0.590628 --> 0.590456).  Saving model ...\n",
            "Validation loss decreased (0.590456 --> 0.590284).  Saving model ...\n",
            "Validation loss decreased (0.590284 --> 0.590112).  Saving model ...\n",
            "Validation loss decreased (0.590112 --> 0.589939).  Saving model ...\n",
            "Validation loss decreased (0.589939 --> 0.589767).  Saving model ...\n",
            "Validation loss decreased (0.589767 --> 0.589595).  Saving model ...\n",
            "Validation loss decreased (0.589595 --> 0.589423).  Saving model ...\n",
            "Validation loss decreased (0.589423 --> 0.589251).  Saving model ...\n",
            "Validation loss decreased (0.589251 --> 0.589079).  Saving model ...\n",
            "Epoch: 1990 \tTraining Loss: 0.611619 \tValidation Loss: 0.588905\n",
            "Validation loss decreased (0.589079 --> 0.588905).  Saving model ...\n",
            "Validation loss decreased (0.588905 --> 0.588731).  Saving model ...\n",
            "Validation loss decreased (0.588731 --> 0.588559).  Saving model ...\n",
            "Validation loss decreased (0.588559 --> 0.588385).  Saving model ...\n",
            "Validation loss decreased (0.588385 --> 0.588213).  Saving model ...\n",
            "Validation loss decreased (0.588213 --> 0.588041).  Saving model ...\n",
            "Validation loss decreased (0.588041 --> 0.587869).  Saving model ...\n",
            "Validation loss decreased (0.587869 --> 0.587696).  Saving model ...\n",
            "Validation loss decreased (0.587696 --> 0.587523).  Saving model ...\n",
            "Validation loss decreased (0.587523 --> 0.587350).  Saving model ...\n",
            "Epoch: 2000 \tTraining Loss: 0.622850 \tValidation Loss: 0.587178\n",
            "Validation loss decreased (0.587350 --> 0.587178).  Saving model ...\n",
            "Validation loss decreased (0.587178 --> 0.587005).  Saving model ...\n",
            "Validation loss decreased (0.587005 --> 0.586832).  Saving model ...\n",
            "Validation loss decreased (0.586832 --> 0.586659).  Saving model ...\n",
            "Validation loss decreased (0.586659 --> 0.586485).  Saving model ...\n",
            "Validation loss decreased (0.586485 --> 0.586312).  Saving model ...\n",
            "Validation loss decreased (0.586312 --> 0.586139).  Saving model ...\n",
            "Validation loss decreased (0.586139 --> 0.585966).  Saving model ...\n",
            "Validation loss decreased (0.585966 --> 0.585793).  Saving model ...\n",
            "Validation loss decreased (0.585793 --> 0.585622).  Saving model ...\n",
            "Epoch: 2010 \tTraining Loss: 0.618602 \tValidation Loss: 0.585449\n",
            "Validation loss decreased (0.585622 --> 0.585449).  Saving model ...\n",
            "Validation loss decreased (0.585449 --> 0.585277).  Saving model ...\n",
            "Validation loss decreased (0.585277 --> 0.585104).  Saving model ...\n",
            "Validation loss decreased (0.585104 --> 0.584931).  Saving model ...\n",
            "Validation loss decreased (0.584931 --> 0.584758).  Saving model ...\n",
            "Validation loss decreased (0.584758 --> 0.584584).  Saving model ...\n",
            "Validation loss decreased (0.584584 --> 0.584411).  Saving model ...\n",
            "Validation loss decreased (0.584411 --> 0.584238).  Saving model ...\n",
            "Validation loss decreased (0.584238 --> 0.584064).  Saving model ...\n",
            "Validation loss decreased (0.584064 --> 0.583891).  Saving model ...\n",
            "Epoch: 2020 \tTraining Loss: 0.620657 \tValidation Loss: 0.583718\n",
            "Validation loss decreased (0.583891 --> 0.583718).  Saving model ...\n",
            "Validation loss decreased (0.583718 --> 0.583546).  Saving model ...\n",
            "Validation loss decreased (0.583546 --> 0.583372).  Saving model ...\n",
            "Validation loss decreased (0.583372 --> 0.583198).  Saving model ...\n",
            "Validation loss decreased (0.583198 --> 0.583026).  Saving model ...\n",
            "Validation loss decreased (0.583026 --> 0.582853).  Saving model ...\n",
            "Validation loss decreased (0.582853 --> 0.582679).  Saving model ...\n",
            "Validation loss decreased (0.582679 --> 0.582506).  Saving model ...\n",
            "Validation loss decreased (0.582506 --> 0.582333).  Saving model ...\n",
            "Validation loss decreased (0.582333 --> 0.582159).  Saving model ...\n",
            "Epoch: 2030 \tTraining Loss: 0.614440 \tValidation Loss: 0.581985\n",
            "Validation loss decreased (0.582159 --> 0.581985).  Saving model ...\n",
            "Validation loss decreased (0.581985 --> 0.581813).  Saving model ...\n",
            "Validation loss decreased (0.581813 --> 0.581638).  Saving model ...\n",
            "Validation loss decreased (0.581638 --> 0.581464).  Saving model ...\n",
            "Validation loss decreased (0.581464 --> 0.581291).  Saving model ...\n",
            "Validation loss decreased (0.581291 --> 0.581117).  Saving model ...\n",
            "Validation loss decreased (0.581117 --> 0.580944).  Saving model ...\n",
            "Validation loss decreased (0.580944 --> 0.580769).  Saving model ...\n",
            "Validation loss decreased (0.580769 --> 0.580596).  Saving model ...\n",
            "Validation loss decreased (0.580596 --> 0.580423).  Saving model ...\n",
            "Epoch: 2040 \tTraining Loss: 0.611968 \tValidation Loss: 0.580248\n",
            "Validation loss decreased (0.580423 --> 0.580248).  Saving model ...\n",
            "Validation loss decreased (0.580248 --> 0.580077).  Saving model ...\n",
            "Validation loss decreased (0.580077 --> 0.579902).  Saving model ...\n",
            "Validation loss decreased (0.579902 --> 0.579728).  Saving model ...\n",
            "Validation loss decreased (0.579728 --> 0.579554).  Saving model ...\n",
            "Validation loss decreased (0.579554 --> 0.579382).  Saving model ...\n",
            "Validation loss decreased (0.579382 --> 0.579208).  Saving model ...\n",
            "Validation loss decreased (0.579208 --> 0.579034).  Saving model ...\n",
            "Validation loss decreased (0.579034 --> 0.578860).  Saving model ...\n",
            "Validation loss decreased (0.578860 --> 0.578687).  Saving model ...\n",
            "Epoch: 2050 \tTraining Loss: 0.606504 \tValidation Loss: 0.578512\n",
            "Validation loss decreased (0.578687 --> 0.578512).  Saving model ...\n",
            "Validation loss decreased (0.578512 --> 0.578337).  Saving model ...\n",
            "Validation loss decreased (0.578337 --> 0.578163).  Saving model ...\n",
            "Validation loss decreased (0.578163 --> 0.577988).  Saving model ...\n",
            "Validation loss decreased (0.577988 --> 0.577814).  Saving model ...\n",
            "Validation loss decreased (0.577814 --> 0.577641).  Saving model ...\n",
            "Validation loss decreased (0.577641 --> 0.577468).  Saving model ...\n",
            "Validation loss decreased (0.577468 --> 0.577294).  Saving model ...\n",
            "Validation loss decreased (0.577294 --> 0.577119).  Saving model ...\n",
            "Validation loss decreased (0.577119 --> 0.576946).  Saving model ...\n",
            "Epoch: 2060 \tTraining Loss: 0.609727 \tValidation Loss: 0.576771\n",
            "Validation loss decreased (0.576946 --> 0.576771).  Saving model ...\n",
            "Validation loss decreased (0.576771 --> 0.576596).  Saving model ...\n",
            "Validation loss decreased (0.576596 --> 0.576422).  Saving model ...\n",
            "Validation loss decreased (0.576422 --> 0.576248).  Saving model ...\n",
            "Validation loss decreased (0.576248 --> 0.576073).  Saving model ...\n",
            "Validation loss decreased (0.576073 --> 0.575898).  Saving model ...\n",
            "Validation loss decreased (0.575898 --> 0.575723).  Saving model ...\n",
            "Validation loss decreased (0.575723 --> 0.575550).  Saving model ...\n",
            "Validation loss decreased (0.575550 --> 0.575378).  Saving model ...\n",
            "Validation loss decreased (0.575378 --> 0.575202).  Saving model ...\n",
            "Epoch: 2070 \tTraining Loss: 0.607935 \tValidation Loss: 0.575028\n",
            "Validation loss decreased (0.575202 --> 0.575028).  Saving model ...\n",
            "Validation loss decreased (0.575028 --> 0.574853).  Saving model ...\n",
            "Validation loss decreased (0.574853 --> 0.574678).  Saving model ...\n",
            "Validation loss decreased (0.574678 --> 0.574504).  Saving model ...\n",
            "Validation loss decreased (0.574504 --> 0.574329).  Saving model ...\n",
            "Validation loss decreased (0.574329 --> 0.574155).  Saving model ...\n",
            "Validation loss decreased (0.574155 --> 0.573980).  Saving model ...\n",
            "Validation loss decreased (0.573980 --> 0.573807).  Saving model ...\n",
            "Validation loss decreased (0.573807 --> 0.573633).  Saving model ...\n",
            "Validation loss decreased (0.573633 --> 0.573458).  Saving model ...\n",
            "Epoch: 2080 \tTraining Loss: 0.608300 \tValidation Loss: 0.573284\n",
            "Validation loss decreased (0.573458 --> 0.573284).  Saving model ...\n",
            "Validation loss decreased (0.573284 --> 0.573110).  Saving model ...\n",
            "Validation loss decreased (0.573110 --> 0.572935).  Saving model ...\n",
            "Validation loss decreased (0.572935 --> 0.572760).  Saving model ...\n",
            "Validation loss decreased (0.572760 --> 0.572586).  Saving model ...\n",
            "Validation loss decreased (0.572586 --> 0.572413).  Saving model ...\n",
            "Validation loss decreased (0.572413 --> 0.572239).  Saving model ...\n",
            "Validation loss decreased (0.572239 --> 0.572065).  Saving model ...\n",
            "Validation loss decreased (0.572065 --> 0.571891).  Saving model ...\n",
            "Validation loss decreased (0.571891 --> 0.571716).  Saving model ...\n",
            "Epoch: 2090 \tTraining Loss: 0.608811 \tValidation Loss: 0.571543\n",
            "Validation loss decreased (0.571716 --> 0.571543).  Saving model ...\n",
            "Validation loss decreased (0.571543 --> 0.571368).  Saving model ...\n",
            "Validation loss decreased (0.571368 --> 0.571194).  Saving model ...\n",
            "Validation loss decreased (0.571194 --> 0.571017).  Saving model ...\n",
            "Validation loss decreased (0.571017 --> 0.570843).  Saving model ...\n",
            "Validation loss decreased (0.570843 --> 0.570669).  Saving model ...\n",
            "Validation loss decreased (0.570669 --> 0.570493).  Saving model ...\n",
            "Validation loss decreased (0.570493 --> 0.570318).  Saving model ...\n",
            "Validation loss decreased (0.570318 --> 0.570142).  Saving model ...\n",
            "Validation loss decreased (0.570142 --> 0.569967).  Saving model ...\n",
            "Epoch: 2100 \tTraining Loss: 0.606268 \tValidation Loss: 0.569793\n",
            "Validation loss decreased (0.569967 --> 0.569793).  Saving model ...\n",
            "Validation loss decreased (0.569793 --> 0.569618).  Saving model ...\n",
            "Validation loss decreased (0.569618 --> 0.569442).  Saving model ...\n",
            "Validation loss decreased (0.569442 --> 0.569267).  Saving model ...\n",
            "Validation loss decreased (0.569267 --> 0.569090).  Saving model ...\n",
            "Validation loss decreased (0.569090 --> 0.568916).  Saving model ...\n",
            "Validation loss decreased (0.568916 --> 0.568740).  Saving model ...\n",
            "Validation loss decreased (0.568740 --> 0.568566).  Saving model ...\n",
            "Validation loss decreased (0.568566 --> 0.568390).  Saving model ...\n",
            "Validation loss decreased (0.568390 --> 0.568215).  Saving model ...\n",
            "Epoch: 2110 \tTraining Loss: 0.605137 \tValidation Loss: 0.568040\n",
            "Validation loss decreased (0.568215 --> 0.568040).  Saving model ...\n",
            "Validation loss decreased (0.568040 --> 0.567865).  Saving model ...\n",
            "Validation loss decreased (0.567865 --> 0.567690).  Saving model ...\n",
            "Validation loss decreased (0.567690 --> 0.567514).  Saving model ...\n",
            "Validation loss decreased (0.567514 --> 0.567338).  Saving model ...\n",
            "Validation loss decreased (0.567338 --> 0.567162).  Saving model ...\n",
            "Validation loss decreased (0.567162 --> 0.566987).  Saving model ...\n",
            "Validation loss decreased (0.566987 --> 0.566812).  Saving model ...\n",
            "Validation loss decreased (0.566812 --> 0.566637).  Saving model ...\n",
            "Validation loss decreased (0.566637 --> 0.566462).  Saving model ...\n",
            "Epoch: 2120 \tTraining Loss: 0.602948 \tValidation Loss: 0.566286\n",
            "Validation loss decreased (0.566462 --> 0.566286).  Saving model ...\n",
            "Validation loss decreased (0.566286 --> 0.566111).  Saving model ...\n",
            "Validation loss decreased (0.566111 --> 0.565936).  Saving model ...\n",
            "Validation loss decreased (0.565936 --> 0.565760).  Saving model ...\n",
            "Validation loss decreased (0.565760 --> 0.565585).  Saving model ...\n",
            "Validation loss decreased (0.565585 --> 0.565409).  Saving model ...\n",
            "Validation loss decreased (0.565409 --> 0.565234).  Saving model ...\n",
            "Validation loss decreased (0.565234 --> 0.565059).  Saving model ...\n",
            "Validation loss decreased (0.565059 --> 0.564882).  Saving model ...\n",
            "Validation loss decreased (0.564882 --> 0.564707).  Saving model ...\n",
            "Epoch: 2130 \tTraining Loss: 0.601409 \tValidation Loss: 0.564532\n",
            "Validation loss decreased (0.564707 --> 0.564532).  Saving model ...\n",
            "Validation loss decreased (0.564532 --> 0.564357).  Saving model ...\n",
            "Validation loss decreased (0.564357 --> 0.564182).  Saving model ...\n",
            "Validation loss decreased (0.564182 --> 0.564006).  Saving model ...\n",
            "Validation loss decreased (0.564006 --> 0.563830).  Saving model ...\n",
            "Validation loss decreased (0.563830 --> 0.563654).  Saving model ...\n",
            "Validation loss decreased (0.563654 --> 0.563478).  Saving model ...\n",
            "Validation loss decreased (0.563478 --> 0.563302).  Saving model ...\n",
            "Validation loss decreased (0.563302 --> 0.563126).  Saving model ...\n",
            "Validation loss decreased (0.563126 --> 0.562950).  Saving model ...\n",
            "Epoch: 2140 \tTraining Loss: 0.598435 \tValidation Loss: 0.562774\n",
            "Validation loss decreased (0.562950 --> 0.562774).  Saving model ...\n",
            "Validation loss decreased (0.562774 --> 0.562597).  Saving model ...\n",
            "Validation loss decreased (0.562597 --> 0.562421).  Saving model ...\n",
            "Validation loss decreased (0.562421 --> 0.562244).  Saving model ...\n",
            "Validation loss decreased (0.562244 --> 0.562069).  Saving model ...\n",
            "Validation loss decreased (0.562069 --> 0.561894).  Saving model ...\n",
            "Validation loss decreased (0.561894 --> 0.561718).  Saving model ...\n",
            "Validation loss decreased (0.561718 --> 0.561541).  Saving model ...\n",
            "Validation loss decreased (0.561541 --> 0.561366).  Saving model ...\n",
            "Validation loss decreased (0.561366 --> 0.561189).  Saving model ...\n",
            "Epoch: 2150 \tTraining Loss: 0.596382 \tValidation Loss: 0.561013\n",
            "Validation loss decreased (0.561189 --> 0.561013).  Saving model ...\n",
            "Validation loss decreased (0.561013 --> 0.560837).  Saving model ...\n",
            "Validation loss decreased (0.560837 --> 0.560661).  Saving model ...\n",
            "Validation loss decreased (0.560661 --> 0.560484).  Saving model ...\n",
            "Validation loss decreased (0.560484 --> 0.560308).  Saving model ...\n",
            "Validation loss decreased (0.560308 --> 0.560131).  Saving model ...\n",
            "Validation loss decreased (0.560131 --> 0.559954).  Saving model ...\n",
            "Validation loss decreased (0.559954 --> 0.559778).  Saving model ...\n",
            "Validation loss decreased (0.559778 --> 0.559603).  Saving model ...\n",
            "Validation loss decreased (0.559603 --> 0.559427).  Saving model ...\n",
            "Epoch: 2160 \tTraining Loss: 0.593558 \tValidation Loss: 0.559251\n",
            "Validation loss decreased (0.559427 --> 0.559251).  Saving model ...\n",
            "Validation loss decreased (0.559251 --> 0.559075).  Saving model ...\n",
            "Validation loss decreased (0.559075 --> 0.558899).  Saving model ...\n",
            "Validation loss decreased (0.558899 --> 0.558724).  Saving model ...\n",
            "Validation loss decreased (0.558724 --> 0.558548).  Saving model ...\n",
            "Validation loss decreased (0.558548 --> 0.558372).  Saving model ...\n",
            "Validation loss decreased (0.558372 --> 0.558195).  Saving model ...\n",
            "Validation loss decreased (0.558195 --> 0.558018).  Saving model ...\n",
            "Validation loss decreased (0.558018 --> 0.557842).  Saving model ...\n",
            "Validation loss decreased (0.557842 --> 0.557664).  Saving model ...\n",
            "Epoch: 2170 \tTraining Loss: 0.595545 \tValidation Loss: 0.557488\n",
            "Validation loss decreased (0.557664 --> 0.557488).  Saving model ...\n",
            "Validation loss decreased (0.557488 --> 0.557310).  Saving model ...\n",
            "Validation loss decreased (0.557310 --> 0.557133).  Saving model ...\n",
            "Validation loss decreased (0.557133 --> 0.556957).  Saving model ...\n",
            "Validation loss decreased (0.556957 --> 0.556780).  Saving model ...\n",
            "Validation loss decreased (0.556780 --> 0.556604).  Saving model ...\n",
            "Validation loss decreased (0.556604 --> 0.556428).  Saving model ...\n",
            "Validation loss decreased (0.556428 --> 0.556250).  Saving model ...\n",
            "Validation loss decreased (0.556250 --> 0.556074).  Saving model ...\n",
            "Validation loss decreased (0.556074 --> 0.555897).  Saving model ...\n",
            "Epoch: 2180 \tTraining Loss: 0.591470 \tValidation Loss: 0.555721\n",
            "Validation loss decreased (0.555897 --> 0.555721).  Saving model ...\n",
            "Validation loss decreased (0.555721 --> 0.555544).  Saving model ...\n",
            "Validation loss decreased (0.555544 --> 0.555368).  Saving model ...\n",
            "Validation loss decreased (0.555368 --> 0.555191).  Saving model ...\n",
            "Validation loss decreased (0.555191 --> 0.555014).  Saving model ...\n",
            "Validation loss decreased (0.555014 --> 0.554837).  Saving model ...\n",
            "Validation loss decreased (0.554837 --> 0.554660).  Saving model ...\n",
            "Validation loss decreased (0.554660 --> 0.554483).  Saving model ...\n",
            "Validation loss decreased (0.554483 --> 0.554307).  Saving model ...\n",
            "Validation loss decreased (0.554307 --> 0.554129).  Saving model ...\n",
            "Epoch: 2190 \tTraining Loss: 0.589444 \tValidation Loss: 0.553953\n",
            "Validation loss decreased (0.554129 --> 0.553953).  Saving model ...\n",
            "Validation loss decreased (0.553953 --> 0.553776).  Saving model ...\n",
            "Validation loss decreased (0.553776 --> 0.553598).  Saving model ...\n",
            "Validation loss decreased (0.553598 --> 0.553421).  Saving model ...\n",
            "Validation loss decreased (0.553421 --> 0.553244).  Saving model ...\n",
            "Validation loss decreased (0.553244 --> 0.553067).  Saving model ...\n",
            "Validation loss decreased (0.553067 --> 0.552889).  Saving model ...\n",
            "Validation loss decreased (0.552889 --> 0.552713).  Saving model ...\n",
            "Validation loss decreased (0.552713 --> 0.552537).  Saving model ...\n",
            "Validation loss decreased (0.552537 --> 0.552359).  Saving model ...\n",
            "Epoch: 2200 \tTraining Loss: 0.589987 \tValidation Loss: 0.552182\n",
            "Validation loss decreased (0.552359 --> 0.552182).  Saving model ...\n",
            "Validation loss decreased (0.552182 --> 0.552005).  Saving model ...\n",
            "Validation loss decreased (0.552005 --> 0.551828).  Saving model ...\n",
            "Validation loss decreased (0.551828 --> 0.551651).  Saving model ...\n",
            "Validation loss decreased (0.551651 --> 0.551473).  Saving model ...\n",
            "Validation loss decreased (0.551473 --> 0.551294).  Saving model ...\n",
            "Validation loss decreased (0.551294 --> 0.551116).  Saving model ...\n",
            "Validation loss decreased (0.551116 --> 0.550939).  Saving model ...\n",
            "Validation loss decreased (0.550939 --> 0.550761).  Saving model ...\n",
            "Validation loss decreased (0.550761 --> 0.550583).  Saving model ...\n",
            "Epoch: 2210 \tTraining Loss: 0.584082 \tValidation Loss: 0.550405\n",
            "Validation loss decreased (0.550583 --> 0.550405).  Saving model ...\n",
            "Validation loss decreased (0.550405 --> 0.550227).  Saving model ...\n",
            "Validation loss decreased (0.550227 --> 0.550051).  Saving model ...\n",
            "Validation loss decreased (0.550051 --> 0.549872).  Saving model ...\n",
            "Validation loss decreased (0.549872 --> 0.549694).  Saving model ...\n",
            "Validation loss decreased (0.549694 --> 0.549515).  Saving model ...\n",
            "Validation loss decreased (0.549515 --> 0.549337).  Saving model ...\n",
            "Validation loss decreased (0.549337 --> 0.549160).  Saving model ...\n",
            "Validation loss decreased (0.549160 --> 0.548982).  Saving model ...\n",
            "Validation loss decreased (0.548982 --> 0.548804).  Saving model ...\n",
            "Epoch: 2220 \tTraining Loss: 0.583875 \tValidation Loss: 0.548626\n",
            "Validation loss decreased (0.548804 --> 0.548626).  Saving model ...\n",
            "Validation loss decreased (0.548626 --> 0.548447).  Saving model ...\n",
            "Validation loss decreased (0.548447 --> 0.548269).  Saving model ...\n",
            "Validation loss decreased (0.548269 --> 0.548092).  Saving model ...\n",
            "Validation loss decreased (0.548092 --> 0.547913).  Saving model ...\n",
            "Validation loss decreased (0.547913 --> 0.547736).  Saving model ...\n",
            "Validation loss decreased (0.547736 --> 0.547558).  Saving model ...\n",
            "Validation loss decreased (0.547558 --> 0.547381).  Saving model ...\n",
            "Validation loss decreased (0.547381 --> 0.547202).  Saving model ...\n",
            "Validation loss decreased (0.547202 --> 0.547024).  Saving model ...\n",
            "Epoch: 2230 \tTraining Loss: 0.580524 \tValidation Loss: 0.546846\n",
            "Validation loss decreased (0.547024 --> 0.546846).  Saving model ...\n",
            "Validation loss decreased (0.546846 --> 0.546667).  Saving model ...\n",
            "Validation loss decreased (0.546667 --> 0.546490).  Saving model ...\n",
            "Validation loss decreased (0.546490 --> 0.546313).  Saving model ...\n",
            "Validation loss decreased (0.546313 --> 0.546134).  Saving model ...\n",
            "Validation loss decreased (0.546134 --> 0.545956).  Saving model ...\n",
            "Validation loss decreased (0.545956 --> 0.545778).  Saving model ...\n",
            "Validation loss decreased (0.545778 --> 0.545600).  Saving model ...\n",
            "Validation loss decreased (0.545600 --> 0.545422).  Saving model ...\n",
            "Validation loss decreased (0.545422 --> 0.545245).  Saving model ...\n",
            "Epoch: 2240 \tTraining Loss: 0.586404 \tValidation Loss: 0.545067\n",
            "Validation loss decreased (0.545245 --> 0.545067).  Saving model ...\n",
            "Validation loss decreased (0.545067 --> 0.544888).  Saving model ...\n",
            "Validation loss decreased (0.544888 --> 0.544711).  Saving model ...\n",
            "Validation loss decreased (0.544711 --> 0.544533).  Saving model ...\n",
            "Validation loss decreased (0.544533 --> 0.544355).  Saving model ...\n",
            "Validation loss decreased (0.544355 --> 0.544177).  Saving model ...\n",
            "Validation loss decreased (0.544177 --> 0.543999).  Saving model ...\n",
            "Validation loss decreased (0.543999 --> 0.543821).  Saving model ...\n",
            "Validation loss decreased (0.543821 --> 0.543644).  Saving model ...\n",
            "Validation loss decreased (0.543644 --> 0.543466).  Saving model ...\n",
            "Epoch: 2250 \tTraining Loss: 0.575784 \tValidation Loss: 0.543287\n",
            "Validation loss decreased (0.543466 --> 0.543287).  Saving model ...\n",
            "Validation loss decreased (0.543287 --> 0.543109).  Saving model ...\n",
            "Validation loss decreased (0.543109 --> 0.542930).  Saving model ...\n",
            "Validation loss decreased (0.542930 --> 0.542752).  Saving model ...\n",
            "Validation loss decreased (0.542752 --> 0.542574).  Saving model ...\n",
            "Validation loss decreased (0.542574 --> 0.542395).  Saving model ...\n",
            "Validation loss decreased (0.542395 --> 0.542216).  Saving model ...\n",
            "Validation loss decreased (0.542216 --> 0.542037).  Saving model ...\n",
            "Validation loss decreased (0.542037 --> 0.541858).  Saving model ...\n",
            "Validation loss decreased (0.541858 --> 0.541680).  Saving model ...\n",
            "Epoch: 2260 \tTraining Loss: 0.576958 \tValidation Loss: 0.541501\n",
            "Validation loss decreased (0.541680 --> 0.541501).  Saving model ...\n",
            "Validation loss decreased (0.541501 --> 0.541322).  Saving model ...\n",
            "Validation loss decreased (0.541322 --> 0.541144).  Saving model ...\n",
            "Validation loss decreased (0.541144 --> 0.540965).  Saving model ...\n",
            "Validation loss decreased (0.540965 --> 0.540785).  Saving model ...\n",
            "Validation loss decreased (0.540785 --> 0.540606).  Saving model ...\n",
            "Validation loss decreased (0.540606 --> 0.540426).  Saving model ...\n",
            "Validation loss decreased (0.540426 --> 0.540247).  Saving model ...\n",
            "Validation loss decreased (0.540247 --> 0.540068).  Saving model ...\n",
            "Validation loss decreased (0.540068 --> 0.539889).  Saving model ...\n",
            "Epoch: 2270 \tTraining Loss: 0.579960 \tValidation Loss: 0.539710\n",
            "Validation loss decreased (0.539889 --> 0.539710).  Saving model ...\n",
            "Validation loss decreased (0.539710 --> 0.539531).  Saving model ...\n",
            "Validation loss decreased (0.539531 --> 0.539353).  Saving model ...\n",
            "Validation loss decreased (0.539353 --> 0.539174).  Saving model ...\n",
            "Validation loss decreased (0.539174 --> 0.538994).  Saving model ...\n",
            "Validation loss decreased (0.538994 --> 0.538814).  Saving model ...\n",
            "Validation loss decreased (0.538814 --> 0.538635).  Saving model ...\n",
            "Validation loss decreased (0.538635 --> 0.538456).  Saving model ...\n",
            "Validation loss decreased (0.538456 --> 0.538277).  Saving model ...\n",
            "Validation loss decreased (0.538277 --> 0.538098).  Saving model ...\n",
            "Epoch: 2280 \tTraining Loss: 0.571153 \tValidation Loss: 0.537919\n",
            "Validation loss decreased (0.538098 --> 0.537919).  Saving model ...\n",
            "Validation loss decreased (0.537919 --> 0.537740).  Saving model ...\n",
            "Validation loss decreased (0.537740 --> 0.537561).  Saving model ...\n",
            "Validation loss decreased (0.537561 --> 0.537382).  Saving model ...\n",
            "Validation loss decreased (0.537382 --> 0.537203).  Saving model ...\n",
            "Validation loss decreased (0.537203 --> 0.537024).  Saving model ...\n",
            "Validation loss decreased (0.537024 --> 0.536845).  Saving model ...\n",
            "Validation loss decreased (0.536845 --> 0.536665).  Saving model ...\n",
            "Validation loss decreased (0.536665 --> 0.536486).  Saving model ...\n",
            "Validation loss decreased (0.536486 --> 0.536305).  Saving model ...\n",
            "Epoch: 2290 \tTraining Loss: 0.568778 \tValidation Loss: 0.536126\n",
            "Validation loss decreased (0.536305 --> 0.536126).  Saving model ...\n",
            "Validation loss decreased (0.536126 --> 0.535946).  Saving model ...\n",
            "Validation loss decreased (0.535946 --> 0.535766).  Saving model ...\n",
            "Validation loss decreased (0.535766 --> 0.535586).  Saving model ...\n",
            "Validation loss decreased (0.535586 --> 0.535407).  Saving model ...\n",
            "Validation loss decreased (0.535407 --> 0.535228).  Saving model ...\n",
            "Validation loss decreased (0.535228 --> 0.535048).  Saving model ...\n",
            "Validation loss decreased (0.535048 --> 0.534868).  Saving model ...\n",
            "Validation loss decreased (0.534868 --> 0.534688).  Saving model ...\n",
            "Validation loss decreased (0.534688 --> 0.534509).  Saving model ...\n",
            "Epoch: 2300 \tTraining Loss: 0.577891 \tValidation Loss: 0.534330\n",
            "Validation loss decreased (0.534509 --> 0.534330).  Saving model ...\n",
            "Validation loss decreased (0.534330 --> 0.534150).  Saving model ...\n",
            "Validation loss decreased (0.534150 --> 0.533972).  Saving model ...\n",
            "Validation loss decreased (0.533972 --> 0.533792).  Saving model ...\n",
            "Validation loss decreased (0.533792 --> 0.533612).  Saving model ...\n",
            "Validation loss decreased (0.533612 --> 0.533432).  Saving model ...\n",
            "Validation loss decreased (0.533432 --> 0.533252).  Saving model ...\n",
            "Validation loss decreased (0.533252 --> 0.533071).  Saving model ...\n",
            "Validation loss decreased (0.533071 --> 0.532892).  Saving model ...\n",
            "Validation loss decreased (0.532892 --> 0.532712).  Saving model ...\n",
            "Epoch: 2310 \tTraining Loss: 0.563973 \tValidation Loss: 0.532532\n",
            "Validation loss decreased (0.532712 --> 0.532532).  Saving model ...\n",
            "Validation loss decreased (0.532532 --> 0.532351).  Saving model ...\n",
            "Validation loss decreased (0.532351 --> 0.532171).  Saving model ...\n",
            "Validation loss decreased (0.532171 --> 0.531991).  Saving model ...\n",
            "Validation loss decreased (0.531991 --> 0.531810).  Saving model ...\n",
            "Validation loss decreased (0.531810 --> 0.531629).  Saving model ...\n",
            "Validation loss decreased (0.531629 --> 0.531449).  Saving model ...\n",
            "Validation loss decreased (0.531449 --> 0.531268).  Saving model ...\n",
            "Validation loss decreased (0.531268 --> 0.531088).  Saving model ...\n",
            "Validation loss decreased (0.531088 --> 0.530907).  Saving model ...\n",
            "Epoch: 2320 \tTraining Loss: 0.561772 \tValidation Loss: 0.530726\n",
            "Validation loss decreased (0.530907 --> 0.530726).  Saving model ...\n",
            "Validation loss decreased (0.530726 --> 0.530543).  Saving model ...\n",
            "Validation loss decreased (0.530543 --> 0.530363).  Saving model ...\n",
            "Validation loss decreased (0.530363 --> 0.530183).  Saving model ...\n",
            "Validation loss decreased (0.530183 --> 0.530002).  Saving model ...\n",
            "Validation loss decreased (0.530002 --> 0.529821).  Saving model ...\n",
            "Validation loss decreased (0.529821 --> 0.529641).  Saving model ...\n",
            "Validation loss decreased (0.529641 --> 0.529460).  Saving model ...\n",
            "Validation loss decreased (0.529460 --> 0.529280).  Saving model ...\n",
            "Validation loss decreased (0.529280 --> 0.529100).  Saving model ...\n",
            "Epoch: 2330 \tTraining Loss: 0.565300 \tValidation Loss: 0.528920\n",
            "Validation loss decreased (0.529100 --> 0.528920).  Saving model ...\n",
            "Validation loss decreased (0.528920 --> 0.528739).  Saving model ...\n",
            "Validation loss decreased (0.528739 --> 0.528558).  Saving model ...\n",
            "Validation loss decreased (0.528558 --> 0.528377).  Saving model ...\n",
            "Validation loss decreased (0.528377 --> 0.528197).  Saving model ...\n",
            "Validation loss decreased (0.528197 --> 0.528016).  Saving model ...\n",
            "Validation loss decreased (0.528016 --> 0.527836).  Saving model ...\n",
            "Validation loss decreased (0.527836 --> 0.527655).  Saving model ...\n",
            "Validation loss decreased (0.527655 --> 0.527475).  Saving model ...\n",
            "Validation loss decreased (0.527475 --> 0.527296).  Saving model ...\n",
            "Epoch: 2340 \tTraining Loss: 0.568856 \tValidation Loss: 0.527115\n",
            "Validation loss decreased (0.527296 --> 0.527115).  Saving model ...\n",
            "Validation loss decreased (0.527115 --> 0.526935).  Saving model ...\n",
            "Validation loss decreased (0.526935 --> 0.526755).  Saving model ...\n",
            "Validation loss decreased (0.526755 --> 0.526573).  Saving model ...\n",
            "Validation loss decreased (0.526573 --> 0.526392).  Saving model ...\n",
            "Validation loss decreased (0.526392 --> 0.526212).  Saving model ...\n",
            "Validation loss decreased (0.526212 --> 0.526031).  Saving model ...\n",
            "Validation loss decreased (0.526031 --> 0.525850).  Saving model ...\n",
            "Validation loss decreased (0.525850 --> 0.525669).  Saving model ...\n",
            "Validation loss decreased (0.525669 --> 0.525489).  Saving model ...\n",
            "Epoch: 2350 \tTraining Loss: 0.563035 \tValidation Loss: 0.525308\n",
            "Validation loss decreased (0.525489 --> 0.525308).  Saving model ...\n",
            "Validation loss decreased (0.525308 --> 0.525127).  Saving model ...\n",
            "Validation loss decreased (0.525127 --> 0.524944).  Saving model ...\n",
            "Validation loss decreased (0.524944 --> 0.524762).  Saving model ...\n",
            "Validation loss decreased (0.524762 --> 0.524582).  Saving model ...\n",
            "Validation loss decreased (0.524582 --> 0.524401).  Saving model ...\n",
            "Validation loss decreased (0.524401 --> 0.524219).  Saving model ...\n",
            "Validation loss decreased (0.524219 --> 0.524038).  Saving model ...\n",
            "Validation loss decreased (0.524038 --> 0.523857).  Saving model ...\n",
            "Validation loss decreased (0.523857 --> 0.523676).  Saving model ...\n",
            "Epoch: 2360 \tTraining Loss: 0.564729 \tValidation Loss: 0.523495\n",
            "Validation loss decreased (0.523676 --> 0.523495).  Saving model ...\n",
            "Validation loss decreased (0.523495 --> 0.523315).  Saving model ...\n",
            "Validation loss decreased (0.523315 --> 0.523135).  Saving model ...\n",
            "Validation loss decreased (0.523135 --> 0.522954).  Saving model ...\n",
            "Validation loss decreased (0.522954 --> 0.522773).  Saving model ...\n",
            "Validation loss decreased (0.522773 --> 0.522592).  Saving model ...\n",
            "Validation loss decreased (0.522592 --> 0.522411).  Saving model ...\n",
            "Validation loss decreased (0.522411 --> 0.522232).  Saving model ...\n",
            "Validation loss decreased (0.522232 --> 0.522051).  Saving model ...\n",
            "Validation loss decreased (0.522051 --> 0.521869).  Saving model ...\n",
            "Epoch: 2370 \tTraining Loss: 0.563247 \tValidation Loss: 0.521688\n",
            "Validation loss decreased (0.521869 --> 0.521688).  Saving model ...\n",
            "Validation loss decreased (0.521688 --> 0.521506).  Saving model ...\n",
            "Validation loss decreased (0.521506 --> 0.521325).  Saving model ...\n",
            "Validation loss decreased (0.521325 --> 0.521143).  Saving model ...\n",
            "Validation loss decreased (0.521143 --> 0.520962).  Saving model ...\n",
            "Validation loss decreased (0.520962 --> 0.520779).  Saving model ...\n",
            "Validation loss decreased (0.520779 --> 0.520597).  Saving model ...\n",
            "Validation loss decreased (0.520597 --> 0.520415).  Saving model ...\n",
            "Validation loss decreased (0.520415 --> 0.520233).  Saving model ...\n",
            "Validation loss decreased (0.520233 --> 0.520051).  Saving model ...\n",
            "Epoch: 2380 \tTraining Loss: 0.561870 \tValidation Loss: 0.519871\n",
            "Validation loss decreased (0.520051 --> 0.519871).  Saving model ...\n",
            "Validation loss decreased (0.519871 --> 0.519689).  Saving model ...\n",
            "Validation loss decreased (0.519689 --> 0.519508).  Saving model ...\n",
            "Validation loss decreased (0.519508 --> 0.519325).  Saving model ...\n",
            "Validation loss decreased (0.519325 --> 0.519144).  Saving model ...\n",
            "Validation loss decreased (0.519144 --> 0.518963).  Saving model ...\n",
            "Validation loss decreased (0.518963 --> 0.518782).  Saving model ...\n",
            "Validation loss decreased (0.518782 --> 0.518601).  Saving model ...\n",
            "Validation loss decreased (0.518601 --> 0.518419).  Saving model ...\n",
            "Validation loss decreased (0.518419 --> 0.518236).  Saving model ...\n",
            "Epoch: 2390 \tTraining Loss: 0.555134 \tValidation Loss: 0.518055\n",
            "Validation loss decreased (0.518236 --> 0.518055).  Saving model ...\n",
            "Validation loss decreased (0.518055 --> 0.517873).  Saving model ...\n",
            "Validation loss decreased (0.517873 --> 0.517691).  Saving model ...\n",
            "Validation loss decreased (0.517691 --> 0.517509).  Saving model ...\n",
            "Validation loss decreased (0.517509 --> 0.517327).  Saving model ...\n",
            "Validation loss decreased (0.517327 --> 0.517143).  Saving model ...\n",
            "Validation loss decreased (0.517143 --> 0.516961).  Saving model ...\n",
            "Validation loss decreased (0.516961 --> 0.516776).  Saving model ...\n",
            "Validation loss decreased (0.516776 --> 0.516594).  Saving model ...\n",
            "Validation loss decreased (0.516594 --> 0.516412).  Saving model ...\n",
            "Epoch: 2400 \tTraining Loss: 0.546917 \tValidation Loss: 0.516229\n",
            "Validation loss decreased (0.516412 --> 0.516229).  Saving model ...\n",
            "Validation loss decreased (0.516229 --> 0.516047).  Saving model ...\n",
            "Validation loss decreased (0.516047 --> 0.515864).  Saving model ...\n",
            "Validation loss decreased (0.515864 --> 0.515682).  Saving model ...\n",
            "Validation loss decreased (0.515682 --> 0.515499).  Saving model ...\n",
            "Validation loss decreased (0.515499 --> 0.515317).  Saving model ...\n",
            "Validation loss decreased (0.515317 --> 0.515134).  Saving model ...\n",
            "Validation loss decreased (0.515134 --> 0.514951).  Saving model ...\n",
            "Validation loss decreased (0.514951 --> 0.514768).  Saving model ...\n",
            "Validation loss decreased (0.514768 --> 0.514586).  Saving model ...\n",
            "Epoch: 2410 \tTraining Loss: 0.556139 \tValidation Loss: 0.514403\n",
            "Validation loss decreased (0.514586 --> 0.514403).  Saving model ...\n",
            "Validation loss decreased (0.514403 --> 0.514220).  Saving model ...\n",
            "Validation loss decreased (0.514220 --> 0.514036).  Saving model ...\n",
            "Validation loss decreased (0.514036 --> 0.513853).  Saving model ...\n",
            "Validation loss decreased (0.513853 --> 0.513670).  Saving model ...\n",
            "Validation loss decreased (0.513670 --> 0.513488).  Saving model ...\n",
            "Validation loss decreased (0.513488 --> 0.513305).  Saving model ...\n",
            "Validation loss decreased (0.513305 --> 0.513123).  Saving model ...\n",
            "Validation loss decreased (0.513123 --> 0.512940).  Saving model ...\n",
            "Validation loss decreased (0.512940 --> 0.512758).  Saving model ...\n",
            "Epoch: 2420 \tTraining Loss: 0.549651 \tValidation Loss: 0.512574\n",
            "Validation loss decreased (0.512758 --> 0.512574).  Saving model ...\n",
            "Validation loss decreased (0.512574 --> 0.512391).  Saving model ...\n",
            "Validation loss decreased (0.512391 --> 0.512208).  Saving model ...\n",
            "Validation loss decreased (0.512208 --> 0.512026).  Saving model ...\n",
            "Validation loss decreased (0.512026 --> 0.511842).  Saving model ...\n",
            "Validation loss decreased (0.511842 --> 0.511660).  Saving model ...\n",
            "Validation loss decreased (0.511660 --> 0.511477).  Saving model ...\n",
            "Validation loss decreased (0.511477 --> 0.511295).  Saving model ...\n",
            "Validation loss decreased (0.511295 --> 0.511112).  Saving model ...\n",
            "Validation loss decreased (0.511112 --> 0.510930).  Saving model ...\n",
            "Epoch: 2430 \tTraining Loss: 0.552646 \tValidation Loss: 0.510747\n",
            "Validation loss decreased (0.510930 --> 0.510747).  Saving model ...\n",
            "Validation loss decreased (0.510747 --> 0.510564).  Saving model ...\n",
            "Validation loss decreased (0.510564 --> 0.510381).  Saving model ...\n",
            "Validation loss decreased (0.510381 --> 0.510196).  Saving model ...\n",
            "Validation loss decreased (0.510196 --> 0.510014).  Saving model ...\n",
            "Validation loss decreased (0.510014 --> 0.509831).  Saving model ...\n",
            "Validation loss decreased (0.509831 --> 0.509647).  Saving model ...\n",
            "Validation loss decreased (0.509647 --> 0.509463).  Saving model ...\n",
            "Validation loss decreased (0.509463 --> 0.509280).  Saving model ...\n",
            "Validation loss decreased (0.509280 --> 0.509096).  Saving model ...\n",
            "Epoch: 2440 \tTraining Loss: 0.547348 \tValidation Loss: 0.508912\n",
            "Validation loss decreased (0.509096 --> 0.508912).  Saving model ...\n",
            "Validation loss decreased (0.508912 --> 0.508729).  Saving model ...\n",
            "Validation loss decreased (0.508729 --> 0.508546).  Saving model ...\n",
            "Validation loss decreased (0.508546 --> 0.508363).  Saving model ...\n",
            "Validation loss decreased (0.508363 --> 0.508181).  Saving model ...\n",
            "Validation loss decreased (0.508181 --> 0.507997).  Saving model ...\n",
            "Validation loss decreased (0.507997 --> 0.507813).  Saving model ...\n",
            "Validation loss decreased (0.507813 --> 0.507630).  Saving model ...\n",
            "Validation loss decreased (0.507630 --> 0.507446).  Saving model ...\n",
            "Validation loss decreased (0.507446 --> 0.507263).  Saving model ...\n",
            "Epoch: 2450 \tTraining Loss: 0.548288 \tValidation Loss: 0.507079\n",
            "Validation loss decreased (0.507263 --> 0.507079).  Saving model ...\n",
            "Validation loss decreased (0.507079 --> 0.506896).  Saving model ...\n",
            "Validation loss decreased (0.506896 --> 0.506712).  Saving model ...\n",
            "Validation loss decreased (0.506712 --> 0.506529).  Saving model ...\n",
            "Validation loss decreased (0.506529 --> 0.506346).  Saving model ...\n",
            "Validation loss decreased (0.506346 --> 0.506161).  Saving model ...\n",
            "Validation loss decreased (0.506161 --> 0.505978).  Saving model ...\n",
            "Validation loss decreased (0.505978 --> 0.505794).  Saving model ...\n",
            "Validation loss decreased (0.505794 --> 0.505608).  Saving model ...\n",
            "Validation loss decreased (0.505608 --> 0.505425).  Saving model ...\n",
            "Epoch: 2460 \tTraining Loss: 0.543772 \tValidation Loss: 0.505241\n",
            "Validation loss decreased (0.505425 --> 0.505241).  Saving model ...\n",
            "Validation loss decreased (0.505241 --> 0.505058).  Saving model ...\n",
            "Validation loss decreased (0.505058 --> 0.504875).  Saving model ...\n",
            "Validation loss decreased (0.504875 --> 0.504690).  Saving model ...\n",
            "Validation loss decreased (0.504690 --> 0.504506).  Saving model ...\n",
            "Validation loss decreased (0.504506 --> 0.504322).  Saving model ...\n",
            "Validation loss decreased (0.504322 --> 0.504139).  Saving model ...\n",
            "Validation loss decreased (0.504139 --> 0.503955).  Saving model ...\n",
            "Validation loss decreased (0.503955 --> 0.503770).  Saving model ...\n",
            "Validation loss decreased (0.503770 --> 0.503587).  Saving model ...\n",
            "Epoch: 2470 \tTraining Loss: 0.542741 \tValidation Loss: 0.503403\n",
            "Validation loss decreased (0.503587 --> 0.503403).  Saving model ...\n",
            "Validation loss decreased (0.503403 --> 0.503219).  Saving model ...\n",
            "Validation loss decreased (0.503219 --> 0.503034).  Saving model ...\n",
            "Validation loss decreased (0.503034 --> 0.502850).  Saving model ...\n",
            "Validation loss decreased (0.502850 --> 0.502666).  Saving model ...\n",
            "Validation loss decreased (0.502666 --> 0.502482).  Saving model ...\n",
            "Validation loss decreased (0.502482 --> 0.502299).  Saving model ...\n",
            "Validation loss decreased (0.502299 --> 0.502114).  Saving model ...\n",
            "Validation loss decreased (0.502114 --> 0.501930).  Saving model ...\n",
            "Validation loss decreased (0.501930 --> 0.501745).  Saving model ...\n",
            "Epoch: 2480 \tTraining Loss: 0.542836 \tValidation Loss: 0.501561\n",
            "Validation loss decreased (0.501745 --> 0.501561).  Saving model ...\n",
            "Validation loss decreased (0.501561 --> 0.501378).  Saving model ...\n",
            "Validation loss decreased (0.501378 --> 0.501193).  Saving model ...\n",
            "Validation loss decreased (0.501193 --> 0.501008).  Saving model ...\n",
            "Validation loss decreased (0.501008 --> 0.500823).  Saving model ...\n",
            "Validation loss decreased (0.500823 --> 0.500638).  Saving model ...\n",
            "Validation loss decreased (0.500638 --> 0.500453).  Saving model ...\n",
            "Validation loss decreased (0.500453 --> 0.500268).  Saving model ...\n",
            "Validation loss decreased (0.500268 --> 0.500083).  Saving model ...\n",
            "Validation loss decreased (0.500083 --> 0.499899).  Saving model ...\n",
            "Epoch: 2490 \tTraining Loss: 0.540027 \tValidation Loss: 0.499714\n",
            "Validation loss decreased (0.499899 --> 0.499714).  Saving model ...\n",
            "Validation loss decreased (0.499714 --> 0.499529).  Saving model ...\n",
            "Validation loss decreased (0.499529 --> 0.499344).  Saving model ...\n",
            "Validation loss decreased (0.499344 --> 0.499160).  Saving model ...\n",
            "Validation loss decreased (0.499160 --> 0.498975).  Saving model ...\n",
            "Validation loss decreased (0.498975 --> 0.498790).  Saving model ...\n",
            "Validation loss decreased (0.498790 --> 0.498607).  Saving model ...\n",
            "Validation loss decreased (0.498607 --> 0.498421).  Saving model ...\n",
            "Validation loss decreased (0.498421 --> 0.498238).  Saving model ...\n",
            "Validation loss decreased (0.498238 --> 0.498053).  Saving model ...\n",
            "Epoch: 2500 \tTraining Loss: 0.535412 \tValidation Loss: 0.497868\n",
            "Validation loss decreased (0.498053 --> 0.497868).  Saving model ...\n",
            "Validation loss decreased (0.497868 --> 0.497682).  Saving model ...\n",
            "Validation loss decreased (0.497682 --> 0.497496).  Saving model ...\n",
            "Validation loss decreased (0.497496 --> 0.497312).  Saving model ...\n",
            "Validation loss decreased (0.497312 --> 0.497126).  Saving model ...\n",
            "Validation loss decreased (0.497126 --> 0.496941).  Saving model ...\n",
            "Validation loss decreased (0.496941 --> 0.496756).  Saving model ...\n",
            "Validation loss decreased (0.496756 --> 0.496570).  Saving model ...\n",
            "Validation loss decreased (0.496570 --> 0.496386).  Saving model ...\n",
            "Validation loss decreased (0.496386 --> 0.496202).  Saving model ...\n",
            "Epoch: 2510 \tTraining Loss: 0.532492 \tValidation Loss: 0.496017\n",
            "Validation loss decreased (0.496202 --> 0.496017).  Saving model ...\n",
            "Validation loss decreased (0.496017 --> 0.495832).  Saving model ...\n",
            "Validation loss decreased (0.495832 --> 0.495647).  Saving model ...\n",
            "Validation loss decreased (0.495647 --> 0.495461).  Saving model ...\n",
            "Validation loss decreased (0.495461 --> 0.495276).  Saving model ...\n",
            "Validation loss decreased (0.495276 --> 0.495090).  Saving model ...\n",
            "Validation loss decreased (0.495090 --> 0.494906).  Saving model ...\n",
            "Validation loss decreased (0.494906 --> 0.494721).  Saving model ...\n",
            "Validation loss decreased (0.494721 --> 0.494536).  Saving model ...\n",
            "Validation loss decreased (0.494536 --> 0.494351).  Saving model ...\n",
            "Epoch: 2520 \tTraining Loss: 0.538977 \tValidation Loss: 0.494167\n",
            "Validation loss decreased (0.494351 --> 0.494167).  Saving model ...\n",
            "Validation loss decreased (0.494167 --> 0.493981).  Saving model ...\n",
            "Validation loss decreased (0.493981 --> 0.493796).  Saving model ...\n",
            "Validation loss decreased (0.493796 --> 0.493611).  Saving model ...\n",
            "Validation loss decreased (0.493611 --> 0.493426).  Saving model ...\n",
            "Validation loss decreased (0.493426 --> 0.493240).  Saving model ...\n",
            "Validation loss decreased (0.493240 --> 0.493056).  Saving model ...\n",
            "Validation loss decreased (0.493056 --> 0.492870).  Saving model ...\n",
            "Validation loss decreased (0.492870 --> 0.492682).  Saving model ...\n",
            "Validation loss decreased (0.492682 --> 0.492495).  Saving model ...\n",
            "Epoch: 2530 \tTraining Loss: 0.529422 \tValidation Loss: 0.492309\n",
            "Validation loss decreased (0.492495 --> 0.492309).  Saving model ...\n",
            "Validation loss decreased (0.492309 --> 0.492122).  Saving model ...\n",
            "Validation loss decreased (0.492122 --> 0.491935).  Saving model ...\n",
            "Validation loss decreased (0.491935 --> 0.491749).  Saving model ...\n",
            "Validation loss decreased (0.491749 --> 0.491564).  Saving model ...\n",
            "Validation loss decreased (0.491564 --> 0.491378).  Saving model ...\n",
            "Validation loss decreased (0.491378 --> 0.491193).  Saving model ...\n",
            "Validation loss decreased (0.491193 --> 0.491007).  Saving model ...\n",
            "Validation loss decreased (0.491007 --> 0.490821).  Saving model ...\n",
            "Validation loss decreased (0.490821 --> 0.490635).  Saving model ...\n",
            "Epoch: 2540 \tTraining Loss: 0.528072 \tValidation Loss: 0.490449\n",
            "Validation loss decreased (0.490635 --> 0.490449).  Saving model ...\n",
            "Validation loss decreased (0.490449 --> 0.490263).  Saving model ...\n",
            "Validation loss decreased (0.490263 --> 0.490077).  Saving model ...\n",
            "Validation loss decreased (0.490077 --> 0.489892).  Saving model ...\n",
            "Validation loss decreased (0.489892 --> 0.489705).  Saving model ...\n",
            "Validation loss decreased (0.489705 --> 0.489518).  Saving model ...\n",
            "Validation loss decreased (0.489518 --> 0.489332).  Saving model ...\n",
            "Validation loss decreased (0.489332 --> 0.489146).  Saving model ...\n",
            "Validation loss decreased (0.489146 --> 0.488960).  Saving model ...\n",
            "Validation loss decreased (0.488960 --> 0.488774).  Saving model ...\n",
            "Epoch: 2550 \tTraining Loss: 0.525086 \tValidation Loss: 0.488586\n",
            "Validation loss decreased (0.488774 --> 0.488586).  Saving model ...\n",
            "Validation loss decreased (0.488586 --> 0.488401).  Saving model ...\n",
            "Validation loss decreased (0.488401 --> 0.488214).  Saving model ...\n",
            "Validation loss decreased (0.488214 --> 0.488027).  Saving model ...\n",
            "Validation loss decreased (0.488027 --> 0.487841).  Saving model ...\n",
            "Validation loss decreased (0.487841 --> 0.487654).  Saving model ...\n",
            "Validation loss decreased (0.487654 --> 0.487467).  Saving model ...\n",
            "Validation loss decreased (0.487467 --> 0.487280).  Saving model ...\n",
            "Validation loss decreased (0.487280 --> 0.487093).  Saving model ...\n",
            "Validation loss decreased (0.487093 --> 0.486907).  Saving model ...\n",
            "Epoch: 2560 \tTraining Loss: 0.530650 \tValidation Loss: 0.486720\n",
            "Validation loss decreased (0.486907 --> 0.486720).  Saving model ...\n",
            "Validation loss decreased (0.486720 --> 0.486534).  Saving model ...\n",
            "Validation loss decreased (0.486534 --> 0.486346).  Saving model ...\n",
            "Validation loss decreased (0.486346 --> 0.486159).  Saving model ...\n",
            "Validation loss decreased (0.486159 --> 0.485973).  Saving model ...\n",
            "Validation loss decreased (0.485973 --> 0.485786).  Saving model ...\n",
            "Validation loss decreased (0.485786 --> 0.485600).  Saving model ...\n",
            "Validation loss decreased (0.485600 --> 0.485413).  Saving model ...\n",
            "Validation loss decreased (0.485413 --> 0.485226).  Saving model ...\n",
            "Validation loss decreased (0.485226 --> 0.485039).  Saving model ...\n",
            "Epoch: 2570 \tTraining Loss: 0.519656 \tValidation Loss: 0.484851\n",
            "Validation loss decreased (0.485039 --> 0.484851).  Saving model ...\n",
            "Validation loss decreased (0.484851 --> 0.484664).  Saving model ...\n",
            "Validation loss decreased (0.484664 --> 0.484477).  Saving model ...\n",
            "Validation loss decreased (0.484477 --> 0.484291).  Saving model ...\n",
            "Validation loss decreased (0.484291 --> 0.484105).  Saving model ...\n",
            "Validation loss decreased (0.484105 --> 0.483918).  Saving model ...\n",
            "Validation loss decreased (0.483918 --> 0.483731).  Saving model ...\n",
            "Validation loss decreased (0.483731 --> 0.483544).  Saving model ...\n",
            "Validation loss decreased (0.483544 --> 0.483357).  Saving model ...\n",
            "Validation loss decreased (0.483357 --> 0.483170).  Saving model ...\n",
            "Epoch: 2580 \tTraining Loss: 0.519153 \tValidation Loss: 0.482982\n",
            "Validation loss decreased (0.483170 --> 0.482982).  Saving model ...\n",
            "Validation loss decreased (0.482982 --> 0.482795).  Saving model ...\n",
            "Validation loss decreased (0.482795 --> 0.482608).  Saving model ...\n",
            "Validation loss decreased (0.482608 --> 0.482420).  Saving model ...\n",
            "Validation loss decreased (0.482420 --> 0.482232).  Saving model ...\n",
            "Validation loss decreased (0.482232 --> 0.482045).  Saving model ...\n",
            "Validation loss decreased (0.482045 --> 0.481858).  Saving model ...\n",
            "Validation loss decreased (0.481858 --> 0.481672).  Saving model ...\n",
            "Validation loss decreased (0.481672 --> 0.481483).  Saving model ...\n",
            "Validation loss decreased (0.481483 --> 0.481296).  Saving model ...\n",
            "Epoch: 2590 \tTraining Loss: 0.519821 \tValidation Loss: 0.481109\n",
            "Validation loss decreased (0.481296 --> 0.481109).  Saving model ...\n",
            "Validation loss decreased (0.481109 --> 0.480921).  Saving model ...\n",
            "Validation loss decreased (0.480921 --> 0.480733).  Saving model ...\n",
            "Validation loss decreased (0.480733 --> 0.480546).  Saving model ...\n",
            "Validation loss decreased (0.480546 --> 0.480359).  Saving model ...\n",
            "Validation loss decreased (0.480359 --> 0.480171).  Saving model ...\n",
            "Validation loss decreased (0.480171 --> 0.479984).  Saving model ...\n",
            "Validation loss decreased (0.479984 --> 0.479795).  Saving model ...\n",
            "Validation loss decreased (0.479795 --> 0.479609).  Saving model ...\n",
            "Validation loss decreased (0.479609 --> 0.479421).  Saving model ...\n",
            "Epoch: 2600 \tTraining Loss: 0.525543 \tValidation Loss: 0.479234\n",
            "Validation loss decreased (0.479421 --> 0.479234).  Saving model ...\n",
            "Validation loss decreased (0.479234 --> 0.479047).  Saving model ...\n",
            "Validation loss decreased (0.479047 --> 0.478859).  Saving model ...\n",
            "Validation loss decreased (0.478859 --> 0.478671).  Saving model ...\n",
            "Validation loss decreased (0.478671 --> 0.478484).  Saving model ...\n",
            "Validation loss decreased (0.478484 --> 0.478297).  Saving model ...\n",
            "Validation loss decreased (0.478297 --> 0.478108).  Saving model ...\n",
            "Validation loss decreased (0.478108 --> 0.477922).  Saving model ...\n",
            "Validation loss decreased (0.477922 --> 0.477734).  Saving model ...\n",
            "Validation loss decreased (0.477734 --> 0.477544).  Saving model ...\n",
            "Epoch: 2610 \tTraining Loss: 0.522139 \tValidation Loss: 0.477357\n",
            "Validation loss decreased (0.477544 --> 0.477357).  Saving model ...\n",
            "Validation loss decreased (0.477357 --> 0.477169).  Saving model ...\n",
            "Validation loss decreased (0.477169 --> 0.476981).  Saving model ...\n",
            "Validation loss decreased (0.476981 --> 0.476793).  Saving model ...\n",
            "Validation loss decreased (0.476793 --> 0.476605).  Saving model ...\n",
            "Validation loss decreased (0.476605 --> 0.476416).  Saving model ...\n",
            "Validation loss decreased (0.476416 --> 0.476229).  Saving model ...\n",
            "Validation loss decreased (0.476229 --> 0.476041).  Saving model ...\n",
            "Validation loss decreased (0.476041 --> 0.475854).  Saving model ...\n",
            "Validation loss decreased (0.475854 --> 0.475666).  Saving model ...\n",
            "Epoch: 2620 \tTraining Loss: 0.514892 \tValidation Loss: 0.475477\n",
            "Validation loss decreased (0.475666 --> 0.475477).  Saving model ...\n",
            "Validation loss decreased (0.475477 --> 0.475288).  Saving model ...\n",
            "Validation loss decreased (0.475288 --> 0.475100).  Saving model ...\n",
            "Validation loss decreased (0.475100 --> 0.474911).  Saving model ...\n",
            "Validation loss decreased (0.474911 --> 0.474722).  Saving model ...\n",
            "Validation loss decreased (0.474722 --> 0.474534).  Saving model ...\n",
            "Validation loss decreased (0.474534 --> 0.474346).  Saving model ...\n",
            "Validation loss decreased (0.474346 --> 0.474157).  Saving model ...\n",
            "Validation loss decreased (0.474157 --> 0.473969).  Saving model ...\n",
            "Validation loss decreased (0.473969 --> 0.473780).  Saving model ...\n",
            "Epoch: 2630 \tTraining Loss: 0.525257 \tValidation Loss: 0.473592\n",
            "Validation loss decreased (0.473780 --> 0.473592).  Saving model ...\n",
            "Validation loss decreased (0.473592 --> 0.473403).  Saving model ...\n",
            "Validation loss decreased (0.473403 --> 0.473216).  Saving model ...\n",
            "Validation loss decreased (0.473216 --> 0.473026).  Saving model ...\n",
            "Validation loss decreased (0.473026 --> 0.472839).  Saving model ...\n",
            "Validation loss decreased (0.472839 --> 0.472649).  Saving model ...\n",
            "Validation loss decreased (0.472649 --> 0.472461).  Saving model ...\n",
            "Validation loss decreased (0.472461 --> 0.472270).  Saving model ...\n",
            "Validation loss decreased (0.472270 --> 0.472081).  Saving model ...\n",
            "Validation loss decreased (0.472081 --> 0.471891).  Saving model ...\n",
            "Epoch: 2640 \tTraining Loss: 0.507186 \tValidation Loss: 0.471701\n",
            "Validation loss decreased (0.471891 --> 0.471701).  Saving model ...\n",
            "Validation loss decreased (0.471701 --> 0.471511).  Saving model ...\n",
            "Validation loss decreased (0.471511 --> 0.471322).  Saving model ...\n",
            "Validation loss decreased (0.471322 --> 0.471134).  Saving model ...\n",
            "Validation loss decreased (0.471134 --> 0.470945).  Saving model ...\n",
            "Validation loss decreased (0.470945 --> 0.470756).  Saving model ...\n",
            "Validation loss decreased (0.470756 --> 0.470568).  Saving model ...\n",
            "Validation loss decreased (0.470568 --> 0.470380).  Saving model ...\n",
            "Validation loss decreased (0.470380 --> 0.470189).  Saving model ...\n",
            "Validation loss decreased (0.470189 --> 0.470000).  Saving model ...\n",
            "Epoch: 2650 \tTraining Loss: 0.507157 \tValidation Loss: 0.469811\n",
            "Validation loss decreased (0.470000 --> 0.469811).  Saving model ...\n",
            "Validation loss decreased (0.469811 --> 0.469621).  Saving model ...\n",
            "Validation loss decreased (0.469621 --> 0.469432).  Saving model ...\n",
            "Validation loss decreased (0.469432 --> 0.469243).  Saving model ...\n",
            "Validation loss decreased (0.469243 --> 0.469054).  Saving model ...\n",
            "Validation loss decreased (0.469054 --> 0.468865).  Saving model ...\n",
            "Validation loss decreased (0.468865 --> 0.468676).  Saving model ...\n",
            "Validation loss decreased (0.468676 --> 0.468488).  Saving model ...\n",
            "Validation loss decreased (0.468488 --> 0.468299).  Saving model ...\n",
            "Validation loss decreased (0.468299 --> 0.468109).  Saving model ...\n",
            "Epoch: 2660 \tTraining Loss: 0.506482 \tValidation Loss: 0.467919\n",
            "Validation loss decreased (0.468109 --> 0.467919).  Saving model ...\n",
            "Validation loss decreased (0.467919 --> 0.467731).  Saving model ...\n",
            "Validation loss decreased (0.467731 --> 0.467541).  Saving model ...\n",
            "Validation loss decreased (0.467541 --> 0.467352).  Saving model ...\n",
            "Validation loss decreased (0.467352 --> 0.467163).  Saving model ...\n",
            "Validation loss decreased (0.467163 --> 0.466975).  Saving model ...\n",
            "Validation loss decreased (0.466975 --> 0.466785).  Saving model ...\n",
            "Validation loss decreased (0.466785 --> 0.466596).  Saving model ...\n",
            "Validation loss decreased (0.466596 --> 0.466406).  Saving model ...\n",
            "Validation loss decreased (0.466406 --> 0.466217).  Saving model ...\n",
            "Epoch: 2670 \tTraining Loss: 0.510596 \tValidation Loss: 0.466027\n",
            "Validation loss decreased (0.466217 --> 0.466027).  Saving model ...\n",
            "Validation loss decreased (0.466027 --> 0.465837).  Saving model ...\n",
            "Validation loss decreased (0.465837 --> 0.465648).  Saving model ...\n",
            "Validation loss decreased (0.465648 --> 0.465459).  Saving model ...\n",
            "Validation loss decreased (0.465459 --> 0.465270).  Saving model ...\n",
            "Validation loss decreased (0.465270 --> 0.465080).  Saving model ...\n",
            "Validation loss decreased (0.465080 --> 0.464891).  Saving model ...\n",
            "Validation loss decreased (0.464891 --> 0.464700).  Saving model ...\n",
            "Validation loss decreased (0.464700 --> 0.464511).  Saving model ...\n",
            "Validation loss decreased (0.464511 --> 0.464321).  Saving model ...\n",
            "Epoch: 2680 \tTraining Loss: 0.504134 \tValidation Loss: 0.464131\n",
            "Validation loss decreased (0.464321 --> 0.464131).  Saving model ...\n",
            "Validation loss decreased (0.464131 --> 0.463940).  Saving model ...\n",
            "Validation loss decreased (0.463940 --> 0.463749).  Saving model ...\n",
            "Validation loss decreased (0.463749 --> 0.463559).  Saving model ...\n",
            "Validation loss decreased (0.463559 --> 0.463370).  Saving model ...\n",
            "Validation loss decreased (0.463370 --> 0.463180).  Saving model ...\n",
            "Validation loss decreased (0.463180 --> 0.462989).  Saving model ...\n",
            "Validation loss decreased (0.462989 --> 0.462799).  Saving model ...\n",
            "Validation loss decreased (0.462799 --> 0.462608).  Saving model ...\n",
            "Validation loss decreased (0.462608 --> 0.462418).  Saving model ...\n",
            "Epoch: 2690 \tTraining Loss: 0.501359 \tValidation Loss: 0.462228\n",
            "Validation loss decreased (0.462418 --> 0.462228).  Saving model ...\n",
            "Validation loss decreased (0.462228 --> 0.462040).  Saving model ...\n",
            "Validation loss decreased (0.462040 --> 0.461850).  Saving model ...\n",
            "Validation loss decreased (0.461850 --> 0.461658).  Saving model ...\n",
            "Validation loss decreased (0.461658 --> 0.461466).  Saving model ...\n",
            "Validation loss decreased (0.461466 --> 0.461276).  Saving model ...\n",
            "Validation loss decreased (0.461276 --> 0.461084).  Saving model ...\n",
            "Validation loss decreased (0.461084 --> 0.460894).  Saving model ...\n",
            "Validation loss decreased (0.460894 --> 0.460702).  Saving model ...\n",
            "Validation loss decreased (0.460702 --> 0.460510).  Saving model ...\n",
            "Epoch: 2700 \tTraining Loss: 0.506335 \tValidation Loss: 0.460321\n",
            "Validation loss decreased (0.460510 --> 0.460321).  Saving model ...\n",
            "Validation loss decreased (0.460321 --> 0.460130).  Saving model ...\n",
            "Validation loss decreased (0.460130 --> 0.459940).  Saving model ...\n",
            "Validation loss decreased (0.459940 --> 0.459749).  Saving model ...\n",
            "Validation loss decreased (0.459749 --> 0.459557).  Saving model ...\n",
            "Validation loss decreased (0.459557 --> 0.459367).  Saving model ...\n",
            "Validation loss decreased (0.459367 --> 0.459175).  Saving model ...\n",
            "Validation loss decreased (0.459175 --> 0.458983).  Saving model ...\n",
            "Validation loss decreased (0.458983 --> 0.458792).  Saving model ...\n",
            "Validation loss decreased (0.458792 --> 0.458601).  Saving model ...\n",
            "Epoch: 2710 \tTraining Loss: 0.497261 \tValidation Loss: 0.458409\n",
            "Validation loss decreased (0.458601 --> 0.458409).  Saving model ...\n",
            "Validation loss decreased (0.458409 --> 0.458217).  Saving model ...\n",
            "Validation loss decreased (0.458217 --> 0.458026).  Saving model ...\n",
            "Validation loss decreased (0.458026 --> 0.457835).  Saving model ...\n",
            "Validation loss decreased (0.457835 --> 0.457643).  Saving model ...\n",
            "Validation loss decreased (0.457643 --> 0.457452).  Saving model ...\n",
            "Validation loss decreased (0.457452 --> 0.457262).  Saving model ...\n",
            "Validation loss decreased (0.457262 --> 0.457070).  Saving model ...\n",
            "Validation loss decreased (0.457070 --> 0.456879).  Saving model ...\n",
            "Validation loss decreased (0.456879 --> 0.456689).  Saving model ...\n",
            "Epoch: 2720 \tTraining Loss: 0.498319 \tValidation Loss: 0.456497\n",
            "Validation loss decreased (0.456689 --> 0.456497).  Saving model ...\n",
            "Validation loss decreased (0.456497 --> 0.456306).  Saving model ...\n",
            "Validation loss decreased (0.456306 --> 0.456115).  Saving model ...\n",
            "Validation loss decreased (0.456115 --> 0.455922).  Saving model ...\n",
            "Validation loss decreased (0.455922 --> 0.455732).  Saving model ...\n",
            "Validation loss decreased (0.455732 --> 0.455542).  Saving model ...\n",
            "Validation loss decreased (0.455542 --> 0.455352).  Saving model ...\n",
            "Validation loss decreased (0.455352 --> 0.455159).  Saving model ...\n",
            "Validation loss decreased (0.455159 --> 0.454969).  Saving model ...\n",
            "Validation loss decreased (0.454969 --> 0.454778).  Saving model ...\n",
            "Epoch: 2730 \tTraining Loss: 0.490196 \tValidation Loss: 0.454585\n",
            "Validation loss decreased (0.454778 --> 0.454585).  Saving model ...\n",
            "Validation loss decreased (0.454585 --> 0.454394).  Saving model ...\n",
            "Validation loss decreased (0.454394 --> 0.454201).  Saving model ...\n",
            "Validation loss decreased (0.454201 --> 0.454010).  Saving model ...\n",
            "Validation loss decreased (0.454010 --> 0.453819).  Saving model ...\n",
            "Validation loss decreased (0.453819 --> 0.453627).  Saving model ...\n",
            "Validation loss decreased (0.453627 --> 0.453436).  Saving model ...\n",
            "Validation loss decreased (0.453436 --> 0.453245).  Saving model ...\n",
            "Validation loss decreased (0.453245 --> 0.453052).  Saving model ...\n",
            "Validation loss decreased (0.453052 --> 0.452860).  Saving model ...\n",
            "Epoch: 2740 \tTraining Loss: 0.490607 \tValidation Loss: 0.452668\n",
            "Validation loss decreased (0.452860 --> 0.452668).  Saving model ...\n",
            "Validation loss decreased (0.452668 --> 0.452477).  Saving model ...\n",
            "Validation loss decreased (0.452477 --> 0.452286).  Saving model ...\n",
            "Validation loss decreased (0.452286 --> 0.452094).  Saving model ...\n",
            "Validation loss decreased (0.452094 --> 0.451901).  Saving model ...\n",
            "Validation loss decreased (0.451901 --> 0.451709).  Saving model ...\n",
            "Validation loss decreased (0.451709 --> 0.451518).  Saving model ...\n",
            "Validation loss decreased (0.451518 --> 0.451326).  Saving model ...\n",
            "Validation loss decreased (0.451326 --> 0.451133).  Saving model ...\n",
            "Validation loss decreased (0.451133 --> 0.450944).  Saving model ...\n",
            "Epoch: 2750 \tTraining Loss: 0.493253 \tValidation Loss: 0.450753\n",
            "Validation loss decreased (0.450944 --> 0.450753).  Saving model ...\n",
            "Validation loss decreased (0.450753 --> 0.450560).  Saving model ...\n",
            "Validation loss decreased (0.450560 --> 0.450368).  Saving model ...\n",
            "Validation loss decreased (0.450368 --> 0.450175).  Saving model ...\n",
            "Validation loss decreased (0.450175 --> 0.449983).  Saving model ...\n",
            "Validation loss decreased (0.449983 --> 0.449791).  Saving model ...\n",
            "Validation loss decreased (0.449791 --> 0.449598).  Saving model ...\n",
            "Validation loss decreased (0.449598 --> 0.449406).  Saving model ...\n",
            "Validation loss decreased (0.449406 --> 0.449215).  Saving model ...\n",
            "Validation loss decreased (0.449215 --> 0.449022).  Saving model ...\n",
            "Epoch: 2760 \tTraining Loss: 0.489519 \tValidation Loss: 0.448829\n",
            "Validation loss decreased (0.449022 --> 0.448829).  Saving model ...\n",
            "Validation loss decreased (0.448829 --> 0.448638).  Saving model ...\n",
            "Validation loss decreased (0.448638 --> 0.448446).  Saving model ...\n",
            "Validation loss decreased (0.448446 --> 0.448255).  Saving model ...\n",
            "Validation loss decreased (0.448255 --> 0.448062).  Saving model ...\n",
            "Validation loss decreased (0.448062 --> 0.447870).  Saving model ...\n",
            "Validation loss decreased (0.447870 --> 0.447677).  Saving model ...\n",
            "Validation loss decreased (0.447677 --> 0.447485).  Saving model ...\n",
            "Validation loss decreased (0.447485 --> 0.447291).  Saving model ...\n",
            "Validation loss decreased (0.447291 --> 0.447098).  Saving model ...\n",
            "Epoch: 2770 \tTraining Loss: 0.489139 \tValidation Loss: 0.446906\n",
            "Validation loss decreased (0.447098 --> 0.446906).  Saving model ...\n",
            "Validation loss decreased (0.446906 --> 0.446713).  Saving model ...\n",
            "Validation loss decreased (0.446713 --> 0.446520).  Saving model ...\n",
            "Validation loss decreased (0.446520 --> 0.446328).  Saving model ...\n",
            "Validation loss decreased (0.446328 --> 0.446135).  Saving model ...\n",
            "Validation loss decreased (0.446135 --> 0.445941).  Saving model ...\n",
            "Validation loss decreased (0.445941 --> 0.445748).  Saving model ...\n",
            "Validation loss decreased (0.445748 --> 0.445555).  Saving model ...\n",
            "Validation loss decreased (0.445555 --> 0.445361).  Saving model ...\n",
            "Validation loss decreased (0.445361 --> 0.445166).  Saving model ...\n",
            "Epoch: 2780 \tTraining Loss: 0.485863 \tValidation Loss: 0.444973\n",
            "Validation loss decreased (0.445166 --> 0.444973).  Saving model ...\n",
            "Validation loss decreased (0.444973 --> 0.444779).  Saving model ...\n",
            "Validation loss decreased (0.444779 --> 0.444586).  Saving model ...\n",
            "Validation loss decreased (0.444586 --> 0.444392).  Saving model ...\n",
            "Validation loss decreased (0.444392 --> 0.444199).  Saving model ...\n",
            "Validation loss decreased (0.444199 --> 0.444005).  Saving model ...\n",
            "Validation loss decreased (0.444005 --> 0.443811).  Saving model ...\n",
            "Validation loss decreased (0.443811 --> 0.443618).  Saving model ...\n",
            "Validation loss decreased (0.443618 --> 0.443424).  Saving model ...\n",
            "Validation loss decreased (0.443424 --> 0.443231).  Saving model ...\n",
            "Epoch: 2790 \tTraining Loss: 0.480743 \tValidation Loss: 0.443037\n",
            "Validation loss decreased (0.443231 --> 0.443037).  Saving model ...\n",
            "Validation loss decreased (0.443037 --> 0.442844).  Saving model ...\n",
            "Validation loss decreased (0.442844 --> 0.442651).  Saving model ...\n",
            "Validation loss decreased (0.442651 --> 0.442458).  Saving model ...\n",
            "Validation loss decreased (0.442458 --> 0.442265).  Saving model ...\n",
            "Validation loss decreased (0.442265 --> 0.442073).  Saving model ...\n",
            "Validation loss decreased (0.442073 --> 0.441878).  Saving model ...\n",
            "Validation loss decreased (0.441878 --> 0.441685).  Saving model ...\n",
            "Validation loss decreased (0.441685 --> 0.441491).  Saving model ...\n",
            "Validation loss decreased (0.441491 --> 0.441299).  Saving model ...\n",
            "Epoch: 2800 \tTraining Loss: 0.483979 \tValidation Loss: 0.441105\n",
            "Validation loss decreased (0.441299 --> 0.441105).  Saving model ...\n",
            "Validation loss decreased (0.441105 --> 0.440912).  Saving model ...\n",
            "Validation loss decreased (0.440912 --> 0.440719).  Saving model ...\n",
            "Validation loss decreased (0.440719 --> 0.440525).  Saving model ...\n",
            "Validation loss decreased (0.440525 --> 0.440330).  Saving model ...\n",
            "Validation loss decreased (0.440330 --> 0.440136).  Saving model ...\n",
            "Validation loss decreased (0.440136 --> 0.439941).  Saving model ...\n",
            "Validation loss decreased (0.439941 --> 0.439747).  Saving model ...\n",
            "Validation loss decreased (0.439747 --> 0.439554).  Saving model ...\n",
            "Validation loss decreased (0.439554 --> 0.439361).  Saving model ...\n",
            "Epoch: 2810 \tTraining Loss: 0.479092 \tValidation Loss: 0.439167\n",
            "Validation loss decreased (0.439361 --> 0.439167).  Saving model ...\n",
            "Validation loss decreased (0.439167 --> 0.438973).  Saving model ...\n",
            "Validation loss decreased (0.438973 --> 0.438780).  Saving model ...\n",
            "Validation loss decreased (0.438780 --> 0.438586).  Saving model ...\n",
            "Validation loss decreased (0.438586 --> 0.438391).  Saving model ...\n",
            "Validation loss decreased (0.438391 --> 0.438196).  Saving model ...\n",
            "Validation loss decreased (0.438196 --> 0.438002).  Saving model ...\n",
            "Validation loss decreased (0.438002 --> 0.437808).  Saving model ...\n",
            "Validation loss decreased (0.437808 --> 0.437615).  Saving model ...\n",
            "Validation loss decreased (0.437615 --> 0.437421).  Saving model ...\n",
            "Epoch: 2820 \tTraining Loss: 0.475627 \tValidation Loss: 0.437226\n",
            "Validation loss decreased (0.437421 --> 0.437226).  Saving model ...\n",
            "Validation loss decreased (0.437226 --> 0.437031).  Saving model ...\n",
            "Validation loss decreased (0.437031 --> 0.436839).  Saving model ...\n",
            "Validation loss decreased (0.436839 --> 0.436645).  Saving model ...\n",
            "Validation loss decreased (0.436645 --> 0.436451).  Saving model ...\n",
            "Validation loss decreased (0.436451 --> 0.436255).  Saving model ...\n",
            "Validation loss decreased (0.436255 --> 0.436059).  Saving model ...\n",
            "Validation loss decreased (0.436059 --> 0.435864).  Saving model ...\n",
            "Validation loss decreased (0.435864 --> 0.435669).  Saving model ...\n",
            "Validation loss decreased (0.435669 --> 0.435473).  Saving model ...\n",
            "Epoch: 2830 \tTraining Loss: 0.478952 \tValidation Loss: 0.435278\n",
            "Validation loss decreased (0.435473 --> 0.435278).  Saving model ...\n",
            "Validation loss decreased (0.435278 --> 0.435084).  Saving model ...\n",
            "Validation loss decreased (0.435084 --> 0.434890).  Saving model ...\n",
            "Validation loss decreased (0.434890 --> 0.434696).  Saving model ...\n",
            "Validation loss decreased (0.434696 --> 0.434500).  Saving model ...\n",
            "Validation loss decreased (0.434500 --> 0.434304).  Saving model ...\n",
            "Validation loss decreased (0.434304 --> 0.434109).  Saving model ...\n",
            "Validation loss decreased (0.434109 --> 0.433914).  Saving model ...\n",
            "Validation loss decreased (0.433914 --> 0.433719).  Saving model ...\n",
            "Validation loss decreased (0.433719 --> 0.433522).  Saving model ...\n",
            "Epoch: 2840 \tTraining Loss: 0.474968 \tValidation Loss: 0.433326\n",
            "Validation loss decreased (0.433522 --> 0.433326).  Saving model ...\n",
            "Validation loss decreased (0.433326 --> 0.433131).  Saving model ...\n",
            "Validation loss decreased (0.433131 --> 0.432935).  Saving model ...\n",
            "Validation loss decreased (0.432935 --> 0.432740).  Saving model ...\n",
            "Validation loss decreased (0.432740 --> 0.432545).  Saving model ...\n",
            "Validation loss decreased (0.432545 --> 0.432349).  Saving model ...\n",
            "Validation loss decreased (0.432349 --> 0.432154).  Saving model ...\n",
            "Validation loss decreased (0.432154 --> 0.431959).  Saving model ...\n",
            "Validation loss decreased (0.431959 --> 0.431765).  Saving model ...\n",
            "Validation loss decreased (0.431765 --> 0.431568).  Saving model ...\n",
            "Epoch: 2850 \tTraining Loss: 0.475751 \tValidation Loss: 0.431373\n",
            "Validation loss decreased (0.431568 --> 0.431373).  Saving model ...\n",
            "Validation loss decreased (0.431373 --> 0.431177).  Saving model ...\n",
            "Validation loss decreased (0.431177 --> 0.430981).  Saving model ...\n",
            "Validation loss decreased (0.430981 --> 0.430786).  Saving model ...\n",
            "Validation loss decreased (0.430786 --> 0.430590).  Saving model ...\n",
            "Validation loss decreased (0.430590 --> 0.430396).  Saving model ...\n",
            "Validation loss decreased (0.430396 --> 0.430201).  Saving model ...\n",
            "Validation loss decreased (0.430201 --> 0.430005).  Saving model ...\n",
            "Validation loss decreased (0.430005 --> 0.429809).  Saving model ...\n",
            "Validation loss decreased (0.429809 --> 0.429612).  Saving model ...\n",
            "Epoch: 2860 \tTraining Loss: 0.469584 \tValidation Loss: 0.429416\n",
            "Validation loss decreased (0.429612 --> 0.429416).  Saving model ...\n",
            "Validation loss decreased (0.429416 --> 0.429221).  Saving model ...\n",
            "Validation loss decreased (0.429221 --> 0.429026).  Saving model ...\n",
            "Validation loss decreased (0.429026 --> 0.428830).  Saving model ...\n",
            "Validation loss decreased (0.428830 --> 0.428634).  Saving model ...\n",
            "Validation loss decreased (0.428634 --> 0.428439).  Saving model ...\n",
            "Validation loss decreased (0.428439 --> 0.428245).  Saving model ...\n",
            "Validation loss decreased (0.428245 --> 0.428049).  Saving model ...\n",
            "Validation loss decreased (0.428049 --> 0.427853).  Saving model ...\n",
            "Validation loss decreased (0.427853 --> 0.427658).  Saving model ...\n",
            "Epoch: 2870 \tTraining Loss: 0.470554 \tValidation Loss: 0.427462\n",
            "Validation loss decreased (0.427658 --> 0.427462).  Saving model ...\n",
            "Validation loss decreased (0.427462 --> 0.427264).  Saving model ...\n",
            "Validation loss decreased (0.427264 --> 0.427067).  Saving model ...\n",
            "Validation loss decreased (0.427067 --> 0.426870).  Saving model ...\n",
            "Validation loss decreased (0.426870 --> 0.426675).  Saving model ...\n",
            "Validation loss decreased (0.426675 --> 0.426479).  Saving model ...\n",
            "Validation loss decreased (0.426479 --> 0.426285).  Saving model ...\n",
            "Validation loss decreased (0.426285 --> 0.426089).  Saving model ...\n",
            "Validation loss decreased (0.426089 --> 0.425893).  Saving model ...\n",
            "Validation loss decreased (0.425893 --> 0.425696).  Saving model ...\n",
            "Epoch: 2880 \tTraining Loss: 0.465474 \tValidation Loss: 0.425499\n",
            "Validation loss decreased (0.425696 --> 0.425499).  Saving model ...\n",
            "Validation loss decreased (0.425499 --> 0.425303).  Saving model ...\n",
            "Validation loss decreased (0.425303 --> 0.425107).  Saving model ...\n",
            "Validation loss decreased (0.425107 --> 0.424912).  Saving model ...\n",
            "Validation loss decreased (0.424912 --> 0.424716).  Saving model ...\n",
            "Validation loss decreased (0.424716 --> 0.424518).  Saving model ...\n",
            "Validation loss decreased (0.424518 --> 0.424322).  Saving model ...\n",
            "Validation loss decreased (0.424322 --> 0.424125).  Saving model ...\n",
            "Validation loss decreased (0.424125 --> 0.423930).  Saving model ...\n",
            "Validation loss decreased (0.423930 --> 0.423734).  Saving model ...\n",
            "Epoch: 2890 \tTraining Loss: 0.460590 \tValidation Loss: 0.423537\n",
            "Validation loss decreased (0.423734 --> 0.423537).  Saving model ...\n",
            "Validation loss decreased (0.423537 --> 0.423342).  Saving model ...\n",
            "Validation loss decreased (0.423342 --> 0.423145).  Saving model ...\n",
            "Validation loss decreased (0.423145 --> 0.422949).  Saving model ...\n",
            "Validation loss decreased (0.422949 --> 0.422752).  Saving model ...\n",
            "Validation loss decreased (0.422752 --> 0.422556).  Saving model ...\n",
            "Validation loss decreased (0.422556 --> 0.422359).  Saving model ...\n",
            "Validation loss decreased (0.422359 --> 0.422163).  Saving model ...\n",
            "Validation loss decreased (0.422163 --> 0.421966).  Saving model ...\n",
            "Validation loss decreased (0.421966 --> 0.421769).  Saving model ...\n",
            "Epoch: 2900 \tTraining Loss: 0.462590 \tValidation Loss: 0.421571\n",
            "Validation loss decreased (0.421769 --> 0.421571).  Saving model ...\n",
            "Validation loss decreased (0.421571 --> 0.421375).  Saving model ...\n",
            "Validation loss decreased (0.421375 --> 0.421179).  Saving model ...\n",
            "Validation loss decreased (0.421179 --> 0.420981).  Saving model ...\n",
            "Validation loss decreased (0.420981 --> 0.420782).  Saving model ...\n",
            "Validation loss decreased (0.420782 --> 0.420583).  Saving model ...\n",
            "Validation loss decreased (0.420583 --> 0.420386).  Saving model ...\n",
            "Validation loss decreased (0.420386 --> 0.420190).  Saving model ...\n",
            "Validation loss decreased (0.420190 --> 0.419993).  Saving model ...\n",
            "Validation loss decreased (0.419993 --> 0.419796).  Saving model ...\n",
            "Epoch: 2910 \tTraining Loss: 0.463385 \tValidation Loss: 0.419599\n",
            "Validation loss decreased (0.419796 --> 0.419599).  Saving model ...\n",
            "Validation loss decreased (0.419599 --> 0.419402).  Saving model ...\n",
            "Validation loss decreased (0.419402 --> 0.419205).  Saving model ...\n",
            "Validation loss decreased (0.419205 --> 0.419008).  Saving model ...\n",
            "Validation loss decreased (0.419008 --> 0.418811).  Saving model ...\n",
            "Validation loss decreased (0.418811 --> 0.418614).  Saving model ...\n",
            "Validation loss decreased (0.418614 --> 0.418415).  Saving model ...\n",
            "Validation loss decreased (0.418415 --> 0.418217).  Saving model ...\n",
            "Validation loss decreased (0.418217 --> 0.418020).  Saving model ...\n",
            "Validation loss decreased (0.418020 --> 0.417821).  Saving model ...\n",
            "Epoch: 2920 \tTraining Loss: 0.462053 \tValidation Loss: 0.417625\n",
            "Validation loss decreased (0.417821 --> 0.417625).  Saving model ...\n",
            "Validation loss decreased (0.417625 --> 0.417427).  Saving model ...\n",
            "Validation loss decreased (0.417427 --> 0.417229).  Saving model ...\n",
            "Validation loss decreased (0.417229 --> 0.417034).  Saving model ...\n",
            "Validation loss decreased (0.417034 --> 0.416836).  Saving model ...\n",
            "Validation loss decreased (0.416836 --> 0.416641).  Saving model ...\n",
            "Validation loss decreased (0.416641 --> 0.416444).  Saving model ...\n",
            "Validation loss decreased (0.416444 --> 0.416246).  Saving model ...\n",
            "Validation loss decreased (0.416246 --> 0.416047).  Saving model ...\n",
            "Validation loss decreased (0.416047 --> 0.415850).  Saving model ...\n",
            "Epoch: 2930 \tTraining Loss: 0.465381 \tValidation Loss: 0.415653\n",
            "Validation loss decreased (0.415850 --> 0.415653).  Saving model ...\n",
            "Validation loss decreased (0.415653 --> 0.415455).  Saving model ...\n",
            "Validation loss decreased (0.415455 --> 0.415257).  Saving model ...\n",
            "Validation loss decreased (0.415257 --> 0.415059).  Saving model ...\n",
            "Validation loss decreased (0.415059 --> 0.414861).  Saving model ...\n",
            "Validation loss decreased (0.414861 --> 0.414662).  Saving model ...\n",
            "Validation loss decreased (0.414662 --> 0.414463).  Saving model ...\n",
            "Validation loss decreased (0.414463 --> 0.414265).  Saving model ...\n",
            "Validation loss decreased (0.414265 --> 0.414066).  Saving model ...\n",
            "Validation loss decreased (0.414066 --> 0.413867).  Saving model ...\n",
            "Epoch: 2940 \tTraining Loss: 0.460146 \tValidation Loss: 0.413667\n",
            "Validation loss decreased (0.413867 --> 0.413667).  Saving model ...\n",
            "Validation loss decreased (0.413667 --> 0.413471).  Saving model ...\n",
            "Validation loss decreased (0.413471 --> 0.413273).  Saving model ...\n",
            "Validation loss decreased (0.413273 --> 0.413073).  Saving model ...\n",
            "Validation loss decreased (0.413073 --> 0.412875).  Saving model ...\n",
            "Validation loss decreased (0.412875 --> 0.412676).  Saving model ...\n",
            "Validation loss decreased (0.412676 --> 0.412478).  Saving model ...\n",
            "Validation loss decreased (0.412478 --> 0.412280).  Saving model ...\n",
            "Validation loss decreased (0.412280 --> 0.412082).  Saving model ...\n",
            "Validation loss decreased (0.412082 --> 0.411886).  Saving model ...\n",
            "Epoch: 2950 \tTraining Loss: 0.459697 \tValidation Loss: 0.411689\n",
            "Validation loss decreased (0.411886 --> 0.411689).  Saving model ...\n",
            "Validation loss decreased (0.411689 --> 0.411490).  Saving model ...\n",
            "Validation loss decreased (0.411490 --> 0.411290).  Saving model ...\n",
            "Validation loss decreased (0.411290 --> 0.411093).  Saving model ...\n",
            "Validation loss decreased (0.411093 --> 0.410895).  Saving model ...\n",
            "Validation loss decreased (0.410895 --> 0.410698).  Saving model ...\n",
            "Validation loss decreased (0.410698 --> 0.410497).  Saving model ...\n",
            "Validation loss decreased (0.410497 --> 0.410297).  Saving model ...\n",
            "Validation loss decreased (0.410297 --> 0.410100).  Saving model ...\n",
            "Validation loss decreased (0.410100 --> 0.409902).  Saving model ...\n",
            "Epoch: 2960 \tTraining Loss: 0.455283 \tValidation Loss: 0.409705\n",
            "Validation loss decreased (0.409902 --> 0.409705).  Saving model ...\n",
            "Validation loss decreased (0.409705 --> 0.409507).  Saving model ...\n",
            "Validation loss decreased (0.409507 --> 0.409308).  Saving model ...\n",
            "Validation loss decreased (0.409308 --> 0.409111).  Saving model ...\n",
            "Validation loss decreased (0.409111 --> 0.408911).  Saving model ...\n",
            "Validation loss decreased (0.408911 --> 0.408714).  Saving model ...\n",
            "Validation loss decreased (0.408714 --> 0.408516).  Saving model ...\n",
            "Validation loss decreased (0.408516 --> 0.408316).  Saving model ...\n",
            "Validation loss decreased (0.408316 --> 0.408116).  Saving model ...\n",
            "Validation loss decreased (0.408116 --> 0.407916).  Saving model ...\n",
            "Epoch: 2970 \tTraining Loss: 0.443371 \tValidation Loss: 0.407717\n",
            "Validation loss decreased (0.407916 --> 0.407717).  Saving model ...\n",
            "Validation loss decreased (0.407717 --> 0.407517).  Saving model ...\n",
            "Validation loss decreased (0.407517 --> 0.407317).  Saving model ...\n",
            "Validation loss decreased (0.407317 --> 0.407117).  Saving model ...\n",
            "Validation loss decreased (0.407117 --> 0.406917).  Saving model ...\n",
            "Validation loss decreased (0.406917 --> 0.406719).  Saving model ...\n",
            "Validation loss decreased (0.406719 --> 0.406520).  Saving model ...\n",
            "Validation loss decreased (0.406520 --> 0.406321).  Saving model ...\n",
            "Validation loss decreased (0.406321 --> 0.406122).  Saving model ...\n",
            "Validation loss decreased (0.406122 --> 0.405922).  Saving model ...\n",
            "Epoch: 2980 \tTraining Loss: 0.454963 \tValidation Loss: 0.405723\n",
            "Validation loss decreased (0.405922 --> 0.405723).  Saving model ...\n",
            "Validation loss decreased (0.405723 --> 0.405523).  Saving model ...\n",
            "Validation loss decreased (0.405523 --> 0.405323).  Saving model ...\n",
            "Validation loss decreased (0.405323 --> 0.405122).  Saving model ...\n",
            "Validation loss decreased (0.405122 --> 0.404923).  Saving model ...\n",
            "Validation loss decreased (0.404923 --> 0.404722).  Saving model ...\n",
            "Validation loss decreased (0.404722 --> 0.404523).  Saving model ...\n",
            "Validation loss decreased (0.404523 --> 0.404323).  Saving model ...\n",
            "Validation loss decreased (0.404323 --> 0.404124).  Saving model ...\n",
            "Validation loss decreased (0.404124 --> 0.403924).  Saving model ...\n",
            "Epoch: 2990 \tTraining Loss: 0.453565 \tValidation Loss: 0.403727\n",
            "Validation loss decreased (0.403924 --> 0.403727).  Saving model ...\n",
            "Validation loss decreased (0.403727 --> 0.403530).  Saving model ...\n",
            "Validation loss decreased (0.403530 --> 0.403334).  Saving model ...\n",
            "Validation loss decreased (0.403334 --> 0.403138).  Saving model ...\n",
            "Validation loss decreased (0.403138 --> 0.402944).  Saving model ...\n",
            "Validation loss decreased (0.402944 --> 0.402749).  Saving model ...\n",
            "Validation loss decreased (0.402749 --> 0.402555).  Saving model ...\n",
            "Validation loss decreased (0.402555 --> 0.402360).  Saving model ...\n",
            "Validation loss decreased (0.402360 --> 0.402164).  Saving model ...\n",
            "Validation loss decreased (0.402164 --> 0.401969).  Saving model ...\n",
            "Epoch: 3000 \tTraining Loss: 0.446469 \tValidation Loss: 0.401774\n",
            "Validation loss decreased (0.401969 --> 0.401774).  Saving model ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5hGTxGr1lLG"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wws7QPHiTzXK",
        "outputId": "b4d4d007-71a6-458f-a2c2-c393c7cc547e"
      },
      "source": [
        "model_A1.load_state_dict(torch.load('sinwave_DeepAnT_2.h5'))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmJwY9wZTzZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d469731d-0e56-4bb2-c43c-fe805fe863bc"
      },
      "source": [
        "# make predictions\r\n",
        "\r\n",
        "test_tensor =  torch.tensor(testX).type('torch.FloatTensor')\r\n",
        "model_A1.eval()\r\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv1d(1, 32, kernel_size=(2,), stride=(1,))\n",
              "  (conv2): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
              "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (lin1): Linear(in_features=256, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7vuBNNsBwNu"
      },
      "source": [
        "out = model_A1(test_tensor)\r\n",
        "out = out.detach().numpy()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7F84W9lk7kK"
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8eaWUF2jhtN"
      },
      "source": [
        "\r\n",
        "df_out = pd.DataFrame()\r\n",
        "df_out['pred'] = out[:,0]\r\n",
        "df_out['actual'] = testY[:,0]\r\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROIrPMzsjkxV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "911f5bda-dc39-4f6d-ef7c-0c6766a28cd8"
      },
      "source": [
        "# compute error (actual - pred)\r\n",
        "\r\n",
        "df_out['error'] = np.abs(df_out['pred'] - df_out['actual'])\r\n",
        "df_out['error_n'] = (df_out['error'] - df_out['error'].mean())/df_out['error'].std()\r\n",
        "df_out.index = data.index[train_percent + valid_percent +w+pred_window-1:-1]\r\n",
        "df_out.head()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pred</th>\n",
              "      <th>actual</th>\n",
              "      <th>error</th>\n",
              "      <th>error_n</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>620</th>\n",
              "      <td>0.493876</td>\n",
              "      <td>1.160000</td>\n",
              "      <td>0.666124</td>\n",
              "      <td>0.422680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>621</th>\n",
              "      <td>0.497719</td>\n",
              "      <td>1.178333</td>\n",
              "      <td>0.680614</td>\n",
              "      <td>0.454401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>622</th>\n",
              "      <td>0.501793</td>\n",
              "      <td>1.148889</td>\n",
              "      <td>0.647096</td>\n",
              "      <td>0.381025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>623</th>\n",
              "      <td>0.510865</td>\n",
              "      <td>1.095556</td>\n",
              "      <td>0.584690</td>\n",
              "      <td>0.244408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>624</th>\n",
              "      <td>0.523164</td>\n",
              "      <td>1.005833</td>\n",
              "      <td>0.482669</td>\n",
              "      <td>0.021068</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               pred    actual     error   error_n\n",
              "timestamp                                        \n",
              "620        0.493876  1.160000  0.666124  0.422680\n",
              "621        0.497719  1.178333  0.680614  0.454401\n",
              "622        0.501793  1.148889  0.647096  0.381025\n",
              "623        0.510865  1.095556  0.584690  0.244408\n",
              "624        0.523164  1.005833  0.482669  0.021068"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvfJzByumGdj"
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "EAoIm7jGlFKb",
        "outputId": "1b5eb5c8-f885-4f43-9b04-3284e171671e"
      },
      "source": [
        "# check whether error is more than the threshold\r\n",
        "\r\n",
        "thresh = df_out.loc[df_out['error_n'].abs() > 3]\r\n",
        "thresh"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pred</th>\n",
              "      <th>actual</th>\n",
              "      <th>error</th>\n",
              "      <th>error_n</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>844</th>\n",
              "      <td>0.533232</td>\n",
              "      <td>3.982222</td>\n",
              "      <td>3.448990</td>\n",
              "      <td>6.514815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1208</th>\n",
              "      <td>0.627807</td>\n",
              "      <td>5.646111</td>\n",
              "      <td>5.018304</td>\n",
              "      <td>9.950292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1209</th>\n",
              "      <td>0.782389</td>\n",
              "      <td>10.452778</td>\n",
              "      <td>9.670389</td>\n",
              "      <td>20.134444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1210</th>\n",
              "      <td>0.911311</td>\n",
              "      <td>4.237778</td>\n",
              "      <td>3.326467</td>\n",
              "      <td>6.246593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1211</th>\n",
              "      <td>1.064602</td>\n",
              "      <td>3.192222</td>\n",
              "      <td>2.127621</td>\n",
              "      <td>3.622128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>0.591537</td>\n",
              "      <td>2.680556</td>\n",
              "      <td>2.089019</td>\n",
              "      <td>3.537622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>0.611888</td>\n",
              "      <td>3.063889</td>\n",
              "      <td>2.452001</td>\n",
              "      <td>4.332249</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               pred     actual     error    error_n\n",
              "timestamp                                          \n",
              "844        0.533232   3.982222  3.448990   6.514815\n",
              "1208       0.627807   5.646111  5.018304   9.950292\n",
              "1209       0.782389  10.452778  9.670389  20.134444\n",
              "1210       0.911311   4.237778  3.326467   6.246593\n",
              "1211       1.064602   3.192222  2.127621   3.622128\n",
              "1457       0.591537   2.680556  2.089019   3.537622\n",
              "1458       0.611888   3.063889  2.452001   4.332249"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghg3UwcKjo_V"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wOABusJjuYl"
      },
      "source": [
        "# calc TP, FN, FP, TN\r\n",
        "\r\n",
        "positives = data.loc[df_out.index].loc[data.is_anomaly == 1].index\r\n",
        "negatives = data.loc[df_out.index].loc[data.is_anomaly == 0].index\r\n",
        "tp = []\r\n",
        "fn = []\r\n",
        "fp = []\r\n",
        "tn = []\r\n",
        "for p in positives:\r\n",
        "    if p in thresh.index:\r\n",
        "        tp.append(p)\r\n",
        "    else:\r\n",
        "        fn.append(p)\r\n",
        "\r\n",
        "for n in negatives:\r\n",
        "    if n in thresh.index:\r\n",
        "        fp.append(n)\r\n",
        "    else:\r\n",
        "        tn.append(n)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD2ErDoWJ_hC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7c293e-4dce-4d1b-fa53-cdf57a188459"
      },
      "source": [
        "# calc F-score\r\n",
        "\r\n",
        "recall = len(tp)/(len(tp)+len(fn))\r\n",
        "precision = len(tp)/(len(tp)+len(fp))\r\n",
        "F_score = 2* recall*precision/(recall + precision)\r\n",
        "F_score"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7777777777777778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAxFChwXkoni"
      },
      "source": [
        "df_out['error'] = np.abs(df_out['pred'] - df_out['actual'])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJZ_qgFzl_RT"
      },
      "source": [
        "df_out.loc[df_out['error'].abs() <0.5, 'anomaly'] = 0 \r\n",
        "df_out.loc[df_out['error'].abs() >0.5, 'anomaly'] = 1 \r\n",
        "\r\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "qfD9iNxumvmI",
        "outputId": "8f9f10fe-fd9d-4ed3-a7f7-a38a989a03d5"
      },
      "source": [
        "df_out.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pred</th>\n",
              "      <th>actual</th>\n",
              "      <th>error</th>\n",
              "      <th>error_n</th>\n",
              "      <th>anomaly</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>620</th>\n",
              "      <td>0.493876</td>\n",
              "      <td>1.160000</td>\n",
              "      <td>0.666124</td>\n",
              "      <td>0.422680</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>621</th>\n",
              "      <td>0.497719</td>\n",
              "      <td>1.178333</td>\n",
              "      <td>0.680614</td>\n",
              "      <td>0.454401</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>622</th>\n",
              "      <td>0.501793</td>\n",
              "      <td>1.148889</td>\n",
              "      <td>0.647096</td>\n",
              "      <td>0.381025</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>623</th>\n",
              "      <td>0.510865</td>\n",
              "      <td>1.095556</td>\n",
              "      <td>0.584690</td>\n",
              "      <td>0.244408</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>624</th>\n",
              "      <td>0.523164</td>\n",
              "      <td>1.005833</td>\n",
              "      <td>0.482669</td>\n",
              "      <td>0.021068</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               pred    actual     error   error_n  anomaly\n",
              "timestamp                                                 \n",
              "620        0.493876  1.160000  0.666124  0.422680      1.0\n",
              "621        0.497719  1.178333  0.680614  0.454401      1.0\n",
              "622        0.501793  1.148889  0.647096  0.381025      1.0\n",
              "623        0.510865  1.095556  0.584690  0.244408      1.0\n",
              "624        0.523164  1.005833  0.482669  0.021068      0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "1pwgkbESunvp",
        "outputId": "af2faf0c-bf09-4e7b-9b62-c27fa32e0f40"
      },
      "source": [
        "df_out[['pred','actual','error','anomaly']].groupby('anomaly').count()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pred</th>\n",
              "      <th>actual</th>\n",
              "      <th>error</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anomaly</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>573</td>\n",
              "      <td>573</td>\n",
              "      <td>573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>268</td>\n",
              "      <td>268</td>\n",
              "      <td>268</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         pred  actual  error\n",
              "anomaly                     \n",
              "0.0       573     573    573\n",
              "1.0       268     268    268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "Z0wQWHWiowvR",
        "outputId": "7c4fa545-7f94-44ef-d286-b1aad7e9eb28"
      },
      "source": [
        "\r\n",
        "data[['is_anomaly','value']].groupby('is_anomaly').count()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is_anomaly</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            value\n",
              "is_anomaly       \n",
              "0            1445\n",
              "1              16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "JwXGS_S6per5",
        "outputId": "25988daf-9d7b-49cc-8cab-87fc56fcf5f2"
      },
      "source": [
        "ax=df_out['actual'].plot()\r\n",
        "df_out['pred'].plot(ax=ax)\r\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEGCAYAAAB8Ys7jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2dd3hcxdm375FWXVaxLMsVyx3bgAvCYEzHGGOKgQCBBEIJ8NI+CAlJCIRekgChvSQvvQVCcwiYYnDB2NgY23Lv3ZYly+pdWmmlne+POduklYuklX20z31durR7zuw5c+bM/OaZZ5rSWiMIgiDYj4jDHQFBEAShbYiAC4Ig2BQRcEEQBJsiAi4IgmBTRMAFQRBsiqMzb9ajRw+dmZnZmbcUBEGwPcuXLy/WWqc3P96pAp6ZmUl2dnZn3lIQBMH2KKV2BzsuLhRBEASbIgIuCIJgU0TABUEQbIoIuCAIgk0RARcEQbApIuCCIAg2RQRcEATBpoiAC0IY0tDo5uPsPbjdspy0nenUiTyCIBwZ/N/323luzhZiHBFMG9P3cEdHaCNigQtCGFJcXQ9ARZ3rMMdEaA8i4IIgCDZFBFwQwhCN+L67AiLgghDGqMMdAaFdiIALgiDYlAMKuFLqTaVUoVJqnd+x7kqp2Uqprdb/1NBGUxAEQWjOwVjgbwNTmh27F5irtR4KzLW+C4JgE7S4wLsEBxRwrfUCoLTZ4WnAO9bnd4CLOzhegiB0Bkq84HamrT7wDK11vvV5H5DRWkCl1M1KqWylVHZRUVEbbycIgiA0p92dmFprDa2PSdJav6q1ztJaZ6Wnt9jSTRAEQWgjbRXwAqVUbwDrf2HHRUkQhFAjLvCuQVsFfAZwrfX5WuDzjomOIAidiXjA7c3BDCP8AFgMDFdK5Sqlfg38FThHKbUVmGR9FwRBEDqRA65GqLW+qpVTZ3dwXARBEIRDQGZiCkIYIuPAuwYi4IIQxsgwcHsjAi4IgmBTRMAFQRBsigi4IIQl4gTvCoiAC0IYo2QkuK0RARcEQbApIuCCIAg2RQRcEATBpoiAC0IYIhN5ugYi4IIQxshEHnsjAi4IgmBTRMAFQRBsigi4IIQh4gPvGoiAC0IYIy5weyMCLgiCYFNEwAVBEGyKCLgghCFaFrPqEoiAC0IYI+PA7Y0IuCAIgk0RARcEQbApIuCCEIbIOPCugQi4IIQxsqGDvREBFwRBsCki4IIgCDZFBFwQBMGmiIALgiDYFBFwQRAEmyICLgiCYFNEwAUhDJFh4F2Ddgm4UupupdR6pdQ6pdQHSqnYjoqYIAidgAwDtzVtFnClVF/gTiBLa30MEAlc2VEREwRBEPZPe10oDiBOKeUA4oG97Y+SIAiCcDC0WcC11nnAM0AOkA9UaK1ndVTEBEEIHbIWStegPS6UVGAaMBDoAyQopa4OEu5mpVS2Uiq7qKio7TEVBEEQAmiPC2USsFNrXaS1dgGfAic3D6S1flVrnaW1zkpPT2/H7QRB6GikD9PetEfAc4CTlFLxSikFnA1s7JhoCYIgCAeiPT7wJcB0YAWw1rrWqx0UL0EQBOEAONrzY631Q8BDHRQXQRA6CdnUuGsgMzEFIYxRsquxrREBF4QwRst4QlsjAi4IYYzIt70RAReEcESUu0sgAi4I4YwIua0RAReEMEZGo9gbEXBBCGOkD9PeiIALQhgiut01EAEXhDBGhNzeiIALQhgjLhR7IwIuCGGMdGLaGxFwQQhDZAZm10AEXBDCGNFxeyMCLghhjOi3vREBF4RwRkxwWyMCLghhiMh210AEXBDCGBFyeyMCLghhjHhQ7I0IuCCEMTKc0N6IgAtCGCK63TUQAReEMEZ03N6IgAtCGCOWuL0RAReEMEb0296IgAtCGCOdmPZGBFwQwhCR7a6BCLggCIJNEQEXhDBGPCj2RgRcEMIY2dDB3oiAC0IYIp2XXQMRcEEIY0TH7Y0IuCCEMaLf9qZdAq6USlFKTVdKbVJKbVRKTeioiAmCEHrEArc3jnb+/gXgG631ZUqpaCC+A+IkCEKIEd3uGrRZwJVSycBpwHUAWusGoKFjoiUIQmcgo1DsTXtcKAOBIuAtpdRKpdTrSqmE5oGUUjcrpbKVUtlFRUXtuJ0gCB2NuFDsTXsE3AGMA/5Paz0WqAHubR5Ia/2q1jpLa52Vnp7ejtsJgiAI/rRHwHOBXK31Euv7dIygC4JwpCOWd5egzQKutd4H7FFKDbcOnQ1s6JBYCYLQKciEHnvT3lEo/w943xqBsgO4vv1REgShsxD9tjftEnCt9Sogq4PiIghCJyP6bW9kJqYgCIJNEQEXhDBGXCj2RgRcEMIYmchjb0TABSGMEQvc3oiAC0IYIpZ310AEXBDCEI/lLTJub0TABSEM8bpOxIdia0TABSEM8bhQRL7tjQi4IIQhYnh3DUTABSEMEQ9K10AEXBDCEF8npii4nREBF4SwxPKBi37bGhFwQQhDZBhh10AEXBDCEBHuroEIuCCEIZ6NHMSFYm9EwAUhDPGOQhFb3NaIgAtCGOKbiXlYoyG0ExFwQQhDRLe7BiLgghCGeH3ghzkeQvsQAReEMEZ2pbc3IuCCEIZ4x4GLftsaEXBBCENk9EnXQARcEMIQmYnZNRABF4QwRFwoXQMRcEEIQ3wbOoiC2xkRcEEIQ8Ty7hqIgAtCGCIbOnQNRMAFIRwR4e4SiIALQhji9YGLCW5rRMAFIQzx6HajWwTczrRbwJVSkUqplUqpLzsiQoIghB6PbDc2iYDbmY6wwO8CNnbAdQRB6CQ8rhOX232YYyK0h3YJuFKqH3A+8HrHREcQhM5ALPCuQXst8OeBPwCtVuNKqZuVUtlKqeyioqJ23k5oK//6aTdTnl9wuKMhHCF4fOCuJrHA7UybBVwpdQFQqLVevr9wWutXtdZZWuus9PT0tt5OaCcPfLaOTfuqDnc0hCMEj93tEgvc1rTHAp8IXKSU2gV8CJyllHqvQ2IlCEJosUzwRvGB25o2C7jW+k9a635a60zgSuA7rfXVHRYzQRBChvjAuwYyDlwQwhDxgXcNHB1xEa3198D3HXEtQRBCj2cmpgi4vRELXBDCEJmJ2TUQAReEMMTnQhEBtzMi4GGGLF4kgH8nprhQ7IwIeJghLWYB/KbSi4DbGhHwMEMscMEfcaHYGxHwMEMscAH8OzHFArczIuBhhmxiK4AvH8hEHnsjAh5miAdFAF8+aBAfuK0RAQ8zRMAFkKn0XQUR8DDDLQou4OvMFh+4vREBDzNEwAUIXE5WRibZFxHwMEOKqgAEZASZTm9fRMDDDC0tZoHAilz84KGjscnN7f9ewdrcipBcXwQ8zJBhhAIETuiSjY1Dx66SWr5ak89dH64MyfVFwMMMaS0LIBZ4Z1Hf2ARAtCM0UisCHmZIJ6bQHFnQKnTUNRgBj4mKDMn1RcDDDNFvAQLzgUzmCR1VzkYAYsQCFzoCGTImQGBfiLhQQkel0wWIgAsdhBRVAYwFHh1pir9M5gkdlZYF7knrjkYEPMwQH7gARsCjIhUgS8qGkiqPBR4lAi50ADIKRfAQZTXrZVOH0OHxgStUSK4vAh5miA9cAJMPPM16scBDR2WdscDrG0NTSYqAhxmi3wKYvpAojw9cLPCQ4bHAPePBOxoR8DBDBFwAqxPT4enElEwRKjyjUMQCFzoE6cQUwAwj9HRiyjjw0OGzwEXAhQ5ABLxrc7B9HFpDjMPMDmwIkbgIPh+4SwRc6AhEvrs2Jzwxl9vfX3HAcBqIizYC7nSFxj8r+CzwUI21FwEPM2QUStelsNJJcXU9X63NP2BYrSHBEnDPeh1Cx+PxgYdqpI8IeJgh+t01yS2rZfyTcw/hF5r4aAcAdWKBh4TGJje1VuUYqrH2IuBhhgw46Jp8u77gkMJr7XOh1IoFHhI87hMQARc6COnE7Jr0SIz2fj6Ytac1EBsVgVLiAw8VHgGPi4o88lwoSqn+Sql5SqkNSqn1Sqm7OjJiQmgQ/e76xB3E2tNaaxSKuKhI8YGHCI//u3tC9BFpgTcCv9NajwROAm5XSo3smGgJoUIs8MPD3vI6Mu/9inmbCkNyfX8RPigBB5SC+OhIasUCDwkeAU9LPAIFXGudr7VeYX2uAjYCfTsqYoIQan7cVsys9fs65V6r95QD8OGynJBc378jsukgKmmtQQGxUZE4xQIPCZV1xoXSPSE6ZGuud4gPXCmVCYwFlgQ5d7NSKlsplV1UVNQRtxPagVjgPn7x+hJu/tfyTrmXCs1idF6cLmPhXXlC/4OamKO1RilFYoyDqvrGA4YXDp0qPxdKo1uHZAhvuwVcKZUI/Af4jda6svl5rfWrWussrXVWenp6e28ntBPR7wNT19DEvf9ZQ2GV85B+V+l0eQtta4Qq/T0WeLdYx0EtnOSJRnJcFOW1DaGJVJjj2cwhLcF0MIeiI7NdAq6UisKI9/ta6087JkpCKBEL/MB8uCyHD5ft4bUFOw7pd8c9PItxj81u5awxwUOV+k5XE7FREcRGRdLQ6D6wtadNqyA1Ppry2v1XOkLb8FTmqV4B73g/eHtGoSjgDWCj1vrZjouSEEpkHPiB2V5UDUBGUuwh/7Y1K8vjQgmZBd7QRFxUJNGREbj1gVcY1JhNBlIToikTAQ8JBZX1dE+IJtZac+aIEnBgInANcJZSapX1N7WD4iWEDFHwA1FWYwStW6yjw67pc4GHJv3rXEbAPVt3HcgPbnzgkBpvXCiyxELHk1deR9+UuJBuXdfmHKq1Xggh2idICBligR8EVq5usNFONU5XE7HRkd5dduob3STEtB7eWODQKzmWRremqKqenm1ocQitk1dWy9Ce3bwbZxxpFrhgQ8TQOjARlr/jUJYAPdglWUOV/k5XE7GOSGKiDm6JWG35wIekJwKwtbA6NBELU7TWxgJPjfPb+egI68QU7Ec4dmIu2lbMloKqgGP7cxl4mpWHstHBgUafhLrlU+dqIi7AAt//SBSNGUY4oEcCALtLakMbwTBCa01ZrQuny03flDgcIdw4QwQ8zAhHAf/l60uY/NyCgGP72yHF0+F4KBZ49QHGUjdZCh6q1Pd0Yh68D9xUVClxUYBv1qDd+GL1XvZVHNpwz1AyZ0MBA//0NUt3lgLQNzWOU4b04D+3TqBvSlyH308EPNwIP/0OyqwNra/e56njDsVi8q8Qgm0S7FnQP1SdhXUuN7FRgT7w/aEBrKn0jgjl3TnGTtQ3NvH/PljJz19dfLij4uX7LWaphFveMxPE+qbEkZYYw/EDuntXf+xIRMDDDOnENOwp9bkM3M0SxSO2hyLg/hZvsPW1Pf7PUCT/95sL2ZhfSVF1vdcH3lzACyud3P7+Cio8Qq3NMEKlFElxUYEWuLsJykMz5b8j8cw+PZLcPz27BXYEh8Lq9kcEPASU1jQwfXkutQ1H3hRlLSY4EOjycDXb7sojtq7Gg08rf7EPKuAh2lIL4Ks1Zgee1XvKvRZ4cxfK377ZzFdr8/nWWvvF+MDNuaRYBxV1fnl13pPw/LFQubfFvdbvrQjBE7SNg5lx2tk0X5o3JT4qpPcLOwHvjPGur8zfzj2frOb5OVtDfq9DRSxwg3+nY/PRAZ5JMA1NrQvEN+vyufzlH72+bX/BdDb4Pu8uqaGh0e29Ziiyn2emH+D1gTcXt4JK4yf2DGXz+MABkuKifJZ5Qw388Iz5XJEXcI2Za/M5/8WF3grjcFPvOvI2Y65zNdEt1sHH/zOBxy8+BhXiRXDCSsCX7Spl4J++DrkVsTrXrDz3+aq8I26CRLh1YgbzRwNU++2WcsH/LuT1H3zT5j0itz8L/Jb3VrBsVxl7y+uAQAGvdZlrF1Y5Of3p73nqm02tDiGbsXovn67IPcinCU6y1RGZFOto1QL3bPKQX26E3LOcLMCAtAQ25VeavPqW31y8qkALfEO+Wepobd6RYYUfKRtRFFQ6ufujVdQ2NOK0JlSNH9idq08aEPJ7h5WA/2PeNgDW5rY9A2qt+XBpDst3lwY973Zr1ueZjF5QWc++yuA95MXV9VR0whTmJTtKmLfZbw3qIDpSWOn0WpJtZVdxTbubtIWVTq54eXGAf7q9+Ls21lgVKwS6UHYW1/D4Vxu93z1iu3Bb8QGvv8paJtZ/koZnbe4Vu8u912kMMgrF7dbc+cFKfvvx6v3e48ftxXyzrnWr1yPWn99xCrFRwTsxPXHaW2EqHM+GDgBnZNQSVZXL9xtyceev9f0oL3B3e89Mwpfnb281v+SV1zF346Ft79ZWnEeIBf7UN5v578o8vl2/z4wGCkFnZWvYQsB3Fdfw3Owt7d45pLCyHti/G+GOf6/gvZ92t3r+k+W53PvpWh78fH3Q8zmltVTVNzJpRE8Anvx6U9BwWY/PYfSjs9olejX1jdzx7xXc/G52q2F+/upPXP/WMu/35hZ4ldPF+Cfnct+na5v/1IvT1cQTX21gRU5Z0PNNbs0Zz3zP8D9/Q3F1fdAw9Y1N/O7j1fxr8a5W7zNrQwFLd5VyyT9/bDXM+Cfm8MgXwdM+6H39Cvmv3zHp9O36fczZGLixgv+0eY+/Oq+8jj9/1nq6ALw4dys19Y3s9RvK5smnnkq+rLYh6LC+Ar/VDltrqV375lJ+8doSbnnPiGml09ViyGKdtZDVwB4JREcGn8jj6Y/xtBi8FrjW/GzBVBbF3sXbX35HBE0sHXo3Ov1onGs/Nz+u3AtLXqHQEn+AzfsCx9V7eHjGen79TjbLdgU3cPzZtK+Sy/7vxzYZMl+s3suFLy085N/5U9/YxKX/XBRo4DRDa82zs7fsNx94WgKuJu1d0qCzOOIFXGvNbe+v4IW5W3lh7tZWBW/RtmJGPPANO4pan1Hm6Wkvrwu+fOa6vAq+XJPPnz9b1+o1PDuqtNZ821Fs7j9xSA/AZLTm1or/uNUX5wb3k6/IKeP4x2Zzw9vLWr3XZS8v5ss1+a0OiQvWidpcJzwF7aPsPUx+bn4LIdFac9O72bz2w05e+m5b0PuU1vjS87b3VgRt4cxcu4//rMjlgc/XM+2lhUHf4z+tFlJxdX3QccmlNQ0UVtXz1qJdQeMBxhK+9b3ljH10Fuv3VgSIXZkVz+dmbwGgW4xPtBOiHX7X8KXBl0H8vTXWNUf2TmJrYTWjHvqWB/zyjKcTM3u3qfAKKutZurMECBRq/1UAf/Fai6X0AZi/xbeG/msLdjD20dlMeT5wTHttQ6N3h/mYVixwTzrkW3nP6wOf94Q3TGOFedY3dqayMuYEYit38tO2QnjtbJj5B+qLdxFhuV025rdYORrwCfvlLy/2+t1b4zcfriJ7dxnZQVqz93yymhf204f0zo+79nvtg2F7YQ0rcsoDDJzmPDt7Cy/O3cp7P+W0Wg7zrEqxqKqeOpfbOxKoMzjiBVwpxSvXHE9qfBQvz9/OFa/8FDTcA5+vo87VxBsLdwY973Q1eS3w1mp8/0zR2sQMz4y+vPK6FsPPAPLKzMsce1Sq91hza2WRX9P8H/O2B113+s2FOympaeC7TYVBrRm3WwcUoj8FsaBX72kppM0t8H8v2eP9vKWgmu1FNQHn91U6+WGrie93mwrZtK9lwf3bN75WxtJdpUEtox+3+555dW4F8zYFbu5RWOUMsGIf/nw925pN7/Z/3q0FVUGt1se/3MDMdfsoq3Xx7bp9XP2GTxgjLPVxupo4tm8yj0wb5T3nvxFwo9vNxCFpXHlC/6DvuLDK5KMbTx1IQpDmcp2rCaeriXV5FVwy1mxStdJytfiLQIXf2OvFO0q8z+N0NeF2axoa3V7BBHji6400uTW5ZXUBz17b4LP6PCvfNRebmnrzPb/C6f1tvKsEFjztDZOMefc7a2L4bqd5F5mzbvD6wiMrdnLeMb0B+N0nq1myo4S3Fu30dmo6XU1eCx/gxCfn8km2L381x9NKKK6u56HP1/H3WZsBU2lNX57Lc3O2cMk/FwX9bfO+wWDiqrWmsNLZauvGY2x5wgbjg6W+4ZQbPf0EfpRU13vdaDuLa6ipbyQuqvNk9YgXcID+3eOZNsYUhNV7ynl5/vaA8zX1jeywhOffS3P4Zl1+i4K3IqfM6w99ZcGOFlOrgYDm/4xVLYdQaa3ZU1rHgLR4nC43g+77mpuauS9yy+uIjozg2L7JnHW0caM0F+C1eRUkxji4xurkOPe5BdQ2NAb4aLcVVjM8oxvRjgienb2lRcbZbfmJ+ySbcacfLG1pIXy9tqX12FyP1uVVcOnYvvztZ8cCkN0srntKTYG8IqsfAIu2lbS45vTlphPu6F7dWpzzv86oPkne72tyy/nvylx++9EqwOcvfvqy4wD4dGUel/wjsPDuLPZVLuc8t4Cb3m25m84SawZctxgHn67MCxgjHKHMO9xb7uTkwWn0SPSt9hTjL+BNmoRoB/1S46h0NvLH6WsCOkM9lmXPbrHUBHHr1TY0sb2oGleTZtKIDLrFOLzWtkdIwSfgHpF/5IsNzNlQwNEPfMOzs7eQW1aLW+N1x/lz2tPzvJ/rGpqItyqSxFgHSkG5X+VQWtPg7YtpaHTzrNUCOTH37YBrJiuTvmk9erLQbfJDr8IfvOcTa3MZ0jPR+/2W95bzyBcbuP3fK3A1uVm/t4JGt+amUwd6w7y3pOV4co+LKdaqdHLL6nhn8W7+12rh+bswV+aUMydIC9M/LwAtJiJV1zdy9rPzGf/k3KBxANiw12cQeCpYf7TW1DU0kWS51+7+aBUD//R1QH9Hbpmvwpq+PJflu8sY3S8l6P1CgS0EHOCEzO7ez+8v2R0gaJ5m4SMXjUJrM0LgXz/tZsmOEm/zfo3VrD+uXzIAk59b0GI0Snmdi5MGdef4Aan8ZebGFi6I8loXDU1ur/ACzN5QwF0frmTZrlLcbk1eWR29U2KJjFC8cW0WA9LieWjGem58J9tbqeSV19EnJZbrJmYCUFbr4vb3V3DRS4soqa5Ha01OaS0nD0nj/qkjWJlTzk87AoXVY42ed2xv77GtBYEW68JtxZwzMoM7zxriPfbKgu1eq6jK6WJfpZMhGYlckdWflPgoVuYEZuTcMiOAt5w+mD7Jsby5cGerHVh/v2K09/PSnaUBPvOCKieZPRK8cVm3t5K7P1rNpyvzKKxyekXxjOE9ue5kky41DY3M3lDA3R+t8qaJv6U8Z2MBtQ2NAUMCXU1uzh2VwenD0wMKF5hOr6Lqehqa3PRMig0oaP4WnavJTVRkBL2SzSSMj7L3cNJf5lJcXU9RVb3XAu+ZFHy5P6eryfs8fVJiA9wZ24qqvZ20HtHxVHxv/7iLGy2D4KV521i03VSW/3P6YO/vH7zA7Bu+p9S0ALXWrM2r8K5dHhmhvLvsFFnx/GhZoBXsEcpUZw70Hk3tiMup11FeC/yUY4aySg9hrTsz4HcZqpShGYmM6W/SzX8d8ZzSWr5YnU+MI4IbTx3EacPSOX1YOuvyKgLK0Y6iakY8+A0fLM2hxjq+fLcvn1Q5XczfUhQwAebGd7MDjJPy2gaKqwPdoC9+F+huWZVT7jXqHvhsHfkVgXkBjFE3yFoL5tJ//ugd5OChuLqBmoYm7jx7KErBLssY8HeBevLYz8YZA6dbrIPfTR7e4l6hwjYCfv5xvZl516mM6pPEntI6bno3m/eXmJrak6DDe3XjkYtMs/ihGev5+as/8cvXTRN6/d5K+qbE8ei0Y7zXbP7CKmpd9EiM4TeThlLlNOLhj6fTqXdyHNNvmcBtZ5iC9fmqvVz+8mL+9u0m7xrAYNw/N55irJE5Gwv4wXKd7Ktw0js5jsHpibx7w3gA5m02LoWN+VWU1bqobWiif2o8Pzu+H9GREdzzyeqALbs25lcSoWDamD7e+E1fvoc3Fu7kQ6vZV1jppH9qPL+dPJztT04lKlKxMqec309fQ21Dozfd+qXGo5RidL8UVu0p54etRV6R92TQvqlx3HrGYPLK67j7o1UB/s20hGhOHpzGqD7JfHXnKQBc8cpiLvXrjCyocJLRLZbfTh7OTacOZIGfb3dVTjnF1fVEKLN/4MMXjeKeycNwa7jp3Wz+uzKPoqp6ckpq6Z8ax9UnHeX97QmPz+HYh2fx6YpcthVWs6O4hhG9kxjR22ftAwxKNwV1nTUELiMphmS/SRZbCqr5aFkOi7eX0OjWOCIV5x/bm5tPGwSYwpz1+Byufn0JhdazZ3SL5aObT+L35wYW2Mo6l9f90ys5lhevGsu1EwYwYVAaDY1uTn3KWM8eq/z843oz2jIs/PH41Qd0j+emUweSFOvg5yf0957fXVrLZ6vyyC2rIyvT57Jzupp4d/FuTnhiDj9sLfL2N7xw5ZiA6yc590LKACJ6DCFGueihKiAymqnjTJ514BPNxshY0ilnaM9uvParLG+LzcO2wmo25ldybN9kMpJiefeG8Vw/MZMmt+b6t5Z5rV2PO/EvX2/0tkB+3O5r1WXvKqOh0c1DF44MuP4363ybT3uucf/UEd73895POQEWtcc4mzKqFwDTXloU0FcDZganv6vTfygpwK4SUwEM7pnoNSoALnppoXcburxyI+oPXjiSx6aN4o1rTwgwMkKNbQQcYETvJBKtjqc5Gwu5/7/ruPODlV4rsXdyLNeenMlJg3zWusdSXb+3gpF9khjTP4VnLUtxw95Kpr20kByrZi2vc5ESH8X4gd3plxrHXR+u4tUF25mxei9OVxN/n2WanhlJMWRldm9R074yfwcrc8rp42c9XDMhk/dvPBGl4I/T1+B0NbGrpIZ+qSbM0IzEgGtsyK9gt5Vx+qXGkRjj4PKsfuSV1zHhyblc9NIitNZszK9iUHoix/RJppdlfb2zeDePfbmBez9dS019IzUNTV4rMTJC8fRlPgv52VlbKLEydA9rIsjYo1LYXFDFNW8s5ffT17C7pIY9pbVkJMUQ44jkwtGmspixei8nPjmXZ2dtxu3WVNS5vFbZyN5J3kIDcM6z8ymrMZZMnxQTz19NyAx45pv/tZx5mwvpnhBDpOX0HdgjMF1W7Slnd2ktA9ISePziY9n2xHkMSk/wujB++/FqVu0pR2u44LjeXJ7VjzOH+/ZgfeB8IwjfWZ3Q/VPjAZh+ywRvmD/+Zy1XvfYT+eVOHBERxGy1pfYAABspSURBVEVHct/UEfz1Up9YbS6oYu7GQqIdESTFOThxUBq3n+lr4STHRfHMrC3e0UfpiTFMOaYXj0w7hu5+E26a3JqdJTWkxEfRNyWOz+84hZevHuc9P9TPVZHeLYb7zx/JmofPJSHGwdd3ngqY0TRLdxrr9YZTfG4L/+F117yxlH0VTlLjo5g2pi9/v9yTBzRJ9fsgdQAxCabyOLefC2JTGGgtMbvWbcRxd/QQHE1OfuGYR2ZCA+ndYgKED8wuRrtLzPvxMHFIDzKSYliys5QrrfVKPG7KSmdj0K3cPP7kEwelseuv57PjyamkJUTzm49W8ZeZGwPCXDquL/dNHcHDlthPfdHn7lm/t5I+ybE8avVzFFbVB/RxNTa5Kah00icllv+9aixgWhT+w4M9bpqBaQk8dOEob14pq3Xx2JcmLrlldSTFOkiOi+KaCZmMH+jTns7AVgIO8OSlgTX/jNV7eXfxbuKjI72FctKIjIAwxdX17Cyu8fpgLx3Xj2tOGsCuklpW51bw8Bfr2VNaS1ltA+mJscQ4InnhSvNSn/x6E3d+sJK/ztzktchHWteJjFB8f88ZvHDlmIDOLI+YeZg4pAfnH9ubfZVObnt/BVXORsZZBaBXs0X0V++p8Prnh2WYpvWpQ40Q1TQ0sbO4hq2F1azJLWdk7yQiIhQ/3Xc2C/94ZkCz0+MKSffz8148ti/bnjiP7gnR/Hdlnvc+3RONsJw8uEdAXE5/+ns+WZ5LPytdU+KjefO6LO/5F7/bxruLd9Ho1t4pw0op/n7FaK8FvLWw2juSwvM8/bvH89jFx3DnWUNwWIK9Lq+SXsm+uHr6Dzz8d2UeOSU1HNXdxMURGcG9U44OCHPPJ2Y8dd+UeHp2i+Wt68dzz+RhfHrbyV7f7Xs/5RAbFeF9h1mZ3VtMd25ocgd0xnl85Y4IRe/kWBbvKCEzLT5glt1vzxnGmP4ppPpd68LRfXBE+orY7lKf33bK8wv4eNkehqQneq9z/ABT+O86eygv/cIn5s1n843sk8RJg7rz15mb+GBpDv27x5EU67tvZlp8QPgPl+0hvVuMN07dYhykU4HDXQ8pA1DdjVD3L5wPcSZf/vn8Efy18SouqX+E0ysf9V4rptz0P3kMEJM+0Tz1zWbjjvOreKIiI5h19+mAEey3Fu1sMS/i5tMG8dRlx/GMVbG8MHcr0ZER3slJERGKMsvafWX+DjLv/YqX52+nV1IsadZ7ufT4ft7reVpYxmBL9j43mGGLFbUub5+AW0OflDguHN2H1Q9NJjoygtveX8F3m0w5/2FrMclxUfS1njUrszvP/3wMk0Zk8P3mQrQ2Hcp9UwPTuzOxnYAPTk/kiUuMG8QjEhvyKzmmb7J3lMHVJw3gnsnDvM2wD5fmoDWM6uNrpl46rq9XPL7bVMipT81Dazjb6jA6fkAq152c6fWNvm3V3sf1S/YO2QLI7JHAtDF9mX7ryd5j5/v5pT14mnrfbSokOjKCMyzrUCnFW9edwKy7T+OKrH7MXJfPi3O3ERcV6RWr5hXC6z/soLCqntOG+SzMfqnxLLr3LG8noGf0xfBmHYuOyAj+9evxlNe5eOSLDQCkWVu3ZA1I5Yqsfozrn4TCZ8WdM9JXIZ46NN3r7wMj4gBH9/K5LBJiHMy861Q+v30iAL+xOir943LNSQP47eThzLr7NO9zjOrtez9x0ZG8cs3x/O6cYVa67KOmoYlThvgqmUkjMjhzeDpP/ew4LvMrxP4TKe44ayjjjkqlb0qc17qfMCjNu8i+55mAALfA9Vb/BEAPSwRuO2Mw9553dECaebjz7KF8dvtEb8ccwLTRfQLC3HfeCG8lu7Wwmka3DhDC9G4xrH14MnefMyxACIPxmJ8rsPmM0e9+dwbH9E0KaPZ7NqmIdkSw5uHJPDXJylMpR8Hgs81ntwvizPEbTx3Eir9exWN3XA/A7103mzANxjUUH+3g3vOO5rGLj+GC43zPeeHowLyfHBfFv35t3ISPfLGB2RsK6Jcaxx+nHM3I3kncfsYQrsjqzwXH9fZWlM2fvU+zBaHKal0M7OGz9JNio1h2/yTSEqK54H8XMvm5+WwvMgabUoo5vz2dIT0T+XZ9AaMfncWl/1zk7Rfw+MCT46JIinNQUFnPDW9nc80bS1i6s4Szj+4ZkFcuHtuXU4akUVLTwCsLdrAyp4zhGft/V6Gk4zb960SuOuEoUuKiOXFQd7IenwPA9X6ZNTYqkjvOGkpNfSPPztrCM5br49i+PoEYe1Qqy+6fRFx0JKf87Ttvp8hIP9/pAxeM5L6pIyiuruf6t5axr9LJO9ePDxqnEb2TiHZE0NDoDlibwsNx/VJ4+epx3PLeCq6ZMMBrPQCcaVmb908dycfZueSV15EQHemtkHolx/LEJccYH/4XG/g4O5fEGAfnjspocZ8px/Ri6c5SPmltZIizklG9k7hsXD9KVnzGPTHT6V6YCJXdiOg7jqcuGw3vXcb20m2cXfMEg9MT+NUEX6dtVGQEf79iNH88bziXv7yY3SW1JMdFcfLgtCDPnExGUgwF1vDNnt1advoNSk/kzWuz+PfSnADXC8C5o3px7qherMurYNaGAiYMSguwzCMiFG9Z7+PUYT34ak0+pwwNbEX4h112/yRemb+da/yeB8zIlz+fP4KMpFh6JMawo6iGyX5xGdM/hf/cejJj+6eglBnv69+p7s/zV45hyvOmKd88LicP6cGie8/i4RnrvQbB8QMCXRHdLEs6MiLQ6m7O0IxuzLhjIhe9tIjb/TqpPc/65f8zbpaRfZL4w/Q1ARWLUoozM6wWRspREOmAqHhw1UJsoLEwqk8SJw7szupdVkeq09fxf4vVudrk1qTGR3PpuL7e1po/pwzpwe1nDuYf87azpaCac0dlcOsZg7n1DF/nbGxUJMvuP5uZ6/YFlFOA9359ImvyKlixu8ybbs0Nk/RuMfzjl+O48tWf2GJ15nuuM6RnIm9dd4K372FXSa23I3eU370uHtOX161hyJ6hs4ODVKSnD+9J9MxN/HWmcZOdeXTLUUIBNDWaoZgpR+0/XBtQnblWR1ZWls7Obn3WYFs459n5bC2sZvmfJwWIooc5Gwp4aMZ6ph7bi/vPHxnkCmakxZTnf+CKrP48eGHwMI1Nbtya/XZQ7KtwUtvQyKD01mtk41OObfU6M9fmc+v7K3jhyjHeoZP+rNpTzns/7eby4/tx4qCWoulh1vp99E6O41j/zrHlb8MXdwFQPPU1Vn33MZOcs33nM46Fn70G/zwJgC03bmVYv9Yz5zfr9nHLe8u5YeLAVtPN6WrimIe+JSU+muw/T2r1Wh1BaU0DyXFRBxS/g6aqAOrKoOfRZubLroUQEQkDTt7vzxqb3Dgb3d7+GmpLIToBHCZ/7ln/I89/+j3nXHIDk0dmeCvq5vz+k9UMzUjk5tMGBz0PZnKav/sEgF2LYN8aGH0VrugkZrz5JBdWfUy0dsH4m6AiF+LTYMFTcN9eE7enh0BNERz3c7j01RbP89bMRdyUfQH0GAZ3tD7xpTXqGpoY8eA3AK3m7QPR5NY8P2cLVc5G/jBleEBL2MO+Cic3vL2MlPgo3rlhfID1POhPX5EQ42BweiKr9pRz4ykD+fMFvnzranKzr8JJfHQkx1uG4avXHB9QmXsorq7n3OcWUFLTwMoHzglqtHlZOx0+vQlunAN9jz/k5wZQSi3XWme1OG53Aa+ub2RrQVWLThWhGe4meNTPakwbYv62fNP6b372Bhx72X4vu2FvJcN7dduvaHpGznRrLjSHm+Kt8J8boXw39BtvnnXHfNj1g1mVr9aafDRkEgw6A2b92Xy/ZRH0slwYm76G4s1w4i0QFWd+t/ELSM2Eo06C/DXw5rkQGQX9ToBtc3z3/9kbkDIAEnvCnqXQZwz0GApFWyD7TTj9DxB/CJ1ilXvh69/Dpi/N95QBkHEMbP4qePjYFLjXGnP9whgo2wnj/wemPtUybH0V/MVyU926GDKCV9j7Y/6WIgoqnVyR1f/AgUNAeW0DERGKpNgoSmsaAjqVm/Ppily+WL2XV3+VFVAJ+FPb0Eh+hZPB+zHY0BpeOxOclXBHNkS0zWvdZQX8iEVr2PA57F0JQ8+B7oNM87O+yghnfHdj4W2fC4kZxqqLCuHi73krTEYadCbssCaBxKaY+5bugCK/NVum/NXM0GtqhN9vA8d+rIu24NlRtzXKdhkh7DEMIhzBw9ZZ49XjDnLSRP5qWPOxCT/xN+bdfP17qDvwmh1BmXgXJKT7RL3XceY9L3vd52a46kP48SXYswT6jjP/wYhqwTrTpG6+ccLwqaYScdVA3yxzjcR0KN8Dq/4NGaMgJtGkDwrSh0PaYFMZvX8ZVBfBxDuNYOyYB4UbIL4H3PAtVOWbFkTRJvP8g88yzwHw4liTDyY9DKfc3fJ5tYZHrLSOcJh4nnCjyT9bZ0NlHgybAin9zfMvecW0XoZMMmmT0APcjaYi62hKtkNkNCT323++6iy0hsUvwQ/PQlOD6Te48EU4/to2X9LeAt5QA9vmGqtk70pjsST3MxZGXCr0Hg2DTofqQiM8lXtNIXFEQ1SCyTzDzjUFZu10+O5x05wdMsmIRO4yU7BPut0Ia/abJlNGJ8DQyUZ8AXqOMOcb62Hh85C/yohxUl+o2AO1JdB/vLF2N3xuLLlgRDhgwERToButXnkVaeIXnWDiPuYq83xf3wPFW0z4mG6QNtSESekPmaeaQrLgGajMNWnRrQ/0GAKNDdBnrInf4pdgzkPmPvdsg2f8fKan/xHOvM+k3ZZvIKEnDJ8Ci16A2Q9CTDLcONv4R7/+PdRXm3sn9TH3ckTDUScbgVr+Nqz/r6mIYrpZIxqUeVdHTYCmepOpd/9o3llssnkWVy04Yo2VW7nX3FtbY5BjU0wY7YbeY0zcdi2CtR9DZIx571Hx5vfpw0yYniPMs2yYYe5RXwnb5+FdCzBlgHlfPYbB5W+bdF//Gcy6HybcDkPOgaLNcNwVRhDmPGJcEuc+aUT7y7thw2fmWkedDEedaMXZbZ5r9FWw6n3YZy1vMPUZGHs1LH0VRl1i7vfV74zYA2TdYCz1PKtspA2Fcb+C2Q+Y78f93DxLY8vJKCb8EFPpuRvhV5+bdPTgrDSiHZ0Q/LcenhpsWhyXvmaeOxjb5pg03/QVrP63EerIGPNewYho1g3mfEWQKfQRUTD6Sjj6fJPfC9ZBdKKpRCvzTNqlH23inNTbpM+GGaYMVheY9I1Pg+4DTdmNiIKFz8HWb831u/WBkReZ8qK1eQ+VeSZtnBVQUwwDTzXvOMbPh+5ugtxsoxOpmYAy+c+/stm31uSR9OFGlLXb5LthU0yFuvMHU5k7y8066m6Xyf/Dz4dex8KJ/9OuysXeAv7WVNi9yFiqg88yVkVDjfEtNregImNMM7Rwg8kkbs9YUwWpA8zL7HWsEYbdi8yLiIwxL8V/sc/0ESZs80LTY5ixoqvyTUEr2Wb9ThnhcllTt1MzTXN01CWQu9QIU1S8qXxWf2Cayf2Oh/E3G9/j7h+hdKd5rtylRpjBCGi/LGNBepr0HlIGmErDVWtEoWxX62k48DQYcZHxge6YD+9eZI5f/R9TGJqzeSZ8cGXgscReJi6lO00BrfdbF0VFmLTsPsikrSeN3I2+SgqMNTjsXGM1VVlpEhHlK8Rg4jn0HBOmtsSkXVwK5PxkwkTGwAm/NhZjyXZT2JyVwdMHICbJFNyJd8Hif8CPLxrRu3m+KXyHitttKufyHCOujmjYu8qIxICTTcEv2Q7/usQI0pXvt7Q8q4tg90LjVkm2XBO1pabiGX6eKfxzH4Uf/g6OOFNxnfWAERpnubG6I6Mhbzms+cSI2hn3mrzdFh62+kqu+woyTzlw+PpqU4nmLTcC23MkzHoAtsw0IjvtnyYuOxeYMlJdYPL21lnekSwBRHeDhiArHA44xVSeSX3Me6wp9AkkmHsdf51Jo10LzP2CkTbEGF6eimXQmTD4TJP/lr5qRN9DhMPk517HGoOgdDvs+D74dR1xRleKNkFyfxPHkm1wzqMmj3ZQi8PeAr79O5OgAya2TBBnhbGWts8z58ZdCwlpppCBEY+KPab5XLLN1PIn3QZRseaFFqyDnqOM2Kx412r2nQ1HX2BEqCLXZKDYZPOSts8zYjL2GiMyDTXgcpoaPcIBFTnme/rwtte4Lies/cRk+mMu9bUAmlzmWJMLchabMPE94JTfmKY1GHF11Rmra/ciqNoHfcYZ0fTEx+2GR60+gwfLgvvltDZi0nu0aU00NcBxV0I3a+RLU6M55qqDTV+YJvzA002aNH/u2lLY8q05PvJik/bB7rd3hamw0gYHTzu3G0q2QrfeEJvU8nx9lfFJV+aaAtpnbMvraA0755uKOKlPy2uEK54WwX35EN2Occ3OSmPtR7SyIl9jvamEmhpMh15DtTG0Enuasliy3bzbHfNh1MXmHTanvtoIamWesbI9eUFrU9YjHMay3/KtKZdDJ/vy+M4FpqWYt8L4/MF03k+4zZT9ujITR3eTMZr2rTEGSdZ1Rlsq9/rKesk20+Ks3AsDJhhdccSafBgsf7YDewu40PEs/qcR58yJhzsmwpGA221aSx3d33EkU55jDLD0o48M3/l+aE3AbTkOXOgAJtx2uGMgHElEREBEGIk3hGRcdmdju5mYgiAIgkEEXBAEwaaIgAuCINiUdgm4UmqKUmqzUmqbUurejoqUIAiCcGDaLOBKqUjgH8B5wEjgKqXUoc+vFQRBENpEeyzw8cA2rfUOrXUD8CEwrWOiJQiCIByI9gh4X8B/vmyudSwApdTNSqlspVR2UVFR89OCIAhCGwl5J6bW+lWtdZbWOis9Pf3APxAEQRAOivZM5MkD/NeF7Gcda5Xly5cXK6V27ydID6B4P+fDHUmfAyNptH8kfQ7MkZhGA4IdbPNUeqWUA9gCnI0R7mXAL7TW69saQ6VUdrDpooJB0ufASBrtH0mfA2OnNGqzBa61blRK3QF8C0QCb7ZHvAVBEIRDo11roWitvwa+7qC4CIIgCIfAkTYT89UDBwlrJH0OjKTR/pH0OTC2SaNOXU5WEARB6DiONAtcEARBOEhEwAVBEGxKpwq4UipFKTVdKbVJKbVRKTVBKdVdKTVbKbXV+p9qhVVKqRethbLWKKXGdWZcDxdKqbuVUuuVUuuUUh8opWKVUgOVUkustPhIKRVthY2xvm+zzmce3th3PEqpN5VShUqpdX7HDjnPKKWutcJvVUq1fXvwI5BW0uhpq5ytUUr9VymV4nfuT1YabVZKnet3vEsuThcsffzO/U4ppZVSPazv9spDWutO+wPeAW60PkcDKcBTwL3WsXuBv1mfpwIzAQWcBCzpzLgejj/MUgQ7gTjr+8fAddb/K61jLwO3Wp9vA162Pl8JfHS4nyEEaXIaMA5Y53fskPIM0B3YYf1PtT6nHu5nC3EaTQYc1ue/+aXRSGA1EAMMBLZjhgFHWp8HWWVzNTDycD9bqNLHOt4fMwx6N9DDjnmo0yxwpVSylZBvAGitG7TW5ZgFsN6xgr0DXGx9nga8qw0/ASlKqd6dFd/DiAOIsyZKxQP5wFnAdOt88zTypN104GyljvDN/Q4RrfUCoLTZ4UPNM+cCs7XWpVrrMmA2MCX0se8cgqWR1nqW1rrR+voTZqY0mDT6UGtdr7XeCWzDLEzXZRenayUPATwH/AHwH8lhqzzUmS6UgUAR8JZSaqVS6nWlVAKQobXOt8LsA6xtzw9usayuhNY6D3gGyMEIdwWwHCj3K4z+6eBNI+t8BZDWmXE+TBxqngm7vNSMGzBWJUgaAaCUmgbkaa1XNztlq/TpTAF3YJox/6e1HgvUYJq/XrRpq4TtuEbLlzsNU9n1ARI4Amr5I5lwzzMHQil1P9AIvH+443KkoJSKB+4DHjzccWkvnSnguUCu1nqJ9X06RtALPK4R63+hdf6QF8vqAkwCdmqti7TWLuBTYCKmGeeZNeufDt40ss4nAyWdG+XDwqHmmXDMSyilrgMuAH5pVXQgaQQwGGMkrVZK7cI86wqlVC9slj6dJuBa633AHqXUcOvQ2cAGYAbg6dG9Fvjc+jwD+JXVK3wSUOHXbO6q5AAnKaXiLV+2J43mAZdZYZqnkSftLgO+8yuoXZlDzTPfApOVUqlWK2eydazLopSagvHvXqS1rvU7NQO40hrBNBAYCizFLEY31BrxFI3pFJ/R2fHuDLTWa7XWPbXWmVrrTIxxOc7SKHvloU7uDR4DZANrgM8wvblpwFxgKzAH6G6FVZgt27YDa4Gsw93j20lp9AiwCVgH/AszWmAQppBtAz4BYqywsdb3bdb5QYc7/iFIjw8w/QEuTEH7dVvyDMYPvM36u/5wP1cnpNE2jM92lfX3sl/4+6002gyc53d8KmaF0e3A/Yf7uUKZPs3O78I3CsVWeUim0guCINgUmYkpCIJgU0TABUEQbIoIuCAIgk0RARcEQbApIuCCIAg2RQRcOKJRZgXL26zPfZRS0w/0m3bca4xSamqori8IHY0IuHCkk4JZdRGt9V6t9WUHCN8exmDGQguCLZBx4MIRjVLKsyreZszEnRFa62OsaeIXY9aLGYpZBCwauAaoB6ZqrUuVUoMxEzPSgVrgJq31JqXU5cBDQBNmEbBJmAkacZgp0n/BLO37AmbCVB1m8sbmQ7j395hlWU/HrAV0g9Z6aWhSSghLDvdMIvmTv/39AZlY6zg3+3wdRnC7YcS5ArjFOvcc8Bvr81xgqPX5RMxyA2Bm2fW1Pqf4XfMlv3sn4VtTexLwn0O89/fAa9bn02i2HrX8yV97/zwLJAmCHZmnta4CqpRSFcAX1vG1wHFKqUTgZOATv2XSY6z/i4C3lVIfYxYNC0Yy8I5SaihmxcOog723X7gPwKxJrZRKUkqlaLMOviC0GxFwwc7U+312+313Y/J2BGYt9THNf6i1vkUpdSJwPrBcKXV8kOs/hhHqS5TZru77Q7i391bNb72f5xGEQ0I6MYUjnSqMq+KQ0VpXAjstf7dnv8PR1ufBWuslWusHMRuN9A9yr2R8S4Ze17bo83PrfqdgVraraON1BKEFIuDCEY3WugRYZG1I+3QbLvFL4NdKqdXAenzbhD2tlFprXfdHTGfjPGCkUmqVUurnmL03/6KUWknbW6tO6/cvY1YJFIQOQ0ahCEKIsEah3KO1zj7ccRG6JmKBC4Ig2BSxwAVBEGyKWOCCIAg2RQRcEATBpoiAC4Ig2BQRcEEQBJsiAi4IgmBT/j+XqYIHM8MvTQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rtxNzsMzzH1"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYXHKiTgpiRG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbSpn7Ewpj_w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d24Nex-n0BNm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}